<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[PostgreSQL之MVCC]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E4%B9%8BMVCC/</url>
      <content type="html"><![CDATA[<p>  Postgre数据库的很大的卖点之一就是它处理并发的方式。我们的期望很简单:读永远不阻塞写,反之亦然。Postgres通过一个叫做 多版本并发控制(MVCC) 的机制做到了这一点。这个技术并不是Postgres所特有的:还有好几种数据库都实现了不同形式的MVCC,包括 Oracle、Berkeley DB、CouchDB 等等 。当你使用PostgreSQL来设计高并发的应用时,理解它的MVCC是怎么实现的很重要。它事实上是复杂问题的一种非常优雅和简单的解法。</p>
<a id="more"></a>
<h2 id="MVCC如何工作"><a href="#MVCC如何工作" class="headerlink" title="MVCC如何工作"></a>MVCC如何工作</h2><p>在Postgres中,每一个事务都会得到一个被称作为 XID 的事务ID。这里说的事务不仅仅是被BEGIN - COMMIT 包裹的一组语句,还包括单条的insert、update或者delete语句。当一个事务开始时,Postgrel递增XID,然后把它赋给这个事务。Postgres还在系统里的每一行记录上都存储了事务相关的信息,这被用来判断某一行记录对于当前事务是否可见。</p>
<p>举个例子,当你插入一行记录时,Postgre会把当前事务的XID存储在这一行中并称之为 xmin 。只有那些*已提交的而且 xmin` 比当前事务的XID小的记录对当前事务才是可见的。这意味着,你可以开始一个新事务然后插入一行记录,直到你提交（ COMMIT ）之前,你插入的这行记录对其他事务永远都是不可见的。等到提交以后,其他后创建的新事务就可以看到这行新记录了,因为他们满足了 xmin &lt; XID 条件,而且创建哪一行记录的事务也已经完成。</p>
<p>对于 DELETE 和 UPDATE 来说,机制也是类似的,但不同的是对于它们Postgres使用叫做 xmax 的值来判断数据的可见性。这幅图展示了在两个并发的插入/读取数据的事务中,MVCC在事务隔离方面是怎么起作用的。</p>
<p>在下面的图中,假设我们先执行了这个建表语句:</p>
<pre><code>CREATE TABLE numbers (value int);
</code></pre><p><img src="https://raw.githubusercontent.com/panwenhang/blog/master/source/images/PostgreSQL/MVCC_1.jpg" alt=""></p>
<p>虽然 xmin 和 xmax 的值在日常使用中都是被隐藏的,但是你可以直接请求他们,Postgres会高兴的把值给你:</p>
<pre><code>SELECT *, xmin, xmax FROM numbers;
</code></pre><p>获取当前事务的XID也很简单:</p>
<pre><code>SELECT txid_current();
</code></pre><p>干净利落！</p>
<p>我知道你现在在想:要是同时有两个事务修改同一行数据会怎么样？这就是事务隔离级别（transaction isolation levels）登场的时候了。Postgres支持两个基本的模型来让你控制应该怎么处理这样的情况。默认情况下使用 读已提交（READ COMMITTED） ,等待初始的事务完成后再读取行记录然后执行语句。如果在等待的过程中记录被修改了,它就从头再来一遍。举一个例子,当你执行一条带有 WHERE 子句的UPDATE 时, WHERE 子句会在最初的事务被提交后返回命中的记录结果,如果这时 WHERE 子句的条件任然能得到满足的话, UPDATE 才会被执行。在下面这个例子中,两个事务同时修改同一行记录,最初的 UPDATE 语句导致第二个事务的 WHERE 不会返回任何记录,因此第二个事务根本没有修改到任何记录:</p>
<p><img src="https://raw.githubusercontent.com/panwenhang/blog/master/source/images/PostgreSQL/MVCC_2.jpg" alt=""></p>
<p>如果你需要更好的控制这种行为,你可以把事务隔离级别设置为 可串行化（SERIALIZABLE） 。在这个策略下,上面的场景会直接失败,因为它遵循这样的规则:“如果我正在修改的行被其他事务修改过的话,就不再尝试”,同时 Postgres会返回这样的错误信息: 由于并发修改导致无法进行串行访问 。捕获这个错误然后重试就是你的应用需要去做的事情了,或者不重试直接放弃也行,如果那样合理的话。</p>
<p><img src="https://raw.githubusercontent.com/panwenhang/blog/master/source/images/PostgreSQL/MVCC_3.jpg" alt=""></p>
<h2 id="MVCC的缺点"><a href="#MVCC的缺点" class="headerlink" title="MVCC的缺点"></a>MVCC的缺点</h2><p>现在你已经知道MVCC和事务隔离是怎么工作了吧,你获得了又一个工具用来解决这类问题:可串行化事务隔离级别 迟早会派上用场。然而MVCC的优点虽然很明显但它也存在着一些缺点。</p>
<p>因为不同的事务会看到不同状态的记录,Postgres连那些可能过期的数据也需要保留着。这就是为什么UPDATE 实际上是创建一行新纪录而 DELETE 并不真正的删除记录（它只是简单的把记录标记成已删除然后设置XID的值）的原因。当事务完成后,数据库里会存在一些对以后的事务永远不可见的记录。它们被称作dead rows。MVCC带来的另外一个问题是,事务的ID只能不断的增加 - 它是32个bits,只能”支持大约四十亿个事务。当XID达到最大值后,它会变回零重新开始。突然间所有的记录都变成了发生在将来的事务所产生的,所有的新事务都没有办法访问到这些旧记录了。</p>
<p>上面说到的dead row和事务XID循环问题都是通过执行VACUUM命令（Postgres用来执行清理操作的命令）来解决的。这应该成为一个例行的维护,所以Postgre自带了auto_vacuum守护进程会在一个可配置的周期内自动执行清理。留意点auto_vacuum很重要,因为在不同的部署环境中需要执行清理的周期也会不同。你可以在Postgres的文档里找到关于VACUUM的更多说明。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[BDR0.9.0版本测试]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-BDR0.9.0%E7%89%88%E6%9C%AC%E6%B5%8B%E8%AF%95/</url>
      <content type="html"><![CDATA[<h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><p>参考bdr官方文档<a href="bdr-project.org/docs/stable/installation.html">Chapter 3. Installation</a></p>
<a id="more"></a>
<h2 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h2><p>先initdb,然后修改配置postgresql.conf:</p>
<pre><code>#必须的配置:
shared_preload_libraries = &apos;bdr&apos;
track_commit_timestamp = on
wal_level = &apos;logical&apos;
max_replication_slots = 30
max_wal_senders = 10
#可选的:
#bdr.synchronous_commit = on
#bdr.log_conflicts_to_table = on
#bdr.max_workers = 512MB
#max_worker_processes = 50
</code></pre><p>配置pg_hba.conf,然后启动实例:</p>
<pre><code>local   replication     postgres                                md5
host    all             all             xxx.xxx.xxx.xxx/32       md5
host    replication     postgres        xxx.xxx.xxx.xxx/32       md5
</code></pre><p>配置.pgpass</p>
<pre><code>xxx.xxx.xxx.xxx:5432:*:postgres:xxxxxxxxxxxxxxxxxxxxxxxx
</code></pre><h2 id="3-初始化第一个节点"><a href="#3-初始化第一个节点" class="headerlink" title="3. 初始化第一个节点"></a>3. 初始化第一个节点</h2><p>进入PostgreSQL后执行:</p>
<pre><code>bdr=# create extension btree_gist;
bdr=# create extension bdr;
bdr=# select bdr.bdr_group_create(
    local_node_name := &apos;node1&apos;,
    node_external_dsn := &apos;host=node1_ip dbname=bdr port=5432&apos;
);
</code></pre><p>然后你能看见第一个节点已经准备好了:</p>
<pre><code>bdr=# select * from bdr.bdr_nodes ;
node_sysid      | node_timeline | node_dboid | node_status | node_name |              node_local_dsn              | node_init_from_dsn
---------------------+---------------+------------+-------------+-----------+------------------------------------------+--------------------
 6139739409922254460 |             1 |      16384 | r           | node1     | host=node1_ip dbname=bdr port=5432  |
(1 row)
</code></pre><h2 id="4-新节点的创建"><a href="#4-新节点的创建" class="headerlink" title="4. 新节点的创建"></a>4. 新节点的创建</h2><h3 id="1-逻辑克隆-initdb-pg-dumpall"><a href="#1-逻辑克隆-initdb-pg-dumpall" class="headerlink" title="(1) 逻辑克隆:initdb + pg_dumpall"></a>(1) 逻辑克隆:initdb + pg_dumpall</h3><blockquote>
<ul>
<li>1) 初始化实例:执行第一步和第二步</li>
<li>2) 使用pg_dumpall从第一个节点复制数据,这个过程需要集群只读,否则数据就不一致了</li>
<li>3) 在pg里面执行:</li>
</ul>
</blockquote>
<pre><code>bdr=# create extension btree_gist;
bdr=# create extension bdr;
bdr=#  select bdr.bdr_group_join(
    local_node_name := &apos;node2&apos;,
    node_external_dsn := &apos;dbname=bdr host=node2_ip user=postgres port=5432&apos;,
    join_using_dsn := &apos;dbname=bdr host=node1_ip user=postgres port=5432&apos;
);
</code></pre><p>查看是否加入成功:</p>
<pre><code>bdr=# select * from bdr.bdr_nodes ;
     node_sysid      | node_timeline | node_dboid | node_status | node_name |                     node_local_dsn                     |                   node_init_from_dsn
---------------------+---------------+------------+-------------+-----------+--------------------------------------------------------+--------------------------------------------------------
 6139739409922254460 |             1 |      16384 | r           | node1     | host=node1_ip dbname=bdr port=5432                     |
 6139757325958541519 |             1 |      16384 | r           | node2     | dbname=bdr host=node2_ip user=postgres port=5432       | dbname=bdr host=node1_ip user=postgres port=5432
(2 rows)

bdr=# select * from pg_replication_slots ;
                slot_name                | plugin | slot_type | datoid | database | active | xmin | catalog_xmin | restart_lsn
-----------------------------------------+--------+-----------+--------+----------+--------+------+--------------+-------------
 bdr_16384_6139739409922254460_1_16384__ | bdr    | logical   |  16384 | bdr      | t      |      |         1839 | 0/18E24A0
(1 row)

bdr=# select * from pg_stat_replication ;
  pid  | usesysid | usename  |              application_name              |  client_addr   | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+--------------------------------------------+----------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 25028 |       10 | postgres | bdr (6139739409922254460,1,16384,):receive | node1_ip       |                 |       52095 | 2015-04-20 09:56:34.211347+00 |              | streaming | 0/18E24D8     | 0/18E24D8      |             0/18E24D8      | 0/18E24D8       |            0  | async
(1 row)
</code></pre><h3 id="2-物理克隆-pg-basebackup-bdr-init-copy"><a href="#2-物理克隆-pg-basebackup-bdr-init-copy" class="headerlink" title="(2) 物理克隆:pg_basebackup + bdr_init_copy"></a>(2) 物理克隆:pg_basebackup + bdr_init_copy</h3><p>获取基础备份:</p>
<pre><code>$ /opt/pg94-bdr/bin/pg_basebackup -h node1_ip -p 5432  -X stream -D PG_DATA
</code></pre><p>bdr_init_copy:</p>
<pre><code>$ /opt/pg94-bdr/bin/bdr_init_copy -d &quot;host=node1_ip dbname=bdr port=5432&quot; --local-dbname=&quot;host=node2_ip dbname=bdr port=5432&quot; -n node2 -D PG_DATA
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL查看没有注释的表和字段]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E6%9F%A5%E7%9C%8B%E6%B2%A1%E6%9C%89%E6%B3%A8%E9%87%8A%E7%9A%84%E8%A1%A8%E5%92%8C%E5%AD%97%E6%AE%B5/</url>
      <content type="html"><![CDATA[<h2 id="查看没有注释的表"><a href="#查看没有注释的表" class="headerlink" title="查看没有注释的表"></a>查看没有注释的表</h2><pre><code>SELECT n.nspname as &quot;Schema&quot;,
    c.relname as &quot;Name&quot;,
    pg_catalog.obj_description(c.oid, &apos;pg_class&apos;) as &quot;Description&quot;
FROM pg_catalog.pg_class c
LEFT JOIN pg_catalog.pg_namespace n
ON n.oid = c.relnamespace
WHERE c.relkind IN (&apos;r&apos;,&apos;&apos;)
    AND n.nspname &lt;&gt; &apos;pg_catalog&apos;
    AND n.nspname &lt;&gt; &apos;information_schema&apos;
    AND n.nspname !~ &apos;^pg_toast&apos;
    AND pg_catalog.pg_table_is_visible(c.oid)
    AND pg_catalog.obj_description(c.oid, &apos;pg_class&apos;) is null
ORDER BY 1,2;
</code></pre><a id="more"></a>
<h2 id="查看没有注释的字段"><a href="#查看没有注释的字段" class="headerlink" title="查看没有注释的字段"></a>查看没有注释的字段</h2><pre><code>SELECT b.nspname, b.relname, a.attname, pg_catalog.format_type(a.atttypid, a.atttypmod),
  pg_catalog.col_description(a.attrelid, a.attnum)
FROM pg_catalog.pg_attribute a,
    (SELECT n.nspname, c.relname, c.oid
    FROM pg_catalog.pg_class c
    LEFT JOIN pg_catalog.pg_namespace n
    ON n.oid = c.relnamespace
    WHERE c.relkind IN (&apos;r&apos;,&apos;&apos;)
        AND n.nspname &lt;&gt; &apos;pg_catalog&apos;
        AND n.nspname &lt;&gt; &apos;information_schema&apos;
        AND n.nspname !~ &apos;^pg_toast&apos;
        AND pg_catalog.pg_table_is_visible(c.oid)
    ) b
WHERE a.attrelid = b.oid
    AND a.attnum &gt; 0
    AND NOT a.attisdropped
    AND pg_catalog.col_description(a.attrelid, a.attnum) is null
ORDER BY a.attrelid, a.attname;
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL权限管理之创建可更新表的普通用户]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E4%B9%8B%E5%88%9B%E5%BB%BA%E5%8F%AF%E6%9B%B4%E6%96%B0%E8%A1%A8%E7%9A%84%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1.环境"></a>1.环境</h2><pre><code>$ psql -U postgres
psql (9.4.4)
Type &quot;help&quot; for help.
postgres=# select version();
</code></pre><a id="more"></a>
<pre><code>                                                version
---------------------------------------------------------------------------------------------------------------
 PostgreSQL 9.4.4 on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-11), 64-bit
(1 row)
postgres=# \dt
No relations found.
postgres=# revoke select on all tables in schema public from public;
REVOKE
</code></pre><p>我们都知道,超级用户的权限太大了,为了数据库的安全,对于非管理员账号,需要创建普通用户。删除public下所有非超户的select权限。</p>
<h2 id="2-语法"><a href="#2-语法" class="headerlink" title="2.语法"></a>2.语法</h2><pre><code>$ psql -U postgres
psql (9.4.4)
Type &quot;help&quot; for help.

postgres=# \h create role
Command:     CREATE ROLE
Description: define a new database role
Syntax:
CREATE ROLE name [ [ WITH ] option [ ... ] ]

where option can be:

      SUPERUSER | NOSUPERUSER
    | CREATEDB | NOCREATEDB
    | CREATEROLE | NOCREATEROLE
    | CREATEUSER | NOCREATEUSER
    | INHERIT | NOINHERIT
    | LOGIN | NOLOGIN
    | REPLICATION | NOREPLICATION
    | CONNECTION LIMIT connlimit
    | [ ENCRYPTED | UNENCRYPTED ] PASSWORD &apos;password&apos;
    | VALID UNTIL &apos;timestamp&apos;
    | IN ROLE role_name [, ...]
    | IN GROUP role_name [, ...]
    | ROLE role_name [, ...]
    | ADMIN role_name [, ...]
    | USER role_name [, ...]
    | SYSID uid
</code></pre><h2 id="3-创建只读用户"><a href="#3-创建只读用户" class="headerlink" title="3.创建只读用户"></a>3.创建只读用户</h2><h3 id="1-先创建表t1"><a href="#1-先创建表t1" class="headerlink" title="(1) 先创建表t1"></a>(1) 先创建表t1</h3><pre><code>postgres=# create table t1 ( id serial, name varchar(64) );
CREATE TABLE
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | t1   | table | postgres
(1 row)
</code></pre><h3 id="2-创建用户u1"><a href="#2-创建用户u1" class="headerlink" title="(2) 创建用户u1"></a>(2) 创建用户u1</h3><pre><code>postgres=# create role u1 with login password &apos;123456&apos;;
CREATE ROLE
</code></pre><p>login是赋予登录权限,否则是不能登录的</p>
<h3 id="3-赋予u1对表的只读权限"><a href="#3-赋予u1对表的只读权限" class="headerlink" title="(3) 赋予u1对表的只读权限"></a>(3) 赋予u1对表的只读权限</h3><p>因为创建的普通用户默认是没有任何权限的</p>
<pre><code>postgres=# \c - u1
You are now connected to database &quot;postgres&quot; as user &quot;u1&quot;.
postgres=&gt; select * from t1;
ERROR:  permission denied for relation t1
postgres=&gt; \c - postgres
You are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
postgres=# grant select on all tables in schema public to u1;
GRANT
postgres=# \c - u1
You are now connected to database &quot;postgres&quot; as user &quot;u1&quot;.
postgres=&gt; select * from t1;
 id | name
----+------
(0 rows)
</code></pre><h3 id="4-创建表t2"><a href="#4-创建表t2" class="headerlink" title="(4) 创建表t2"></a>(4) 创建表t2</h3><pre><code>postgres=&gt; \c - postgres
You are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
postgres=# create table t2 ( id serial, name varchar(64) );
CREATE TABLE
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | t1   | table | postgres
 public | t2   | table | postgres
(2 rows)
</code></pre><h3 id="5-验证u1的权限"><a href="#5-验证u1的权限" class="headerlink" title="(5) 验证u1的权限"></a>(5) 验证u1的权限</h3><pre><code>postgres=# \c - u1
You are now connected to database &quot;postgres&quot; as user &quot;u1&quot;.
postgres=&gt; select * from t1;
 id | name
----+------
(0 rows)

postgres=&gt; select * from t2;
ERROR:  permission denied for relation t2
</code></pre><p>可见u1是有t1表的读权限,但没有t2表的读权限,这样是不是意味着每次新建表就要赋一次权限？</p>
<h3 id="6-解决办法"><a href="#6-解决办法" class="headerlink" title="(6) 解决办法"></a>(6) 解决办法</h3><pre><code>postgres=&gt; \c - postgres
You are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
postgres=# alter default privileges in schema public grant select on tables to u1;
ALTER DEFAULT PRIVILEGES
postgres=# create table t3 ( id serial, name varchar(64) );
CREATE TABLE
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | t1   | table | postgres
 public | t2   | table | postgres
 public | t3   | table | postgres
(3 rows)

postgres=# \c - u1
You are now connected to database &quot;postgres&quot; as user &quot;u1&quot;.
postgres=&gt; select * from t3;
 id | name
----+------
(0 rows)
postgres=&gt; select * from t2;
ERROR:  permission denied for relation t2
postgres=&gt; \c - postgres
You are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
postgres=# grant select on all tables in schema public to u1;
GRANT
postgres=# \c - u1
You are now connected to database &quot;postgres&quot; as user &quot;u1&quot;.
postgres=&gt; select * from t2;
 id | name
----+------
(0 rows)
</code></pre><p>grant是赋予用户schema下当前表的权限,alter default privileges是赋予用户schema下表的默认权限,这样以后新建表就不用再赋权限了。当我们创建只读账号的时候,需要执行grant和alter default privileges。</p>
<h2 id="4-创建可更新用户"><a href="#4-创建可更新用户" class="headerlink" title="4.创建可更新用户"></a>4.创建可更新用户</h2><h3 id="1-创建u2用户"><a href="#1-创建u2用户" class="headerlink" title="(1) 创建u2用户"></a>(1) 创建u2用户</h3><pre><code>postgres=# create role u2 with login password &apos;123456&apos;;
CREATE ROLE
</code></pre><h3 id="2-赋予更新权限"><a href="#2-赋予更新权限" class="headerlink" title="(2) 赋予更新权限"></a>(2) 赋予更新权限</h3><pre><code>postgres=# alter default privileges in schema public grant select,insert,update,delete on tables to u2;
ALTER DEFAULT PRIVILEGES
</code></pre><h3 id="3-创建表t4"><a href="#3-创建表t4" class="headerlink" title="(3) 创建表t4"></a>(3) 创建表t4</h3><pre><code>postgres=# create table t4 ( id serial, name varchar(64) );
CREATE TABLE
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | t1   | table | postgres
 public | t2   | table | postgres
 public | t3   | table | postgres
 public | t4   | table | postgres
(4 rows)
</code></pre><h3 id="4-查看权限"><a href="#4-查看权限" class="headerlink" title="(4) 查看权限"></a>(4) 查看权限</h3><pre><code>postgres=# \c - u2
You are now connected to database &quot;postgres&quot; as user &quot;u2&quot;.
postgres=&gt; insert into t4 values ( 1, &apos;aa&apos; );
INSERT 0 1
postgres=&gt; select * from t4;
 id | name
----+------
  1 | aa
(1 row)

postgres=&gt; update t4 set name = &apos;bb&apos; where id = 1;
UPDATE 1
postgres=&gt; select * from t4;
 id | name
----+------
  1 | bb
(1 row)

postgres=&gt; delete from t4 where id = 1;
DELETE 1
postgres=&gt; select * from t4;
 id | name
----+------
(0 rows)
</code></pre><p>可以正常增删改查</p>
<h3 id="5-序列的权限与解决办法"><a href="#5-序列的权限与解决办法" class="headerlink" title="(5) 序列的权限与解决办法"></a>(5) 序列的权限与解决办法</h3><p>在insert的时候,指定列插入,主键id是serial类型会默认走sequence的下一个值,但前面只赋予了表的权限,所以会出现下面的问题:</p>
<pre><code>postgres=&gt; insert into t4 ( name ) values ( &apos;aa&apos; );
ERROR:  permission denied for sequence t4_id_seq
</code></pre><p>解决方法就是再赋一次sequence的值就行了</p>
<pre><code>postgres=&gt; \c - postgres
You are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
postgres=# alter default privileges in schema public grant usage on sequences to u2;
ALTER DEFAULT PRIVILEGES
postgres=# create table t5 ( id serial, name varchar(64) );
CREATE TABLE
postgres=# \c - u2
You are now connected to database &quot;postgres&quot; as user &quot;u2&quot;.
postgres=&gt; insert into t5 ( name ) values ( &apos;cc&apos; );
INSERT 0 1
postgres=&gt; select * from t5;
 id | name
----+------
  1 | cc
(1 row)
</code></pre><h2 id="5-删除用户"><a href="#5-删除用户" class="headerlink" title="5.删除用户"></a>5.删除用户</h2><pre><code>postgres=&gt; \c - postgres
You are now connected to database &quot;postgres&quot; as user &quot;postgres&quot;.
postgres=# drop role u2;
ERROR:  role &quot;u2&quot; cannot be dropped because some objects depend on it
DETAIL:  privileges for table t5
privileges for sequence t5_id_seq
privileges for default privileges on new sequences belonging to role postgres in schema public
privileges for table t4
privileges for default privileges on new relations belonging to role postgres in schema public
</code></pre><p>当我们删除用户的时候,会提示有权限依赖,所以我们要删除这些权限</p>
<pre><code>postgres=# alter default privileges in schema public revoke usage on sequences from u2;
ALTER DEFAULT PRIVILEGES
postgres=# alter default privileges in schema public revoke select,insert,delete,update on tables from u2;
ALTER DEFAULT PRIVILEGES
postgres=# revoke select,insert,delete,update on all tables in schema public from u2;
REVOKE
postgres=# revoke usage on all sequences in schema public from u2;
REVOKE
postgres=# drop role u2;
DROP ROLE
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL日志换行处理]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E6%97%A5%E5%BF%97%E6%8D%A2%E8%A1%8C%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h2 id="PostgreSQL日志Line-Break处理"><a href="#PostgreSQL日志Line-Break处理" class="headerlink" title="PostgreSQL日志Line Break处理"></a>PostgreSQL日志Line Break处理</h2><pre><code>$ cat logfile | awk &apos;{if ($0 !~  &quot;^[\t]&quot;) printf(&quot;\n%s&quot; ,$0); else printf $0}&apos;
$ cat logfile | perl -e &apos;while(&lt;&gt;){ if($_ !~ /^\t/ig) { chomp; print &quot;\n&quot;,$_;} else {chomp; print;}}&apos;
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL日志使用rsyslog]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E6%97%A5%E5%BF%97%E4%BD%BF%E7%94%A8rsyslog/</url>
      <content type="html"><![CDATA[<h2 id="1-服务端"><a href="#1-服务端" class="headerlink" title="1. 服务端"></a>1. 服务端</h2><h3 id="1-安装syslog包"><a href="#1-安装syslog包" class="headerlink" title="(1) 安装syslog包"></a>(1) 安装syslog包</h3><pre><code>$ sudo yum install -y rsyslog
</code></pre><a id="more"></a>
<h3 id="2-创建目录"><a href="#2-创建目录" class="headerlink" title="(2) 创建目录"></a>(2) 创建目录</h3><pre><code>sudo mkdir -p /export/postgreslog/
</code></pre><h3 id="3-配置"><a href="#3-配置" class="headerlink" title="(3) 配置"></a>(3) 配置</h3><pre><code>$ sudo vim /etc/rsyslog.conf
    # Provides TCP syslog reception
    $ModLoad imtcp.so
    $InputTCPServerRun 514

    #### GLOBAL DIRECTIVES ####
    $MainMsgQueueSize 500000
    $MainMsgQueueDequeueBatchSize 128
    $MainMsgQueueDiscardMark 20000
    $MainMsgQueueHighWaterMark 16000
    $MainMsgQueueLowWaterMark 4000

    $EscapeControlCharactersOnReceive off

    # Use default timestamp format
    $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
    # log format
    $template pgFormat,&quot;%msg:R,ERE,2,DFLT:^ \[[0-9]+-[0-9]+\] (#011)?(.*)--end%\n&quot;
    # logfile name
    $template PostgresLogFile,&quot;/export/postgreslog/%hostname%_%programname%/%$year%-%$month%-%$day%/postgresql-%$year%-%$month%-%$day%-%$hour%.log&quot;

    # collect log of local0
    local0.* ?PostgresLogFile;pgFormat
</code></pre><h3 id="4-启动"><a href="#4-启动" class="headerlink" title="(4) 启动"></a>(4) 启动</h3><pre><code>$ sudo /etc/init.d/rsyslog restart
</code></pre><h2 id="2-客户端"><a href="#2-客户端" class="headerlink" title="2. 客户端"></a>2. 客户端</h2><h3 id="1-安装syslog包-1"><a href="#1-安装syslog包-1" class="headerlink" title="(1) 安装syslog包"></a>(1) 安装syslog包</h3><pre><code>$ sudo yum install -y rsyslog
</code></pre><h3 id="2-配置rsyslog"><a href="#2-配置rsyslog" class="headerlink" title="(2) 配置rsyslog"></a>(2) 配置rsyslog</h3><pre><code>sudo vim /etc/rsyslog.conf
    EscapeControlCharactersOnReceive off
    local0.* @@rsyslog_server:514
</code></pre><h3 id="3-启动rsyslog"><a href="#3-启动rsyslog" class="headerlink" title="(3) 启动rsyslog"></a>(3) 启动rsyslog</h3><pre><code>$ sudo /etc/init.d/rsyslog restart
</code></pre><h3 id="4-配置-postgresql-conf"><a href="#4-配置-postgresql-conf" class="headerlink" title="(4) 配置 postgresql.conf"></a>(4) 配置 postgresql.conf</h3><pre><code>$ vim postgresql.conf
    log_destination = &apos;syslog&apos;
    syslog_facility = &apos;LOCAL0&apos;
    syslog_ident = &apos;pg-test&apos;
</code></pre><h3 id="5-reload配置"><a href="#5-reload配置" class="headerlink" title="(5) reload配置"></a>(5) reload配置</h3><pre><code># select pg_reload_conf();
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL恢复到任意时间点]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E6%81%A2%E5%A4%8D%E5%88%B0%E4%BB%BB%E6%84%8F%E6%97%B6%E9%97%B4%E7%82%B9/</url>
      <content type="html"><![CDATA[<p>参考: <a href="http://www.postgresql.org/docs/9.4/static/recovery-config.html" target="_blank" rel="external">Recovery Configuration</a></p>
<h2 id="1-获取基础备份"><a href="#1-获取基础备份" class="headerlink" title="1. 获取基础备份"></a>1. 获取基础备份</h2><p>  你可以使用 pg_basebackup 或者其他工具获取Postgres的基础备份, 还要有基础备份到恢复时间点之前的 xlog 文件</p>
<a id="more"></a>
<h2 id="2-准备"><a href="#2-准备" class="headerlink" title="2. 准备"></a>2. 准备</h2><h3 id="recovery-conf"><a href="#recovery-conf" class="headerlink" title="recovery.conf"></a>recovery.conf</h3><pre><code>recovery_target_time=&apos;2015-05-08 12:00:00+8&apos;
pause_at_recovery_target=true
recovery_target_inclusive=false
restore_command=&apos;cp /xlog/%f %p&apos;
</code></pre><ul>
<li>注意:<br>你不能在 recovery.conf 中设置 standby_mode = ‘on’ , 并且 recovery_target_time 应该在基础备份的结束时间之后.</li>
</ul>
<h3 id="删除或者重命名-backup-label"><a href="#删除或者重命名-backup-label" class="headerlink" title="删除或者重命名 backup_label"></a>删除或者重命名 backup_label</h3><h3 id="启动-Postgres-实例"><a href="#启动-Postgres-实例" class="headerlink" title="启动 Postgres 实例"></a>启动 Postgres 实例</h3><p>  当 Postgres 实例到达你恢复的时间, recovery.conf 会变成 recovery.done.</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL多线程备份与恢复]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1.环境"></a>1.环境</h2><p>CentOS7下8核的cpu,数据库的配置是initdb后默认的:</p>
<pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-203 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
$  cat /proc/cpuinfo  | grep processor | wc -l
8
</code></pre><a id="more"></a>
<p>  $ psql –version<br>    psql (PostgreSQL) 9.4.5<br>    $ psql -U postgres -d test<br>    psql (9.4.5)<br>    Type “help” for help.</p>
<pre><code>test=# \dt+
                    List of relations
 Schema | Name  | Type  |  Owner   |  Size  | Description
--------+-------+-------+----------+--------+-------------
 public | test1 | table | postgres | 275 MB |
 public | test2 | table | postgres | 275 MB |
 public | test3 | table | postgres | 275 MB |
 public | test4 | table | postgres | 275 MB |
 public | test5 | table | postgres | 275 MB |
 public | test6 | table | postgres | 275 MB |
 public | test7 | table | postgres | 275 MB |
 public | test8 | table | postgres | 275 MB |
(8 rows)

test=# \q
</code></pre><h2 id="2-参数"><a href="#2-参数" class="headerlink" title="2.参数"></a>2.参数</h2><h3 id="1-pg-dump"><a href="#1-pg-dump" class="headerlink" title="(1) pg_dump"></a>(1) pg_dump</h3><pre><code>$ pg_dump --help
pg_dump dumps a database as a text file or to other formats.

Usage:
  pg_dump [OPTION]... [DBNAME]

General options:
  -f, --file=FILENAME          output file or directory name
  -F, --format=c|d|t|p         output file format (custom, directory, tar,
                               plain text (default))
  -j, --jobs=NUM               use this many parallel jobs to dump
  -v, --verbose                verbose mode
  -V, --version                output version information, then exit
  -Z, --compress=0-9           compression level for compressed formats
  --lock-wait-timeout=TIMEOUT  fail after waiting TIMEOUT for a table lock
  -?, --help                   show this help, then exit

Options controlling the output content:
  -a, --data-only              dump only the data, not the schema
  -b, --blobs                  include large objects in dump
  -c, --clean                  clean (drop) database objects before recreating
  -C, --create                 include commands to create database in dump
  -E, --encoding=ENCODING      dump the data in encoding ENCODING
  -n, --schema=SCHEMA          dump the named schema(s) only
  -N, --exclude-schema=SCHEMA  do NOT dump the named schema(s)
  -o, --oids                   include OIDs in dump
  -O, --no-owner               skip restoration of object ownership in
                               plain-text format
  -s, --schema-only            dump only the schema, no data
  -S, --superuser=NAME         superuser user name to use in plain-text format
  -t, --table=TABLE            dump the named table(s) only
  -T, --exclude-table=TABLE    do NOT dump the named table(s)
  -x, --no-privileges          do not dump privileges (grant/revoke)
  --binary-upgrade             for use by upgrade utilities only
  --column-inserts             dump data as INSERT commands with column names
  --disable-dollar-quoting     disable dollar quoting, use SQL standard quoting
  --disable-triggers           disable triggers during data-only restore
  --exclude-table-data=TABLE   do NOT dump data for the named table(s)
  --if-exists                  use IF EXISTS when dropping objects
  --inserts                    dump data as INSERT commands, rather than COPY
  --no-security-labels         do not dump security label assignments
  --no-synchronized-snapshots  do not use synchronized snapshots in parallel jobs
  --no-tablespaces             do not dump tablespace assignments
  --no-unlogged-table-data     do not dump unlogged table data
  --quote-all-identifiers      quote all identifiers, even if not key words
  --section=SECTION            dump named section (pre-data, data, or post-data)
  --serializable-deferrable    wait until the dump can run without anomalies
  --use-set-session-authorization
                               use SET SESSION AUTHORIZATION commands instead of
                               ALTER OWNER commands to set ownership

Connection options:
  -d, --dbname=DBNAME      database to dump
  -h, --host=HOSTNAME      database server host or socket directory
  -p, --port=PORT          database server port number
  -U, --username=NAME      connect as specified database user
  -w, --no-password        never prompt for password
  -W, --password           force password prompt (should happen automatically)
  --role=ROLENAME          do SET ROLE before dump

If no database name is supplied, then the PGDATABASE environment
variable value is used.

Report bugs to &lt;pgsql-bugs@postgresql.org&gt;.
</code></pre><h3 id="2-pg-restore"><a href="#2-pg-restore" class="headerlink" title="(2) pg_restore"></a>(2) pg_restore</h3><pre><code>$ pg_restore --help
pg_restore restores a PostgreSQL database from an archive created by pg_dump.

Usage:
  pg_restore [OPTION]... [FILE]

General options:
  -d, --dbname=NAME        connect to database name
  -f, --file=FILENAME      output file name
  -F, --format=c|d|t       backup file format (should be automatic)
  -l, --list               print summarized TOC of the archive
  -v, --verbose            verbose mode
  -V, --version            output version information, then exit
  -?, --help               show this help, then exit

Options controlling the restore:
  -a, --data-only              restore only the data, no schema
  -c, --clean                  clean (drop) database objects before recreating
  -C, --create                 create the target database
  -e, --exit-on-error          exit on error, default is to continue
  -I, --index=NAME             restore named index
  -j, --jobs=NUM               use this many parallel jobs to restore
  -L, --use-list=FILENAME      use table of contents from this file for
                               selecting/ordering output
  -n, --schema=NAME            restore only objects in this schema
  -O, --no-owner               skip restoration of object ownership
  -P, --function=NAME(args)    restore named function
  -s, --schema-only            restore only the schema, no data
  -S, --superuser=NAME         superuser user name to use for disabling triggers
  -t, --table=NAME             restore named table
  -T, --trigger=NAME           restore named trigger
  -x, --no-privileges          skip restoration of access privileges (grant/revoke)
  -1, --single-transaction     restore as a single transaction
  --disable-triggers           disable triggers during data-only restore
  --if-exists                  use IF EXISTS when dropping objects
  --no-data-for-failed-tables  do not restore data of tables that could not be
                               created
  --no-security-labels         do not restore security labels
  --no-tablespaces             do not restore tablespace assignments
  --section=SECTION            restore named section (pre-data, data, or post-data)
  --use-set-session-authorization
                               use SET SESSION AUTHORIZATION commands instead of
                               ALTER OWNER commands to set ownership

Connection options:
  -h, --host=HOSTNAME      database server host or socket directory
  -p, --port=PORT          database server port number
  -U, --username=NAME      connect as specified database user
  -w, --no-password        never prompt for password
  -W, --password           force password prompt (should happen automatically)
  --role=ROLENAME          do SET ROLE before restore

The options -I, -n, -P, -t, -T, and --section can be combined and specified
multiple times to select multiple objects.

If no input file name is supplied, then standard input is used.

Report bugs to &lt;pgsql-bugs@postgresql.org&gt;.
</code></pre><p>可以看到pg_dump和pg_restore都有-j, –jobs=NUM参数,可以使用多线程并行。这个功能是从9.3开始支持的,具体相见release note:<a href="http://www.postgresql.org/docs/9.4/interactive/release-9-3.html" target="_blank" rel="external">http://www.postgresql.org/docs/9.4/interactive/release-9-3.html</a></p>
<h2 id="3-pg-dump备份数据对比"><a href="#3-pg-dump备份数据对比" class="headerlink" title="3.pg_dump备份数据对比"></a>3.pg_dump备份数据对比</h2><h3 id="1-单线程"><a href="#1-单线程" class="headerlink" title="(1) 单线程"></a>(1) 单线程</h3><pre><code>$ rm -fr test; mkdir -p test; date &apos;+%F %T&apos;; pg_dump -U postgres -d test -F d -f test; date &apos;+%F %T&apos;
2015-11-05 17:10:04
2015-11-05 17:10:44
$  ps aux | grep pg_dump | grep -v grep
wenhang+ 50634  116  0.0 118424  2996 pts/1    R+   17:10   0:02 pg_dump -U postgres -d test -F d -f test
</code></pre><p>这条命令的意思是先删除test文件夹；再创建test文件夹；打印当前时间；使用pg_dump备份数据库到test目录,-F d是指定备份格式是目录；再打印当前时间。这两个时间差就是备份需要的时间1分40秒。</p>
<h3 id="2-多线程"><a href="#2-多线程" class="headerlink" title="(2) 多线程"></a>(2) 多线程</h3><pre><code>$ rm -fr test; mkdir -p test; date &apos;+%F %T&apos;; pg_dump -U postgres -d test -j 8 -F d -f test; date &apos;+%F %T&apos;
2015-11-05 17:09:00
2015-11-05 17:09:14
$  ps aux | grep pg_dump | grep -v grep
wenhang+ 50546  0.0  0.0 118100  2668 pts/1    S+   17:09   0:00 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50548 85.9  0.0 120516  4136 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50549 84.8  0.0 120516  4200 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50550 86.3  0.0 120516  4264 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50551 84.8  0.0 120516  4324 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50552 87.3  0.0 120516  4344 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50553 86.8  0.0 121992  5820 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50554 86.5  0.0 120516  4392 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
wenhang+ 50556 85.7  0.0 120516  4208 pts/1    R+   17:09   0:15 pg_dump -U postgres -d test -j 8 -F d -f test
</code></pre><p>这里加了-j 8参数,使用8线程并行,只用了14秒,可见相差了很多倍。</p>
<p>使用目录格式备份的原因是并行备份只支持目录格式,否则会出现如下错误:</p>
<pre><code>pg_dump: parallel backup only supported by the directory format
</code></pre><h2 id="4-pg-restore恢复数据对比"><a href="#4-pg-restore恢复数据对比" class="headerlink" title="4.pg_restore恢复数据对比"></a>4.pg_restore恢复数据对比</h2><h3 id="1-单线程-1"><a href="#1-单线程-1" class="headerlink" title="(1) 单线程"></a>(1) 单线程</h3><pre><code>$ psql -U postgres -c &apos;drop database if exists test;&apos;; psql -U postgres -c &apos;create database test;&apos;; date &apos;+%F %T&apos;; pg_restore -U postgres -d test -F d test ; date &apos;+%F %T&apos;
DROP DATABASE
CREATE DATABASE
2015-11-05 17:11:36
2015-11-05 17:13:20
$ ps aux | grep pg_restore | grep -v grep
wenhang+ 50758 26.0  0.0 116740  1200 pts/1    S+   17:11   0:00 pg_restore -U postgres -d test -F d test
</code></pre><p>这条命令的意思是先删除数据库test；再创建数据库test；打印当前时间；从test目录恢复数据库test；-F d是指定恢复格式是目录；再打印当前时间。这两个时间差就是恢复需要的时间差1分44秒。</p>
<h3 id="2-多线程-1"><a href="#2-多线程-1" class="headerlink" title="(2) 多线程"></a>(2) 多线程</h3><pre><code>$ psql -U postgres -c &apos;drop database if exists test;&apos;; psql -U postgres -c &apos;create database test;&apos;; date &apos;+%F %T&apos;; pg_restore -U postgres -d test -j 8 -F d test ; date &apos;+%F %T&apos;
DROP DATABASE
CREATE DATABASE
2015-11-05 17:26:22
2015-11-05 17:26:56
$ ps aux | grep pg_restore | grep -v grep
wenhang+ 53848  0.0  0.0 116600  1264 pts/1    S+   17:26   0:00 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53850 11.7  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53851 11.9  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53852 12.1  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53853 12.2  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53854 12.0  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53855 12.6  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53857 11.6  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
wenhang+ 53858 12.2  0.0 116744   976 pts/1    S+   17:26   0:02 pg_restore -U postgres -d test -j 8 -F d test
</code></pre><p>可见多线程恢复也快了不少,如果io更快的话,这个提升会更明显</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL备份之pg_rman]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%A4%87%E4%BB%BD%E4%B9%8Bpg_rman/</url>
      <content type="html"><![CDATA[<h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h2><p>PostgreSQL版本是9.4.4</p>
<pre><code># git clone -b REL9_4_STABLE https://github.com/ossc-db/pg_rman.git
Initialized empty Git repository in /opt/pg_rman/.git/
remote: Counting objects: 1939, done.
remote: Total 1939 (delta 0), reused 0 (delta 0), pack-reused 1939
Receiving objects: 100% (1939/1939), 702.67 KiB | 311 KiB/s, done.
Resolving deltas: 100% (1393/1393), done.
# yum install -y pam-devel readline-devel zlib-devel
# make
# make install
/bin/mkdir -p &apos;/opt/pg94/bin&apos;
/usr/bin/install -c  pg_rman &apos;/opt/pg94/bin&apos;
# pg_rman --version
pg_rman 1.3.1
</code></pre><a id="more"></a>
<h2 id="2-使用"><a href="#2-使用" class="headerlink" title="2.使用"></a>2.使用</h2><h3 id="1-准备"><a href="#1-准备" class="headerlink" title="(1) 准备"></a>(1) 准备</h3><p>首先需要初始化备份目录</p>
<pre><code># mkdir -p /data/pg_backup
# mkdir -p /data/test_xlog_94/fulltime
# pg_rman  init --backup-path=/data/pg_backup/
WARNING: ARCLOG_PATH is not set yet
DETAIL: The archive_command is not set in postgresql.conf.
HINT: Please set ARCLOG_PATH in pg_rman.ini or environmental variable.
# vim /data/pg_backup/pg_rman.ini
ARCLOG_PATH=&apos;/data/test_xlog_94/fulltime&apos;
</code></pre><h3 id="2-全备份"><a href="#2-全备份" class="headerlink" title="(2) 全备份"></a>(2) 全备份</h3><p>在master节点上制作一个全备,在从库上需要指定–host, –standby-host=172.17.5.47 –standby-port=5432</p>
<pre><code># pg_rman backup --backup-mode=full --backup-path=/data/pg_backup/ --pgdata=/data/test_pgdata_94/ -U postgres -d postgres
INFO: copying database files
NOTICE:  pg_stop_backup complete, all required WAL segments have been archived
INFO: copying archived WAL files
INFO: backup complete
HINT: Please execute &apos;pg_rman validate&apos; to verify the files are correctly copied.
# pg_rman show detail --backup-path=/data/pg_backup/
============================================================================================================
 StartTime           Mode  Duration    Data  ArcLog  SrvLog   Total  Compressed  CurTLI  ParentTLI  Status
============================================================================================================
2015-09-07 02:52:22  FULL        0m   422MB   106kB    ----   394MB       false       1          0  OK
</code></pre><h3 id="3-使用全备进行恢复"><a href="#3-使用全备进行恢复" class="headerlink" title="(3) 使用全备进行恢复"></a>(3) 使用全备进行恢复</h3><pre><code># mkdir restore_test
# pg_rman restore --backup-path=/data/pg_backup/ --pgdata=/data/restore_test/
WARNING: could not open pg_controldata file &quot;/data/restore_test//global/pg_control&quot;: No such file or directory
WARNING: could not open pg_controldata file &quot;/data/restore_test//global/pg_control&quot;: No such file or directory
INFO: backup &quot;2015-09-07 02:52:22&quot; is valid
INFO: the recovery target timeline ID is not given
INFO: use timeline ID of latest full backup as recovery target: 1
INFO: calculating timeline branches to be used to recovery target point
INFO: searching latest full backup which can be used as restore start point
INFO: found the full backup can be used as base in recovery: &quot;2015-09-07 02:52:22&quot;
INFO: copying online WAL files and server log files
INFO: clearing restore destination
INFO: validate: &quot;2015-09-07 02:52:22&quot; backup and archive log files by SIZE
INFO: backup &quot;2015-09-07 02:52:22&quot; is valid
INFO: restoring database files from the full mode backup &quot;2015-09-07 02:52:22&quot;
INFO: searching incremental backup to be restored
INFO: searching backup which contained archived WAL files to be restored
INFO: backup &quot;2015-09-07 02:52:22&quot; is valid
INFO: restoring WAL files from backup &quot;2015-09-07 02:52:22&quot;
INFO: restoring online WAL files and server log files
INFO: generating recovery.conf
INFO: restore complete
HINT: Recovery will start automatically when the PostgreSQL server is started.
</code></pre><p>由于是用root用户进行备份和恢复的,目录权限也是root,需要修改为postgres</p>
<pre><code># ls -ld /data/restore_test/
drwxr-xr-x. 18 root root 4096 Sep  7 03:00 restore_test/
# cat /data/restore_test/recovery.conf
# recovery.conf generated by pg_rman 1.3.1
restore_command = &apos;cp /data/test_pgdata_94/pg_xlog/%f %p&apos;
recovery_target_timeline = &apos;1&apos;
# chown -R postgres:postgres /data/restore_test/
# chmod 0700 /data/restore_test/
# ls -ld /data/restore_test/
drwx------. 18 postgres postgres 4096 Sep  7 03:05 /data/restore_test/
</code></pre><p>将端口修改为5433,并启动恢复的数据库</p>
<pre><code>$ pg_ctl -D /data/restore_test/ start
server starting
[postgres@QA-5-45 data]$ [    2015-09-07 07:05:40.291 UTC 31889 55ed3744.7c91 1 0]LOG:  redirecting log output to logging collector process
[    2015-09-07 07:05:40.291 UTC 31889 55ed3744.7c91 2 0]HINT:  Future log output will appear in directory &quot;pg_log&quot;.
$ psql -p 5433
psql (9.4.4)
Type &quot;help&quot; for help.
</code></pre><p>恢复到指定时间点的话,执行restore的时候使用如下参数:</p>
<pre><code>--recovery-target-time    time stamp up to which recovery will proceed
--recovery-target-xid     transaction ID up to which recovery will proceed
--recovery-target-inclusive whether we stop just after the recovery target
--recovery-target-timeline  recovering into a particular timeline
</code></pre><p>会将相应的参数写到recovery.conf里面</p>
<h3 id="5-archive模式进行增量备份"><a href="#5-archive模式进行增量备份" class="headerlink" title="(5) archive模式进行增量备份"></a>(5) archive模式进行增量备份</h3><p>首先需要制作全备（如上）,并对全备进行认证</p>
<pre><code># pg_rman validate --backup-path=/data/pg_backup/
INFO: validate: &quot;2015-09-07 02:52:22&quot; backup and archive log files by CRC
INFO: backup &quot;2015-09-07 02:52:22&quot; is valid
</code></pre><p>在进行增量备份</p>
<pre><code># pg_rman backup --backup-mode=archive --backup-path=/data/pg_backup/ --pgdata=/data/test_pgdata_94/ -U postgres -d postgres --progress
INFO: copying archived WAL files
Processed 7 of 7 files, skipped 7
INFO: backup complete
HINT: Please execute &apos;pg_rman validate&apos; to verify the files are correctly copied.
# pg_rman show detail --backup-path=/data/pg_backup/
============================================================================================================
 StartTime           Mode  Duration    Data  ArcLog  SrvLog   Total  Compressed  CurTLI  ParentTLI  Status
============================================================================================================
2015-09-07 02:57:07  ARCH        0m    ----      0B    ----      0B       false       1          0  DONE
</code></pre><p>参考:<a href="http://ossc-db.github.io/pg_rman/index.html" target="_blank" rel="external">http://ossc-db.github.io/pg_rman/index.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL备份之pg_basebackup]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%A4%87%E4%BB%BD%E4%B9%8Bpg_basebackup/</url>
      <content type="html"><![CDATA[<h2 id="1-准备"><a href="#1-准备" class="headerlink" title="1.准备"></a>1.准备</h2><p>连接上master节点创建replication用户</p>
<pre><code># CREATE ROLE replicator with login replication password 123456；
</code></pre><a id="more"></a>
<p>修改白名单pg_hba.conf</p>
<pre><code># host    replication    all        xxxxx/32    md5
</code></pre><h2 id="2-制作基础备份"><a href="#2-制作基础备份" class="headerlink" title="2.制作基础备份"></a>2.制作基础备份</h2><pre><code># pg_basebackup --format=tar \ #使用打包方式备份
--xlog-method=fetch \    #获取wal文件方式,等备份结束后把备份期间所有的wal备份
--compress=1 \    #压缩等级
--checkpoint=fast \    #执行检查点的方式
--label=backup \    #标签
--pgdata=/data/backup/ #备份目录
--progress \    #打印过程
--verbose \    #打印详细信息
--host=&apos;172.17.5.45&apos; \    #master节点ip
--port=5432 \    #master节点端口
--user=postgres    #连接master节点的用户
Password:
transaction log start point: 2/34000028 on timeline 1
414272/414272 kB (100%), 3/3 tablespaces
transaction log end point: 2/34000128
pg_basebackup: base backup completed
</code></pre><p>备份默认会占用一个max_wal_senders连接,需要设置大一点,还可以使用下面的参数限制传输速度</p>
<pre><code>-r, --max-rate=RATE    maximum transfer rate to transfer data directory
                        (in kB/s, or use suffix &quot;k&quot; or &quot;M&quot;)

# ll backup/
total 123196
-rw-r--r--. 1 root root       129 Sep  8 01:21 25375.tar.gz
-rw-r--r--. 1 root root       128 Sep  8 01:21 25376.tar.gz
-rw-r--r--. 1 root root 126141406 Sep  8 01:22 base.tar.gz
</code></pre><p>base.tar.gz是数据目录的备份,其他两个是表空间的备份</p>
<h2 id="3-恢复"><a href="#3-恢复" class="headerlink" title="3.恢复"></a>3.恢复</h2><h3 id="1-解压缩"><a href="#1-解压缩" class="headerlink" title="(1) 解压缩"></a>(1) 解压缩</h3><pre><code># cd /data/backup/
# mkdir pgdata 25375 25376
# tar zxf base.tar.gz -C pgdata/
# ls pgdata/
backup_label  global   pg_dynshmem  pg_ident.conf  pg_logical    pg_notify    pg_serial     pg_stat      pg_subtrans  pg_twophase  pg_xlog               postgresql.conf
base          pg_clog  pg_hba.conf  pg_log         pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    PG_VERSION   postgresql.auto.conf  recovery.conf
# tar zxf 25375.tar.gz -C 25375
# tar zxf 25376.tar.gz -C 25376
# ls
25375  25375.tar.gz  25376  25376.tar.gz  base.tar.gz  pgdata
</code></pre><h3 id="2-修改表空间软连接"><a href="#2-修改表空间软连接" class="headerlink" title="(2) 修改表空间软连接"></a>(2) 修改表空间软连接</h3><pre><code># ln -sf /data/backup/25375 pgdata/pg_tblspc/25375 [root@node2 backup]# ln -sf /data/backup/25376 pgdata/pg_tblspc/25376                     [root@node2 backup]# ll pgdata/pg_tblspc/ total 0 lrwxrwxrwx. 1 root root 18 Sep  8 01:35 25375 -&gt; /data/backup/25375 lrwxrwxrwx. 1 root root 18 Sep  8 01:35 25376 -&gt; /data/backup/25376
</code></pre><h3 id="3-修改数据目录权限"><a href="#3-修改数据目录权限" class="headerlink" title="(3) 修改数据目录权限"></a>(3) 修改数据目录权限</h3><pre><code># chown -R postgres:postgres pgdata/ 25375 25376
# chmod 0700 pgdata/
# ll
total 123208
drwxr-xr-x.  3 postgres postgres      4096 Sep  8 01:32 25375
-rw-r--r--.  1 root     root           129 Sep  8 01:21 25375.tar.gz
drwxr-xr-x.  3 postgres postgres      4096 Sep  8 01:33 25376
-rw-r--r--.  1 root     root           128 Sep  8 01:21 25376.tar.gz
-rw-r--r--.  1 root     root     126141406 Sep  8 01:22 base.tar.gz
drwx------. 19 postgres postgres      4096 Sep  8 01:31 pgdata
</code></pre><h3 id="4-启动"><a href="#4-启动" class="headerlink" title="(4) 启动"></a>(4) 启动</h3><pre><code># su postgres
$ pg_ctl -D pgdata/ start
server starting
 [    2015-09-08 05:40:04.510 UTC 11107 55ee74b4.2b63 1 0 ]LOG:  redirecting log output to logging collector process
[    2015-09-08 05:40:04.511 UTC 11107 55ee74b4.2b63 2 0 ]HINT:  Future log output will appear in directory &quot;pg_log&quot;.
$ psql
psql (9.4.4)
Type &quot;help&quot; for help.

postgres=# \db+
                                  List of tablespaces
    Name    |  Owner   |      Location      | Access privileges | Options | Description
------------+----------+--------------------+-------------------+---------+-------------
 pg_default | postgres |                    |                   |         |
 pg_global  | postgres |                    |                   |         |
 tblspc1    | postgres | /data/backup/25375 |                   |         |
 tblspc2    | postgres | /data/backup/25376 |                   |         |
(4 rows)
</code></pre><h2 id="4-制作从库"><a href="#4-制作从库" class="headerlink" title="4.制作从库"></a>4.制作从库</h2><h3 id="1-获取备份文件"><a href="#1-获取备份文件" class="headerlink" title="(1) 获取备份文件"></a>(1) 获取备份文件</h3><pre><code># pg_basebackup --format=plain \    #使用目录方式
--write-recovery-conf \    #生成简单recovery.conf
--xlog-method=stream \    #使用stream的方式备份wal文件,会再占用一个max_wal_sender,wal文件一生成就会备份
--checkpoint=fast \
--label=backup \
--pgdata=/data/test_pgdata_94 \
--progress \
--verbose \
--host=&apos;172.17.5.45&apos; \
--port=5432 \
--user=postgres
Password:
transaction log start point: 2/38000028 on timeline 1
pg_basebackup: starting background WAL receiver
397887/397887 kB (100%), 3/3 tablespaces
transaction log end point: 2/380000F0
pg_basebackup: waiting for background process to finish streaming ...
pg_basebackup: base backup completed
</code></pre><h3 id="2-修改目录权限"><a href="#2-修改目录权限" class="headerlink" title="(2) 修改目录权限"></a>(2) 修改目录权限</h3><pre><code># chown -R postgres:postgres test_pgdata_94 tblspc1/ tblspc2/
# chmod 0700 test_pgdata_94
# ll test_pgdata_94
total 104
-rw-------. 1 postgres postgres  189 Sep  8 01:43 backup_label
drwx------. 6 postgres postgres 4096 Sep  8 01:43 base
drwx------. 2 postgres postgres 4096 Sep  8 01:43 global
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_clog
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_dynshmem
-rw-------. 1 postgres postgres 4581 Sep  8 01:43 pg_hba.conf
-rw-------. 1 postgres postgres 1636 Sep  8 01:43 pg_ident.conf
drwxr-xr-x. 2 postgres postgres 4096 Sep  8 01:43 pg_log
drwx------. 4 postgres postgres 4096 Sep  8 01:43 pg_logical
drwx------. 4 postgres postgres 4096 Sep  8 01:43 pg_multixact
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_notify
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_replslot
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_serial
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_snapshots
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_stat
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_stat_tmp
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_subtrans
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_tblspc
drwx------. 2 postgres postgres 4096 Sep  8 01:43 pg_twophase
-rw-------. 1 postgres postgres    4 Sep  8 01:43 PG_VERSION
drwx------. 3 postgres postgres 4096 Sep  8 01:43 pg_xlog
-rw-------. 1 postgres postgres   88 Sep  8 01:43 postgresql.auto.conf
-rw-------. 1 postgres postgres 4676 Sep  8 01:43 postgresql.conf
-rw-r--r--. 1 postgres postgres  131 Sep  8 01:43 recovery.conf
# ll tblspc1/
total 4
drwx------. 2 postgres postgres 4096 Sep  8 01:43 PG_9.4_201409291
# ll tblspc2
total 4
drwx------. 2 postgres postgres 4096 Sep  8 01:43 PG_9.4_201409291
</code></pre><h3 id="3-启动"><a href="#3-启动" class="headerlink" title="(3) 启动"></a>(3) 启动</h3><pre><code># su postgres
$ pg_ctl -D test_pgdata_94 start
server starting
$ [    2015-09-08 05:49:55.383 UTC 12100 55ee7703.2f44 1 0 ]LOG:  redirecting log output to logging collector process
[    2015-09-08 05:49:55.383 UTC 12100 55ee7703.2f44 2 0 ]HINT:  Future log output will appear in directory &quot;pg_log&quot;.

$ psql
psql (9.4.4)
Type &quot;help&quot; for help.

postgres=# \db+
                                List of tablespaces
    Name    |  Owner   |   Location    | Access privileges | Options | Description
------------+----------+---------------+-------------------+---------+-------------
 pg_default | postgres |               |                   |         |
 pg_global  | postgres |               |                   |         |
 tblspc1    | postgres | /data/tblspc1 |                   |         |
 tblspc2    | postgres | /data/tblspc2 |                   |         |
(4 rows)
</code></pre><p>参考:<a href="http://www.postgresql.org/docs/9.4/static/app-pgbasebackup.html" target="_blank" rel="external">http://www.postgresql.org/docs/9.4/static/app-pgbasebackup.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL备份之手工备份]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%A4%87%E4%BB%BD%E4%B9%8B%E6%89%8B%E5%B7%A5%E5%A4%87%E4%BB%BD/</url>
      <content type="html"><![CDATA[<h2 id="1-备份"><a href="#1-备份" class="headerlink" title="1. 备份"></a>1. 备份</h2><h3 id="1-需要保证-archive-mode-on-和-archive-command-是有效的"><a href="#1-需要保证-archive-mode-on-和-archive-command-是有效的" class="headerlink" title="(1) 需要保证 archive_mode = on 和 archive_command 是有效的"></a>(1) 需要保证 archive_mode = on 和 archive_command 是有效的</h3><h3 id="2-在-master-节点上连接上数据库并执行"><a href="#2-在-master-节点上连接上数据库并执行" class="headerlink" title="(2) 在 master 节点上连接上数据库并执行"></a>(2) 在 master 节点上连接上数据库并执行</h3><pre><code>SELECT pg_start_backup(&apos;label&apos;, true);
</code></pre><p>pg_start_backup第二个参数设置为true的好处是备份开始会执行一个检查点,设置为true能尽快的完成检查点,并且能减少备份期间产生的wal文件,但是会对查询有影响,默认设置的checkpoint_completion_target的一半的时间,半夜的备份建议打开</p>
<a id="more"></a>
<h3 id="3-把物理文件复制到别的地方-可进行压缩或者使用nc-rsync等其他工具传送到其他地方"><a href="#3-把物理文件复制到别的地方-可进行压缩或者使用nc-rsync等其他工具传送到其他地方" class="headerlink" title="(3) 把物理文件复制到别的地方,可进行压缩或者使用nc,rsync等其他工具传送到其他地方"></a>(3) 把物理文件复制到别的地方,可进行压缩或者使用nc,rsync等其他工具传送到其他地方</h3><p>排除掉一些文件:</p>
<p>1） pg_log/下的所有日志文件,也可以一起备份</p>
<p>2） pg_xlog/下的所有wal文件</p>
<p>3） pg_xlog/archive_status/*下的所有文件</p>
<p>4） recovery.conf如果在从库下备份需要排除</p>
<p>5） postmaster.pid, postmaster.opts</p>
<h3 id="4-复制完后-连接数据库执行"><a href="#4-复制完后-连接数据库执行" class="headerlink" title="(4) 复制完后,连接数据库执行"></a>(4) 复制完后,连接数据库执行</h3><pre><code>SELECT pg_stop_backup();
</code></pre><h2 id="2-恢复"><a href="#2-恢复" class="headerlink" title="2.恢复"></a>2.恢复</h2><h3 id="1-将备份转化（解压或者复制）到要恢复的数据目录"><a href="#1-将备份转化（解压或者复制）到要恢复的数据目录" class="headerlink" title="(1) 将备份转化（解压或者复制）到要恢复的数据目录"></a>(1) 将备份转化（解压或者复制）到要恢复的数据目录</h3><h3 id="2-准备recovery-conf"><a href="#2-准备recovery-conf" class="headerlink" title="(2) 准备recovery.conf"></a>(2) 准备recovery.conf</h3><pre><code>recovery_target_time=&apos;2015-08-04 12:00:00+8&apos; #需要恢复到的时间点
pause_at_recovery_target=true                #到恢复目标后暂停
recovery_target_inclusive=false              #到恢复目标后是否停止恢复,设置为true会把recovery.conf重命名为recovery.done
restore_command=&apos;cp /data/xlog/%f %p&apos;        #wal文件归档的位置,必须指定
</code></pre><p>不能设置standby_mode = ‘on’ ,recovery_target_time需要在基础备份之后,还可以恢复到指定事务id和指定备份位置,详见:<a href="http://www.postgresql.org/docs/9.4/static/recovery-target-settings.html#RECOVERY-TARGET-INCLUSIVE" target="_blank" rel="external">http://www.postgresql.org/docs/9.4/static/recovery-target-settings.html#RECOVERY-TARGET-INCLUSIVE</a></p>
<h3 id="3-启动备份数据库"><a href="#3-启动备份数据库" class="headerlink" title="(3) 启动备份数据库"></a>(3) 启动备份数据库</h3><pre><code>pg_ctl -D /data/restore_data start
</code></pre><p>参考:<a href="http://www.postgresql.org/docs/9.4/interactive/continuous-archiving.html" target="_blank" rel="external">http://www.postgresql.org/docs/9.4/interactive/continuous-archiving.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL备份之omniPITR]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%A4%87%E4%BB%BD%E4%B9%8BomniPITR/</url>
      <content type="html"><![CDATA[<h2 id="1-准备"><a href="#1-准备" class="headerlink" title="1.准备"></a>1.准备</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="(1) 安装"></a>(1) 安装</h3><p>omnipitr是用perl写的,直接下载下来就可以用了</p>
<pre><code># git clone https://github.com/omniti-labs/omnipitr.git /opt/omnipitr/
Initialized empty Git repository in /opt/omnipitr/.git/
remote: Counting objects: 2627, done.
remote: Total 2627 (delta 0), reused 0 (delta 0), pack-reused 2627
Receiving objects: 100% (2627/2627), 742.71 KiB | 28 KiB/s, done.
Resolving deltas: 100% (1287/1287), done.
</code></pre><a id="more"></a>
<h3 id="2-检查"><a href="#2-检查" class="headerlink" title="(2) 检查"></a>(2) 检查</h3><pre><code># /opt/omnipitr/bin/sanity-check.sh
Checking:
- /opt/omnipitr/bin
- /opt/omnipitr/lib
9 programs, 31 libraries.
Tar version
All checked, and looks ok.
</code></pre><h3 id="3-创建目录"><a href="#3-创建目录" class="headerlink" title="(3) 创建目录"></a>(3) 创建目录</h3><pre><code># mkdir -p /data/omnipitr/test_94/{log,state,tmp}
# chown -R postgres:postgres /data/omnipitr
</code></pre><h2 id="2-备份"><a href="#2-备份" class="headerlink" title="2.备份"></a>2.备份</h2><h3 id="1-从master节点上备份"><a href="#1-从master节点上备份" class="headerlink" title="(1) 从master节点上备份"></a>(1) 从master节点上备份</h3><p>1）修改archive_command</p>
<pre><code>test ! -f /data/omnipitr/test_94/tmp/backup/%f &amp;&amp; cp %p /data/omnipitr/test_94/tmp/backup/%f
</code></pre><p>如果已经将wal归档到其他地方,不需要备份wal文件,就不需要配置这个</p>
<p>2）备份到本地</p>
<pre><code>$ /opt/omnipitr/bin/omnipitr-backup-master --host=localhost \    #master节点ip
--username=postgres \    #连接master节点的用户
--port=5432 \    #master节点的端口
--data-dir=/data/postgres/data/test_94/ \    #master节点的数据目录
--dst-local gzip=/data/postgres/backup/test_94/ \    #本地备份目录
--xlogs=/data/omnipitr/test_94/tmp/backup \    #用于归档xlog的目录,执行命令前不能存在,如果不需要备份xlog,替换成--skip-xlogs
--temp-dir=/data/omnipitr/test_94/tmp/ \    #创建临时文件的目录,可以不指定,默认是/tmp
--pid-file=/data/omnipitr/test_94/state/base_backup.pid \    #进程文件,保证同时只有一个备份在执行
--log=/data/omnipitr/test_94/log/base_backup.log \    #日志输出文件
--verbose    #打印详细信息
</code></pre><p>3）备份的详细日志信息</p>
<pre><code>2015-09-09 04:50:54.185227 -0400 : 9766 : omnipitr-backup-master : LOG : Called with parameters: --host=localhost --username=postgres --port=5432 --data-dir=/data/postgres/data/test_94/ --dst-local gzip=/data/postgres/backup/test_94/ --xlogs=/data/omnipitr/test_94/tmp/backup --temp-dir=/data/omnipitr/test_94/tmp/ --pid-file=/data/omnipitr/test_94/state/base_backup.pid --log=/data/omnipitr/test_94/log/base_backup.log --verbose
2015-09-09 04:50:54.363777 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [SELECT w, pg_xlogfile_name(w) from (select pg_start_backup(&apos;omnipitr&apos;) as w ) as x] took: 0.175s
2015-09-09 04:50:54.366482 -0400 : 9766 : omnipitr-backup-master : LOG : pg_start_backup(&apos;omnipitr&apos;) returned 1/3C000028|00000001000000010000003C.
2015-09-09 04:50:54.368554 -0400 : 9766 : omnipitr-backup-master : LOG : Script to make tarballs:
2015-09-09 04:50:54.368554 -0400 : 9766 : omnipitr-backup-master : LOG : mkfifo \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0
2015-09-09 04:50:54.368554 -0400 : 9766 : omnipitr-backup-master : LOG : nice gzip \-\-stdout \- &lt; \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0 &gt; \/data\/postgres\/backup\/test_94\/QA\-5\-45\-data\-2015\-09\-09\.tar\.gz &amp;
2015-09-09 04:50:54.368554 -0400 : 9766 : omnipitr-backup-master : LOG : nice tar cf \- \-\-exclude\=pg_log\/\* \-\-exclude\=pg_xlog\/0\* \-\-exclude\=pg_xlog\/archive_status\/\* \-\-exclude\=postmaster\.pid test_94 2&gt; \/data\/omnipitr\/test_94\/tmp\/omnipitr\-backup\-master\/9766\/tar\.stderr &gt; \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0
2015-09-09 04:50:54.368554 -0400 : 9766 : omnipitr-backup-master : LOG : wait
2015-09-09 04:50:54.368554 -0400 : 9766 : omnipitr-backup-master : LOG : rm \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0
2015-09-09 04:51:14.415299 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Compressing $PGDATA] took: 20.048s
2015-09-09 04:51:15.483531 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [SELECT pg_stop_backup()] took: 1.066s
2015-09-09 04:51:15.486756 -0400 : 9766 : omnipitr-backup-master : LOG : pg_stop_backup(&apos;omnipitr&apos;) returned 1/3C000128.
2015-09-09 04:51:15.487734 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Making data archive] took: 21.300s
2015-09-09 04:51:15.489015 -0400 : 9766 : omnipitr-backup-master : LOG : File 00000001000000010000003C.00000028.backup arrived after 0 seconds.
2015-09-09 04:51:15.490275 -0400 : 9766 : omnipitr-backup-master : LOG : File 00000001000000010000003C arrived after 0 seconds.
2015-09-09 04:51:15.491816 -0400 : 9766 : omnipitr-backup-master : LOG : Script to make tarballs:
2015-09-09 04:51:15.491816 -0400 : 9766 : omnipitr-backup-master : LOG : mkfifo \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0
2015-09-09 04:51:15.491816 -0400 : 9766 : omnipitr-backup-master : LOG : nice gzip \-\-stdout \- &lt; \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0 &gt; \/data\/postgres\/backup\/test_94\/QA\-5\-45\-xlog\-2015\-09\-09\.tar\.gz &amp;
2015-09-09 04:51:15.491816 -0400 : 9766 : omnipitr-backup-master : LOG : nice tar cf \- test_94 2&gt; \/data\/omnipitr\/test_94\/tmp\/omnipitr\-backup\-master\/9766\/tar\.stderr &gt; \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0
2015-09-09 04:51:15.491816 -0400 : 9766 : omnipitr-backup-master : LOG : wait
2015-09-09 04:51:15.491816 -0400 : 9766 : omnipitr-backup-master : LOG : rm \/tmp\/CommandPiper\-9766\-Ldy5aL\/fifo\-0
2015-09-09 04:51:15.936762 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Compressing xlogs] took: 0.445s
2015-09-09 04:51:15.947223 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Making xlog archive] took: 0.458s
2015-09-09 04:51:15.948752 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Delivering to all remote destinations] took: 0.000s
2015-09-09 04:51:15.950304 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Delivering meta files] took: 0.001s
2015-09-09 04:51:15.951272 -0400 : 9766 : omnipitr-backup-master : LOG : Timer [Whole backup procedure] took: 21.764s
2015-09-09 04:51:15.952141 -0400 : 9766 : omnipitr-backup-master : LOG : All done.
</code></pre><p>4） 备份文件</p>
<pre><code>$ ll /data/postgres/backup/test_94/
total 95620
-rw-rw-r--. 1 postgres postgres 97851471 Sep  9 04:56 QA-5-45-data-2015-09-09.tar.gz
-rw-rw-r--. 1 postgres postgres       93 Sep  9 04:56 QA-5-45-meta-2015-09-09.tar.gz
-rw-rw-r--. 1 postgres postgres    54742 Sep  9 04:56 QA-5-45-xlog-2015-09-09.tar.gz
</code></pre><p>5） 备份到远端</p>
<pre><code>$ /opt/omnipitr/bin/omnipitr-backup-master --host=localhost \
--username=postgres \
--port=5432 \
--data-dir=/data/postgres/data/test_94/ \
--dst-direct gzip=root@172.17.5.46:/data/postgres/backup/test_94/ \    #使用ssh方式传送备份
--skip-xlogs \    #不备份wal文件
--temp-dir=/data/omnipitr/test_94/tmp/ \
--pid-file=/data/omnipitr/test_94/state/base_backup.pid \
--log=/data/omnipitr/test_94/log/base_backup.log \
--verbose
root@172.17.5.46&apos;s password:
</code></pre><h3 id="2-从slave节点备份"><a href="#2-从slave节点备份" class="headerlink" title="(2) 从slave节点备份"></a>(2) 从slave节点备份</h3><p>1) 备份</p>
<pre><code>$ /opt/omnipitr/bin/omnipitr-backup-slave --host=172.17.5.45 \    #master节点ip
--username=postgres \
--port=5432 \
--call-master \    #保证在master上执行SELECT pg_start_backup( &apos;...&apos;  );和SELECT pg_stop_backup()；,否则做出来的备份可能无效,对应的用户名必须是superuser
--data-dir=/data/postgres/data/test_94/ \
--dst-local gzip=/data/postgres/backup/test_94/ \
--source=/data/postgres/data/test_94/pg_xlog \
--temp-dir=/data/omnipitr/test_94/tmp/ \
--pid-file=/data/omnipitr/test_94/state/base_backup.pid \
--log=/data/omnipitr/test_94/log/base_backup.log \
--verbose
Password for user postgres:
Password for user postgres:
Password for user postgres:
</code></pre><p>2） 备份日志</p>
<pre><code>2015-09-09 06:16:55.621897 -0400 : 23447 : omnipitr-backup-slave : LOG : Called with parameters: --host=172.17.5.45 --username=postgres --port=5432 --call-master --data-dir=/data/postgres/data/test_94/ --dst-local gzip=/data/postgres/backup/test_94/ --skip-xlogs --temp-dir=/data/omnipitr/test_94/tmp/ --pid-file=/data/omnipitr/test_94/state/base_backup.pid --log=/data/omnipitr/test_94/log/base_backup.log --verbose
2015-09-09 06:16:58.856673 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [SELECT w, pg_xlogfile_name(w) from ( as w  ) as x] took: 3.220s
2015-09-09 06:16:58.864733 -0400 : 23447 : omnipitr-backup-slave : LOG : pg_start_backup(&apos;omnipitr&apos;, true) returned 1/62000028|000000010000000100000062.
2015-09-09 06:17:01.862033 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [select pg_read_file( &apos;backup_label&apos;, 0, ( pg_stat_file( &apos;backup_label&apos;  )  ).size  )] took: 2.996s
2015-09-09 06:17:01.863971 -0400 : 23447 : omnipitr-backup-slave : LOG : Waiting for checkpoint (based on backup_label from master) - CHECKPOINT LOCATION: 1/62000060
2015-09-09 06:18:37.008788 -0400 : 23447 : omnipitr-backup-slave : LOG : Checkpoint .
2015-09-09 06:18:37.013235 -0400 : 23447 : omnipitr-backup-slave : LOG : Script to make tarballs:
2015-09-09 06:18:37.013235 -0400 : 23447 : omnipitr-backup-slave : LOG : mkfifo \/tmp\/CommandPiper\-23447\-buP6d5\/fifo\-0
2015-09-09 06:18:37.013235 -0400 : 23447 : omnipitr-backup-slave : LOG : nice gzip \-\-stdout \- &lt; \/tmp\/CommandPiper\-23447\-buP6d5\/fifo\-0 &gt; \/data\/postgres\/backup\/test_94\/node2\-data\-2015\-09\-09\.tar\.gz &amp;
2015-09-09 06:18:37.013235 -0400 : 23447 : omnipitr-backup-slave : LOG : nice tar cf \- \-\-exclude\=test_94\/pg_log\/\* \-\-exclude\=test_94\/pg_xlog\/0\* \-\-exclude\=test_94\/pg_xlog\/archive_status\/\* \-\-exclude\=test_94\/recovery\.conf \-\-exclude\=test_94\/postmaster\.pid \-\-transform\=s\#\^data\/omnipitr\/test_94\/tmp\/omnipitr\-backup\-slave\/23447\/\#test_94\/\# test_94 \/data\/omnipitr\/test_94\/tmp\/omnipitr\-backup\-slave\/23447\/backup_label 2&gt; \/data\/omnipitr\/test_94\/tmp\/omnipitr\-backup\-slave\/23447\/tar\.stderr &gt; \/tmp\/CommandPiper\-23447\-buP6d5\/fifo\-0
2015-09-09 06:18:37.013235 -0400 : 23447 : omnipitr-backup-slave : LOG : wait
2015-09-09 06:18:37.013235 -0400 : 23447 : omnipitr-backup-slave : LOG : rm \/tmp\/CommandPiper\-23447\-buP6d5\/fifo\-0
2015-09-09 06:18:57.288130 -0400 : 23447 : omnipitr-backup-slave : LOG : tar stderr:
2015-09-09 06:18:57.290137 -0400 : 23447 : omnipitr-backup-slave : LOG : ==============================================
2015-09-09 06:18:57.290849 -0400 : 23447 : omnipitr-backup-slave : LOG : tar: Removing leading `/&apos; from member names
2015-09-09 06:18:57.291659 -0400 : 23447 : omnipitr-backup-slave : LOG : ==============================================
2015-09-09 06:18:57.292502 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [Compressing $PGDATA] took: 20.280s
2015-09-09 06:19:34.578726 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [SELECT pg_stop_backup()] took: 37.285s
2015-09-09 06:19:34.580554 -0400 : 23447 : omnipitr-backup-slave : LOG : pg_stop_backup() returned 1/62000128.
2015-09-09 06:19:34.581542 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [Making data archive] took: 158.950s
2015-09-09 06:19:34.582615 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [Making xlog archive] took: 0.000s
2015-09-09 06:19:34.583894 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [Delivering to all remote destinations] took: 0.000s
2015-09-09 06:19:34.585621 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [Delivering meta files] took: 0.000s
2015-09-09 06:19:34.586895 -0400 : 23447 : omnipitr-backup-slave : LOG : Timer [Whole backup procedure] took: 158.955s
2015-09-09 06:19:34.587940 -0400 : 23447 : omnipitr-backup-slave : LOG : All done.
</code></pre><p>3） 备份文件</p>
<pre><code># ll /data/postgres/backup/test_94/
total 95548
-rw-r--r--. 1 postgres postgres 97836713 Sep  9 06:18 node2-data-2015-09-09.tar.gz
-rw-r--r--. 1 postgres postgres       91 Sep  9 06:19 node2-meta-2015-09-09.tar.gz
</code></pre><p>4) 备注</p>
<p>默认执行pg_start_backup()是没有加true的,上面红色的部分是我修改了下代码,这样执行checkpoint会快很多</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL同步复制故障测试]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%90%8C%E6%AD%A5%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E6%B5%8B%E8%AF%95/</url>
      <content type="html"><![CDATA[<h2 id="1-备用同步复制节点down了"><a href="#1-备用同步复制节点down了" class="headerlink" title="1.备用同步复制节点down了"></a>1.备用同步复制节点down了</h2><pre><code>postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12262 |       10 | postgres | node3            | 172.17.5.47 |                 |       48691 | 2015-09-06 14:44:52.674976+08 |       400592 | streaming | 1/740002E8    | 1/740002E8     | 1/
740002E8     | 1/740002E8      |             2 | potential
 12263 |       10 | postgres | node2            | 172.17.5.46 |                 |       35271 | 2015-09-06 14:44:52.677004+08 |       400592 | streaming | 1/740002E8    | 1/740002E8     | 1/
740002E8     | 1/740002E8      |             1 | sync
(2 rows)
</code></pre><a id="more"></a>
<pre><code>postgres=# create table test1 ( id bigint );
CREATE TABLE
postgres=# \d test1
    Table &quot;public.test1&quot;
 Column |  Type  | Modifiers
--------+--------+-----------
 id     | bigint |
</code></pre><p>在mastrer上查看是正常工作的,现在把172.17.5.47节点停机</p>
<pre><code>pg_ctl -D /data/pg940_data/ -mf stop
</code></pre><p>在master上验证</p>
<pre><code>postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12263 |       10 | postgres | node2            | 172.17.5.46 |                 |       35271 | 2015-09-06 14:44:52.677004+08 |       400593 | streaming | 1/74001678    | 1/74001678     | 1/
74001678     | 1/74001678      |             1 | sync
(1 row)

postgres=# create table test2 ( id bigint );
CREATE TABLE
postgres=# \d test2
    Table &quot;public.test2&quot;
 Column |  Type  | Modifiers
--------+--------+-----------
 id     | bigint |
</code></pre><p>master节点和同步复制slave节点可以正常工作</p>
<h2 id="2-同步复制slave节点down了"><a href="#2-同步复制slave节点down了" class="headerlink" title="2.同步复制slave节点down了"></a>2.同步复制slave节点down了</h2><pre><code>postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12377 |       10 | postgres | node3            | 172.17.5.47 |                 |       55722 | 2015-09-06 15:10:02.882202+08 |       400594 | streaming | 1/740029F0    | 1/740029F0     | 1/
740029F0     | 1/740029F0      |             2 | potential
 12263 |       10 | postgres | node2            | 172.17.5.46 |                 |       35271 | 2015-09-06 14:44:52.677004+08 |       400594 | streaming | 1/740029F0    | 1/740029F0     | 1/
740029F0     | 1/740029F0      |             1 | sync
(2 rows)

postgres=# create table test3 ( id bigint  );
CREATE TABLE
postgres=# \d test3
    Table &quot;public.test3&quot;
 Column |  Type  | Modifiers
--------+--------+-----------
 id     | bigint |
</code></pre><p>在mastrer上查看是正常工作的,现在把172.17.5.46节点停机</p>
<pre><code>pg_ctl -D /data/pg940_data/ -mf stop
</code></pre><p>在master上验证</p>
<pre><code>postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12377 |       10 | postgres | node3            | 172.17.5.47 |                 |       55722 | 2015-09-06 15:10:02.882202+08 |       400595 | streaming | 1/74004C10    | 1/74004C10     | 1/
74004C10     | 1/74004C10      |             2 | sync
(1 row)
postgres=# create table test4 ( id bigint  );
CREATE TABLE
postgres=# \d test4
    Table &quot;public.test4&quot;
 Column |  Type  | Modifiers
--------+--------+-----------
 id     | bigint |
</code></pre><p>可以看出同步复制节点切换到了node3,并且是可以正常工作的</p>
<h2 id="3-slave节点都down掉了"><a href="#3-slave节点都down掉了" class="headerlink" title="3.slave节点都down掉了"></a>3.slave节点都down掉了</h2><pre><code>postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12377 |       10 | postgres | node3            | 172.17.5.47 |                 |       55722 | 2015-09-06 15:10:02.882202+08 |       400596 | streaming | 1/74005F38    | 1/74005F38     | 1/
74005F38     | 1/74005F38      |             2 | sync
(1 row)

postgres=# create table test5 ( id bigint  );
CREATE TABLE
postgres=# \d test5
    Table &quot;public.test5&quot;
 Column |  Type  | Modifiers
--------+--------+-----------
 id     | bigint |
</code></pre><p>在master上查看是正常工作的,现在把172.17.5.47节点也停机</p>
<pre><code>pg_ctl -D /data/pg940_data/ -mf stop
</code></pre><p>在master上验证</p>
<pre><code>postgres=# select * from pg_stat_replication ;
 pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | backend_xmin | state | sent_location | write_location | flush_location | replay_l
ocation | sync_priority | sync_state
-----+----------+---------+------------------+-------------+-----------------+-------------+---------------+--------------+-------+---------------+----------------+----------------+---------
--------+---------------+------------
(0 rows)

postgres=# create table test6 ( id bigint  );
</code></pre><p>master上不能更新了,会一直等待,直到synchronous_standby_names中的一个节点连接上master节点,但是可以ctrl+c取消,那样会进行本地提交</p>
<pre><code>^CCancel request sent
WARNING:  canceling wait for synchronous replication due to user request
DETAIL:  The transaction has already committed locally, but might not have been replicated to the standby.
CREATE TABLE
postgres=# \d test6
    Table &quot;public.test6&quot;
 Column |  Type  | Modifiers
--------+--------+-----------
 id     | bigint |
</code></pre><h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h2><p>在同步复制中:</p>
<pre><code>同步的slave节点down掉会将后面的节点切换为同步节点；

非同步slave节点down掉不会进行切换；

如果slave节点没有全部down掉,不会影响master的使用,如果都down掉的话,master则不能进行提交。
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL同步复制搭建]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E5%90%8C%E6%AD%A5%E5%A4%8D%E5%88%B6%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="1-初始化master节点"><a href="#1-初始化master节点" class="headerlink" title="1.初始化master节点"></a>1.初始化master节点</h2><h3 id="1-安装PostgreSQL"><a href="#1-安装PostgreSQL" class="headerlink" title="(1) 安装PostgreSQL"></a>(1) 安装PostgreSQL</h3><h3 id="2-初始化db"><a href="#2-初始化db" class="headerlink" title="(2) 初始化db"></a>(2) 初始化db</h3><pre><code>initdb -D /data/pg940_data
</code></pre><a id="more"></a>
<h2 id="2-配置master节点"><a href="#2-配置master节点" class="headerlink" title="2.配置master节点"></a>2.配置master节点</h2><h3 id="1-修改postgresql-conf"><a href="#1-修改postgresql-conf" class="headerlink" title="(1) 修改postgresql.conf"></a>(1) 修改postgresql.conf</h3><pre><code>$ vim /data/pg940_data/postgresql.conf
wal_level = hot_standby
fsync = on
synchronous_commit = on    #同步复制必须设置为on
wal_sync_method = fdatasync
max_wal_senders = 4        #配置的是两个slave节点,最好设置大一点,方便以后添加slave节点,pg_basebackup使用stream方式需要占用2个槽
wal_keep_segments = 100
</code></pre><h3 id="2-修改白名单"><a href="#2-修改白名单" class="headerlink" title="(2) 修改白名单"></a>(2) 修改白名单</h3><pre><code>$ vim /data/pg940_data/pg_hba.conf
host    all                 all             172.17.0.0/16           md5
host    replication         all             172.17.0.0/16           md5
</code></pre><h3 id="3-启动master节点"><a href="#3-启动master节点" class="headerlink" title="(3) 启动master节点"></a>(3) 启动master节点</h3><pre><code>$ pg_ctl -D /data/pg940_data start
</code></pre><h2 id="3-制作slave节点"><a href="#3-制作slave节点" class="headerlink" title="3.制作slave节点"></a>3.制作slave节点</h2><h3 id="1-获取master节点的基础备份"><a href="#1-获取master节点的基础备份" class="headerlink" title="(1) 获取master节点的基础备份"></a>(1) 获取master节点的基础备份</h3><pre><code>$ sudo mkdir -p /data/pg940_data
$ sudo chown -R postgres:postgres /data/pg940_data
$ pg_basebackup -X s -h 172.17.5.45 -U postgres -D /data/pg940_data
</code></pre><h3 id="2-配置postgresql-conf"><a href="#2-配置postgresql-conf" class="headerlink" title="(2) 配置postgresql.conf"></a>(2) 配置postgresql.conf</h3><pre><code>hot_standby = on
max_standby_archive_delay = 30s
max_standby_streaming_delay = 30s
</code></pre><h3 id="3-配置recovery-conf"><a href="#3-配置recovery-conf" class="headerlink" title="(3) 配置recovery.conf"></a>(3) 配置recovery.conf</h3><pre><code>standby_mode = &apos;on&apos;
primary_conninfo = &apos;host=172.17.5.45 port=5432 user=postgres password=xxxxxx application_name=node2&apos;
restore_command = &apos;&apos;
archive_cleanup_command = &apos;&apos;
</code></pre><h3 id="4-启动slave节点"><a href="#4-启动slave节点" class="headerlink" title="(4) 启动slave节点"></a>(4) 启动slave节点</h3><pre><code>$ pg_ctl -D /data/pg940_data start
$ psql -U postgres
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12262 |       10 | postgres | node3            | 172.17.5.47 |                 |       48691 | 2015-09-06 14:44:52.674976+08 |       400592 | streaming | 1/74000210    | 1/74000210     | 1/
74000210     | 1/74000210      |             0 | async
 12263 |       10 | postgres | node2            | 172.17.5.46 |                 |       35271 | 2015-09-06 14:44:52.677004+08 |       400592 | streaming | 1/74000210    | 1/74000210     | 1/
74000210     | 1/74000210      |             0 | async
(2 rows)
</code></pre><p>可以看出两个节点现在都还是异步的</p>
<h2 id="4-修改为同步复制"><a href="#4-修改为同步复制" class="headerlink" title="4.修改为同步复制"></a>4.修改为同步复制</h2><pre><code>$ vim /data/pg940_data/postgresql.conf
synchronous_standby_names = &apos;node2, node3&apos;    # reload配置就行,不需要重启
$ psql -U postgres
psql (9.4.4)
Type &quot;help&quot; for help.
postgres=# show synchronous_standby_names;
 synchronous_standby_names
---------------------------

(1 row)
postgres=# select pg_reload_conf();
 pg_reload_conf
----------------
 t
(1 row)
postgres=# show synchronous_standby_names;
 synchronous_standby_names
---------------------------
 node2, node3
(1 row)
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | fl
ush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+---
-------------+-----------------+---------------+------------
 12262 |       10 | postgres | node3            | 172.17.5.47 |                 |       48691 | 2015-09-06 14:44:52.674976+08 |       400592 | streaming | 1/740002E8    | 1/740002E8     | 1/
740002E8     | 1/740002E8      |             2 | potential
 12263 |       10 | postgres | node2            | 172.17.5.46 |                 |       35271 | 2015-09-06 14:44:52.677004+08 |       400592 | streaming | 1/740002E8    | 1/740002E8     | 1/
740002E8     | 1/740002E8      |             1 | sync
(2 rows)
</code></pre><p>可以看出现在node2是同步复制的,node3是备用同步复制节点</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL之网络地址类型]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%9C%B0%E5%9D%80%E7%B1%BB%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>官方文档:<a href="http://www.postgresql.org/docs/9.4/interactive/datatype-net-types.html" target="_blank" rel="external">http://www.postgresql.org/docs/9.4/interactive/datatype-net-types.html</a></p>
<h2 id="1-cidr"><a href="#1-cidr" class="headerlink" title="1. cidr"></a>1. cidr</h2><pre><code>postgres=# create table test (id int, name text);
CREATE TABLE
postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |
</code></pre><a id="more"></a>
<pre><code>postgres=# alter table test add column ip cidr;
ALTER TABLE
postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |
 ip     | cidr    |

postgres=# insert into test values (1, &apos;a&apos;, &apos;192.168.1.100&apos;);
INSERT 0 1
postgres=# select * from test ;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
(1 row)

postgres=# insert into test values (2, &apos;b&apos;, &apos;192.168.0.0/16&apos;);
INSERT 0 1
postgres=# select * from test ;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  2 | b    | 192.168.0.0/16
(2 rows)

postgres=# insert into test values (3, &apos;c&apos;, &apos;192.168.1.0/24&apos;);
INSERT 0 1
postgres=# select * from test ;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  2 | b    | 192.168.0.0/16
  3 | c    | 192.168.1.0/24
(3 rows)
</code></pre><p>查询使用</p>
<pre><code>postgres=# select * from test where ip = &apos;192.168.1.100&apos;;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
(1 row)

postgres=#  select * from test where ip &gt;= &apos;192.168.1.0/24&apos;;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  3 | c    | 192.168.1.0/24
(2 rows)

postgres=#  select * from test where ip &gt;= &apos;192.168.0.0/16&apos;;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  2 | b    | 192.168.0.0/16
  3 | c    | 192.168.1.0/24
(3 rows)

postgres=# update test set ip = &apos;192.168.1.101/32&apos; where id = 2;
UPDATE 1
postgres=# update test set ip = &apos;192.168.1.102/32&apos; where id = 3;
UPDATE 1
postgres=# select * from test ;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  2 | b    | 192.168.1.101/32
  3 | c    | 192.168.1.102/32
(3 rows)

postgres=# select * from test where  ip between &apos;192.168.1.100&apos; and &apos;192.168.1.101&apos;;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  2 | b    | 192.168.1.101/32
(2 rows)

postgres=# select * from test where  ip between &apos;192.168.1.100&apos; and &apos;192.168.1.102&apos;;
 id | name |        ip
----+------+------------------
  1 | a    | 192.168.1.100/32
  2 | b    | 192.168.1.101/32
  3 | c    | 192.168.1.102/32
(3 rows)
</code></pre><h2 id="2-inet"><a href="#2-inet" class="headerlink" title="2. inet"></a>2. inet</h2><p>将cidr修改为inet</p>
<pre><code>postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |
 ip     | cidr    |
 postgres=# alter table test alter column ip type inet;
ALTER TABLE
postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |
 ip     | inet    |

postgres=# select * from test ;
 id | name |      ip
----+------+---------------
  1 | a    | 192.168.1.100
  2 | b    | 192.168.1.101
  3 | c    | 192.168.1.102
(3 rows)

postgres=# update test set ip = &apos;192.168.0.0/16&apos; where id = 3;
UPDATE 1
postgres=# select * from test ;
 id | name |       ip
----+------+----------------
  1 | a    | 192.168.1.100
  2 | b    | 192.168.1.101
  3 | c    | 192.168.0.0/16
(3 rows)

postgres=# update test set ip = &apos;192.168.1.0/24&apos; where id = 2;
UPDATE 1
postgres=# select * from test ;
 id | name |       ip
----+------+----------------
  1 | a    | 192.168.1.100
  3 | c    | 192.168.0.0/16
  2 | b    | 192.168.1.0/24
(3 rows)
</code></pre><p>可见,inet默认32位掩码的ip是不带’/32’的</p>
<pre><code>postgres=# select * from test where ip &gt;= &apos;192.168.1.100&apos;;
 id | name |      ip
----+------+---------------
  1 | a    | 192.168.1.100
(1 row)

postgres=# select * from test where ip &gt;= &apos;192.168.1.1&apos;;
 id | name |      ip
----+------+---------------
  1 | a    | 192.168.1.100
(1 row)

postgres=# select * from test where ip &gt;= &apos;192.168.1.101&apos;;
 id | name | ip
----+------+----
(0 rows)
postgres=# select * from test where ip &gt;= &apos;192.168.1.0/32&apos;;
 id | name |      ip
----+------+---------------
  1 | a    | 192.168.1.100
(1 row)
postgres=# select * from test where ip &gt;= &apos;192.168.1.0/16&apos;;
 id | name |       ip
----+------+----------------
  1 | a    | 192.168.1.100
  2 | b    | 192.168.1.0/24
(2 rows)

postgres=# select * from test where ip &gt;= &apos;192.168.0.0/16&apos;;
 id | name |       ip
----+------+----------------
  1 | a    | 192.168.1.100
  3 | c    | 192.168.0.0/16
  2 | b    | 192.168.1.0/24
(3 rows)
</code></pre><p>使用跟cidr差不多</p>
<h2 id="3-macaddr"><a href="#3-macaddr" class="headerlink" title="3. macaddr"></a>3. macaddr</h2><pre><code>postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |
 ip     | inet    |

postgres=# alter table test add column mac macaddr;
ALTER TABLE
postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |
 ip     | inet    |
 mac    | macaddr |

postgres=# select * from test ;
 id | name |       ip       | mac
----+------+----------------+-----
  1 | a    | 192.168.1.100  |
  3 | c    | 192.168.0.0/16 |
  2 | b    | 192.168.1.0/24 |
(3 rows)

postgres=# update test set mac = &apos;08:00:2b:01:02:03&apos; where id = 1;
UPDATE 1
postgres=# select * from test ;
 id | name |       ip       |        mac
----+------+----------------+-------------------
  3 | c    | 192.168.0.0/16 |
  2 | b    | 192.168.1.0/24 |
  1 | a    | 192.168.1.100  | 08:00:2b:01:02:03
(3 rows)
postgres=# update test set mac = &apos;08:00:2b:01:02:04&apos; where id = 2;
UPDATE 1
postgres=# update test set mac = &apos;08:00:2b:01:02:05&apos; where id = 3;
UPDATE 1
postgres=# select * from test ;
 id | name |       ip       |        mac
----+------+----------------+-------------------
  1 | a    | 192.168.1.100  | 08:00:2b:01:02:03
  2 | b    | 192.168.1.0/24 | 08:00:2b:01:02:04
  3 | c    | 192.168.0.0/16 | 08:00:2b:01:02:05
(3 rows)
</code></pre><p>查询使用</p>
<pre><code>postgres=# select * from test where mac = &apos;08:00:2b:01:02:03&apos;;
 id | name |      ip       |        mac
----+------+---------------+-------------------
  1 | a    | 192.168.1.100 | 08:00:2b:01:02:03
(1 row)

postgres=# select * from test where mac &gt; &apos;08:00:2b:01:02:03&apos;;
 id | name |       ip       |        mac
----+------+----------------+-------------------
  2 | b    | 192.168.1.0/24 | 08:00:2b:01:02:04
  3 | c    | 192.168.0.0/16 | 08:00:2b:01:02:05
(2 rows)
</code></pre><p>PostgreSQL默认还不支持iprange,需要安装ip4r的扩展,详见:<a href="http://pgfoundry.org/projects/ip4r/" target="_blank" rel="external">http://pgfoundry.org/projects/ip4r/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL主键膨胀维护]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E4%B8%BB%E9%94%AE%E8%86%A8%E8%83%80%E7%BB%B4%E6%8A%A4/</url>
      <content type="html"><![CDATA[<h2 id="1-维护前"><a href="#1-维护前" class="headerlink" title="1. 维护前"></a>1. 维护前</h2><pre><code>postgres=# \d+ test
                                                        Table
&quot;public.test&quot;
  Column |            Type             |                     Modifiers
| Storage | Stats target | Description
--------+-----------------------------+----------------------------------------------------+---------+--------------+-------------
  id     | integer                     | not null default
nextval(&apos;test_id_seq&apos;::regclass) | plain   |              |
  val    | timestamp without time zone | default now()
| plain   |              |
Indexes:
     &quot;test_pkey&quot; PRIMARY KEY, btree (id)
</code></pre><a id="more"></a>
<h2 id="2-创建新的unique-index"><a href="#2-创建新的unique-index" class="headerlink" title="2. 创建新的unique index"></a>2. 创建新的unique index</h2><pre><code>postgres=# CREATE UNIQUE INDEX CONCURRENTLY ON test USING btree(id);
CREATE INDEX
</code></pre><h2 id="3-替换主键"><a href="#3-替换主键" class="headerlink" title="3. 替换主键"></a>3. 替换主键</h2><pre><code>postgres=# BEGIN;
BEGIN
postgres=# ALTER TABLE test DROP CONSTRAINT test_pkey;
ALTER TABLE
postgres=# ALTER TABLE test ADD CONSTRAINT test_id_idx PRIMARY KEY USING
INDEX test_id_idx;
ALTER TABLE
postgres=# COMMIT;
COMMIT
</code></pre><h2 id="4-维护后"><a href="#4-维护后" class="headerlink" title="4. 维护后"></a>4. 维护后</h2><pre><code>postgres=# \d+ test
                                                        Table
&quot;public.test&quot;
  Column |            Type             |                     Modifiers
| Storage | Stats target | Description
--------+-----------------------------+----------------------------------------------------+---------+--------------+-------------
  id     | integer                     | not null default
nextval(&apos;test_id_seq&apos;::regclass) | plain   |              |
  val    | timestamp without time zone | default now()
| plain   |              |
Indexes:
     &quot;test_id_idx&quot; PRIMARY KEY, btree (id)
</code></pre><p>这样的好处是,可以在白天负载低的时候维护主键</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pacemaker+Corosync搭建PostgreSQL集群]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-Pacemaker+Corosync%E6%90%AD%E5%BB%BA2016-07-05-PostgreSQL%E9%9B%86%E7%BE%A4/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1.环境"></a>1.环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-5-90 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
node1: 172.17.5.90
node2: 172.17.5.91
node3: 172.17.5.92
vip-master: 172.17.5.99
vip-slave:  172.17.5.98
</code></pre><a id="more"></a>
<h2 id="2-配置Linux集群环境"><a href="#2-配置Linux集群环境" class="headerlink" title="2.配置Linux集群环境"></a>2.配置Linux集群环境</h2><h3 id="1-安装Pacemaker和Corosync包"><a href="#1-安装Pacemaker和Corosync包" class="headerlink" title="(1) 安装Pacemaker和Corosync包"></a>(1) 安装Pacemaker和Corosync包</h3><p>在所有节点执行:</p>
<pre><code>$ sudo yum install -y pacemaker pcs psmisc policycoreutils-python postgresql-server
</code></pre><h3 id="2-禁用防火墙"><a href="#2-禁用防火墙" class="headerlink" title="(2) 禁用防火墙"></a>(2) 禁用防火墙</h3><p>在所有节点执行:</p>
<pre><code>$ sudo setenforce 0
$ sudo sed -i.bak &quot;s/SELINUX=enforcing/SELINUX=permissive/g&quot; /etc/selinux/config
$ sudo systemctl disable firewalld.service
$ sudo systemctl stop firewalld.service
$ sudo iptables --flush
</code></pre><h3 id="3-启用pcs"><a href="#3-启用pcs" class="headerlink" title="(3) 启用pcs"></a>(3) 启用pcs</h3><p>在所有节点执行:</p>
<pre><code>$ sudo systemctl start pcsd.service
$ sudo systemctl enable pcsd.service
ln -s &apos;/usr/lib/systemd/system/pcsd.service&apos; &apos;/etc/systemd/system/multi-user.target.wants/pcsd.service&apos;
$ echo hacluster | sudo passwd hacluster --stdin
Changing password for user hacluster.
Changing password for user hacluster.
passwd: all authentication tokens updated successfully.
</code></pre><h3 id="4-集群认证"><a href="#4-集群认证" class="headerlink" title="(4) 集群认证"></a>(4) 集群认证</h3><p>在任何一个节点上执行,这里选择node1:</p>
<pre><code>$ sudo pcs cluster auth -u hacluster -p hacluster 172.17.5.90 172.17.5.91 172.17.5.92
172.17.5.90: Authorized
172.17.5.91: Authorized
172.17.5.92: Authorized
</code></pre><h3 id="5-同步配置"><a href="#5-同步配置" class="headerlink" title="(5) 同步配置"></a>(5) 同步配置</h3><p>在node1上执行:</p>
<pre><code>$ sudo pcs cluster setup --last_man_standing=1 --name pgcluster 172.17.5.90 172.17.5.91 172.17.5.92
Shutting down pacemaker/corosync services...
Redirecting to /bin/systemctl stop  pacemaker.service
Redirecting to /bin/systemctl stop  corosync.service
Killing any remaining services...
Removing all cluster configuration files...
172.17.5.90: Succeeded
172.17.5.91: Succeeded
172.17.5.92: Succeeded
</code></pre><h3 id="6-启动集群"><a href="#6-启动集群" class="headerlink" title="(6) 启动集群"></a>(6) 启动集群</h3><p>在node1上执行:</p>
<pre><code>$ sudo pcs cluster start --all
172.17.5.90: Starting Cluster...
172.17.5.91: Starting Cluster...
172.17.5.92: Starting Cluster...
</code></pre><h3 id="7-检验"><a href="#7-检验" class="headerlink" title="(7) 检验"></a>(7) 检验</h3><p>1）检验corosync</p>
<p>在node1上执行:</p>
<pre><code>$ sudo pcs status corosync
Membership information
----------------------
    Nodeid      Votes Name
         1          1 172.17.5.90 (local)
         2          1 172.17.5.91
         3          1 172.17.5.92
</code></pre><p>2）检验pacemaker</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: no stonith devices and stonith-enabled is not false
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Mon Oct 19 15:08:06 2015          Last change:
Stack: unknown
Current DC: NONE
0 nodes and 0 resources configured
Full list of resources:
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/disabled
</code></pre><h2 id="3-安装和配置PostgreSQL"><a href="#3-安装和配置PostgreSQL" class="headerlink" title="3.安装和配置PostgreSQL"></a>3.安装和配置PostgreSQL</h2><h3 id="1-创建目录"><a href="#1-创建目录" class="headerlink" title="(1) 创建目录"></a>(1) 创建目录</h3><p>在所有节点上执行:</p>
<pre><code>$ sudo mkdir -p /data/postgresql/{data,xlog_archive}
$ sudo chown -R postgres:postgres /data/postgresql/
$ sudo chmod 0700 /data/postgresql/data
</code></pre><h3 id="2-初始化db"><a href="#2-初始化db" class="headerlink" title="(2) 初始化db"></a>(2) 初始化db</h3><p>在node1上执行:</p>
<pre><code>$ sudo su - postgres
$ initdb -D /data/postgresql/data/
The files belonging to this database system will be owned by user &quot;postgres&quot;.
This user must also own the server process.
The database cluster will be initialized with locale &quot;en_US.UTF-8&quot;.
The default database encoding has accordingly been set to &quot;UTF8&quot;.
The default text search configuration will be set to &quot;english&quot;.
fixing permissions on existing directory /data/postgresql/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 32MB
creating configuration files ... ok
creating template1 database in /data/postgresql/data/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects&apos; descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
WARNING: enabling &quot;trust&quot; authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
Success. You can now start the database server using:
    postgres -D /data/postgresql/data
or
    pg_ctl -D /data/postgresql/data -l logfile start
</code></pre><h3 id="3-修改配置文件"><a href="#3-修改配置文件" class="headerlink" title="(3) 修改配置文件"></a>(3) 修改配置文件</h3><p>在node1上执行:</p>
<pre><code>$ vim /data/postgresql/data/postgresql.conf
listen_addresses = &apos;*&apos;
wal_level = hot_standby
synchronous_commit = on
archive_mode = on
archive_command = &apos;cp %p /data/postgresql/xlog_archive/%f&apos;
max_wal_senders=5
wal_keep_segments = 32
hot_standby = on
restart_after_crash = off
replication_timeout = 5000
wal_receiver_status_interval = 2
max_standby_streaming_delay = -1
max_standby_archive_delay = -1
synchronous_commit = on
restart_after_crash = off
hot_standby_feedback = on
$ vim /data/postgresql/data/pg_hba.conf
local   all                 all                              trust
host    all                 all     172.17.0.0/16            md5
host    replication         all     172.17.0.0/16            md5
</code></pre><h3 id="4-启动"><a href="#4-启动" class="headerlink" title="(4) 启动"></a>(4) 启动</h3><p>在node1上执行:</p>
<pre><code>$ pg_ctl -D /data/postgresql/data/ start
server starting
[    2015-10-16 08:51:31.451 UTC 53158 5620ba93.cfa6 1 0]LOG:  redirecting log output to logging collector process
[    2015-10-16 08:51:31.451 UTC 53158 5620ba93.cfa6 2 0]HINT:  Future log output will appear in directory &quot;pg_log&quot;.
$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# create role replicator with login replication password &apos;8d5e9531-3817-460d-a851-659d2e51ca99&apos;;
CREATE ROLE
postgres=# \q
</code></pre><h3 id="5-制作slave"><a href="#5-制作slave" class="headerlink" title="(5) 制作slave"></a>(5) 制作slave</h3><p>在node2和node3上执行:</p>
<pre><code>$ sudo su - postgres
$ pg_basebackup -h 172.17.5.90 -U postgres -D /data/postgresql/data/ -X stream -P
could not change directory to &quot;/home/wenhang.pan&quot;
20127/20127 kB (100%), 1/1 tablespace
node2:
$ vim /data/postgresql/data/recovery.conf
standby_mode = &apos;on&apos;
primary_conninfo = &apos;host=172.17.5.90 port=5432 user=replicator password=8d5e9531-3817-460d-a851-659d2e51ca99 application_name=node2&apos;
restore_command = &apos;&apos;
recovery_target_timeline = &apos;latest&apos;
node3:
$ vim /data/postgresql/data/recovery.conf
standby_mode = &apos;on&apos;
primary_conninfo = &apos;host=172.17.5.90 port=5432 user=replicator password=8d5e9531-3817-460d-a851-659d2e51ca99 application_name=node3&apos;
restore_command = &apos;&apos;
recovery_target_timeline = &apos;latest&apos;
</code></pre><h3 id="6-启动slave"><a href="#6-启动slave" class="headerlink" title="(6) 启动slave"></a>(6) 启动slave</h3><p>在node2和node3上执行:</p>
<pre><code>$ pg_ctl -D /data/postgresql/data/ start
pg_ctl: another server might be running; trying to start server anyway
server starting
-bash-4.2$ LOG:  database system was interrupted while in recovery at log time 2015-10-16 08:19:07 GMT
HINT:  If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.
LOG:  entering standby mode
LOG:  redo starts at 0/3000020
LOG:  consistent recovery state reached at 0/30000E0
LOG:  database system is ready to accept read only connections
LOG:  streaming replication successfully connected to primary
</code></pre><h3 id="7-查看集群状态"><a href="#7-查看集群状态" class="headerlink" title="(7) 查看集群状态"></a>(7) 查看集群状态</h3><p>在node1上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid |  usename   | application_name | client_addr  | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+------------+------------------+--------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 10745 |    16384 | postgres   | node2            | 172.17.5.91 |                 |       43013 | 2015-10-16 02:54:02.279384+00 |         1911 | streaming | 39/7B000060   | 39/7B000060    | 39/7B000060    | 39/7B000000     |             0 | async
 50361 |    16384 | postgres   | node3            | 172.17.5.92 |                 |       52073 | 2015-10-15 10:13:15.436745+00 |         1911 | streaming | 39/7B000060   | 39/7B000060    | 39/7B000060    | 39/7B000000     |             0 | async
(2 rows)
postgres=# \q
</code></pre><h3 id="8-停止PostgreSQL服务"><a href="#8-停止PostgreSQL服务" class="headerlink" title="(8) 停止PostgreSQL服务"></a>(8) 停止PostgreSQL服务</h3><p>在node1、node2和node3上执行:</p>
<pre><code>$ pg_ctl -D /data/postgresql/data/ -mi stop
waiting for server to shut down.... done
server stopped
</code></pre><h2 id="4-配置自动切换"><a href="#4-配置自动切换" class="headerlink" title="4.配置自动切换"></a>4.配置自动切换</h2><h3 id="1-配置"><a href="#1-配置" class="headerlink" title="(1) 配置"></a>(1) 配置</h3><p>在node1执行:</p>
<p>1）将配置步骤先写到脚本</p>
<pre><code>$ vim cluster_setup.sh
# 将cib配置保存到文件
pcs cluster cib pgsql_cfg
# 在pacemaker级别忽略quorum
pcs -f pgsql_cfg property set no-quorum-policy=&quot;ignore&quot;
# 禁用STONITH
pcs -f pgsql_cfg property set stonith-enabled=&quot;false&quot;
# 设置资源粘性,防止节点在故障恢复后发生迁移
pcs -f pgsql_cfg resource defaults resource-stickiness=&quot;INFINITY&quot;
# 设置多少次失败后迁移
pcs -f pgsql_cfg resource defaults migration-threshold=&quot;3&quot;
# 设置master节点虚ip
pcs -f pgsql_cfg resource create vip-master IPaddr2 ip=&quot;172.17.5.99&quot; cidr_netmask=&quot;24&quot;    op start   timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;restart&quot;    op monitor timeout=&quot;60s&quot; interval=&quot;10s&quot; on-fail=&quot;restart&quot;    op stop    timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;block&quot;
# 设置slave节点虚ip
pcs -f pgsql_cfg resource create vip-slave IPaddr2 ip=&quot;172.17.5.98&quot; cidr_netmask=&quot;24&quot;    op start   timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;restart&quot;    op monitor timeout=&quot;60s&quot; interval=&quot;10s&quot; on-fail=&quot;restart&quot;    op stop    timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;block&quot;
# 设置pgsql集群资源
# pgctl、psql、pgdata和config等配置根据自己的环境修改
pcs -f pgsql_cfg resource create pgsql pgsql pgctl=&quot;/opt/pgsql/bin/pg_ctl&quot; psql=&quot;/opt/pgsql/bin/psql&quot; pgdata=&quot;/data/postgresql/data/&quot; config=&quot;/data/postgresql/data/postgresql.conf&quot; rep_mode=&quot;sync&quot; node_list=&quot;zhaopin-5-90 zhaopin-5-91 zhaopin-5-92&quot; master_ip=&quot;172.17.5.98&quot;  repuser=&quot;replicator&quot; primary_conninfo_opt=&quot;password=8d5e9531-3817-460d-a851-659d2e51ca99 keepalives_idle=60 keepalives_interval=5 keepalives_count=5&quot; restore_command=&quot;cp /data/postgresql/xlog_archive/%f %p&quot; restart_on_promote=&apos;true&apos; op start   timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;restart&quot; op monitor timeout=&quot;60s&quot; interval=&quot;4s&quot; on-fail=&quot;restart&quot; op monitor timeout=&quot;60s&quot; interval=&quot;3s&quot;  on-fail=&quot;restart&quot; role=&quot;Master&quot; op promote timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;restart&quot; op demote  timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;stop&quot; op stop    timeout=&quot;60s&quot; interval=&quot;0s&quot;  on-fail=&quot;block&quot;
 # 设置master/slave模式
pcs -f pgsql_cfg resource master pgsql-cluster pgsql master-max=1 master-node-max=1 clone-max=3 clone-node-max=1 notify=true
# 配置master ip组
pcs -f pgsql_cfg resource group add master-group vip-master
# 配置slave ip组
pcs -f pgsql_cfg resource group add slave-group vip-slave
# 配置master ip组绑定master节点
pcs -f pgsql_cfg constraint colocation add master-group with master pgsql-cluster INFINITY
# 配置启动master节点
pcs -f pgsql_cfg constraint order promote pgsql-cluster then start master-group symmetrical=false score=INFINITY
# 配置停止master节点
pcs -f pgsql_cfg constraint order demote  pgsql-cluster then stop  master-group symmetrical=false score=0
# 配置slave ip组绑定slave节点
pcs -f pgsql_cfg constraint colocation add slave-group with slave pgsql-cluster INFINITY
# 配置启动slave节点
pcs -f pgsql_cfg constraint order promote pgsql-cluster then start slave-group symmetrical=false score=INFINITY
# 配置停止slave节点
pcs -f pgsql_cfg constraint order demote  pgsql-cluster then stop  slave-group symmetrical=false score=0
# 把配置文件push到cib
pcs cluster cib-push pgsql_cfg
</code></pre><p>2）执行操作文件</p>
<pre><code>$ sudo sh cluster_setup.sh
</code></pre><h3 id="2-查看状态"><a href="#2-查看状态" class="headerlink" title="(2) 查看状态"></a>(2) 查看状态</h3><p>1）查看cluster状态</p>
<p>在node1上执行:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Mon Oct 19 15:10:52 2015          Last change: Mon Oct 19 15:10:12 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-90 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-cluster [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-90 zhaopin-5-91 ]
 Resource Group: master-group
     vip-master (ocf::heartbeat:IPaddr2):       Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave  (ocf::heartbeat:IPaddr2):       Started zhaopin-5-90
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/disabled
</code></pre><p>2）查看PostgreSQL集群状态</p>
<p>在node3上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid |  usename   | application_name |  client_addr  | client_hostname | client_port |         backend_start         | backend_xmin |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+------------+------------------+---------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 11522 |    16384 | replicator | zhaopin-5-91     | 172.17.5.91   |                 |       41356 | 2015-10-19 07:10:01.898257+00 |         1915 | streaming | 81/D9000000   | 81/D9000000    | 81/D9000000    | 81/D9000000     |             2 | potential
 11532 |    16384 | replicator | zhaopin-5-90     | 172.17.5.99   |                 |       41786 | 2015-10-19 07:10:01.945532+00 |         1915 | streaming | 81/D9000000   | 81/D9000000    | 81/D9000000    | 81/D9000000     |             1 | sync
(2 rows)
</code></pre><h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h2><p><a href="http://clusterlabs.org/doc/zh-CN/Pacemaker/1.1-pcs/html-single/Clusters_from_Scratch/index.html#_verify_corosync_installation" target="_blank" rel="external">从头开始搭建集群</a></p>
<p><a href="http://clusterlabs.org/wiki/PgSQL_Replicated_Cluster" target="_blank" rel="external">PgSQL Replicated Cluster</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL事务特性之嵌套事务]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E4%BA%8B%E5%8A%A1%E7%89%B9%E6%80%A7%E4%B9%8B%E5%B5%8C%E5%A5%97%E4%BA%8B%E5%8A%A1/</url>
      <content type="html"><![CDATA[<p>嵌套事务的实现是基于SAVEPOINT、ROLLBACK TO SAVEPOINT和RELEASE SAVEPOINT的,也就是设置一个保存点,可以回滚到保存点和释放保存点。</p>
<p>测试表的初始状态如下:</p>
<a id="more"></a>
<pre><code>postgres=# \d test
     Table &quot;public.test&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
 name   | text    |

postgres=# select * from test ;
 id | name
----+------
(0 rows)
</code></pre><p>开始测试</p>
<pre><code>postgres=# begin ;
BEGIN
postgres=# insert into test values (1, &apos;a&apos;);
INSERT 0 1
postgres=# savepoint insert_a;
SAVEPOINT
postgres=# select * from test ;
 id | name
----+------
  1 | a
(1 row)

postgres=# insert into test values (2, &apos;b&apos;);
INSERT 0 1
postgres=# savepoint insert_b;
SAVEPOINT
postgres=# select * from test ;
 id | name
----+------
  1 | a
  2 | b
(2 rows)

postgres=# insert into test values (3, &apos;c&apos;);
INSERT 0 1
postgres=# select * from test ;
 id | name
----+------
  1 | a
  2 | b
  3 | c
(3 rows)
</code></pre><p>现在定义了两个SAVEPOINT,并且插入了3条数据,现在测试ROLLBACK TO SAVEPOINT</p>
<pre><code>postgres=# rollback to insert_b;
ROLLBACK
postgres=# select * from test ;
 id | name
----+------
  1 | a
  2 | b
(2 rows)

postgres=# rollback to insert_a;
ROLLBACK
postgres=# select * from test ;
 id | name
----+------
  1 | a
(1 row)
</code></pre><p>可见回滚到前面定义的保存点成功了。</p>
<p>如果回滚到前面的保存点,后面的更改就丢失了,包括保存点,比如回滚到insert_a,那么在insert_a之后的数据就没有了,insert_b这个保存点也不存在了。</p>
<pre><code>postgres=# rollback to insert_a;
ROLLBACK
postgres=# select * from test ;
 id | name
----+------
  1 | a
(1 row)

postgres=# rollback to insert_b;
ERROR:  no such savepoint
</code></pre><p>测试RELEASE SAVEPOINT</p>
<pre><code>postgres=# select * from test ;
 id | name
----+------
(0 rows)

postgres=# begin ;
BEGIN
postgres=# insert into test values (1, &apos;a&apos;);
INSERT 0 1
postgres=# savepoint insert_a;
SAVEPOINT
postgres=# select * from test ;
 id | name
----+------
  1 | a
(1 row)

postgres=# insert into test values (2, &apos;b&apos;);
INSERT 0 1
postgres=# savepoint insert_b;
SAVEPOINT
postgres=# select * from test ;
 id | name
----+------
  1 | a
  2 | b
(2 rows)

postgres=# release insert_a;
RELEASE
postgres=# select * from test ;
 id | name
----+------
  1 | a
  2 | b
(2 rows)

postgres=# rollback to insert_a;
ERROR:  no such savepoint
</code></pre><p>保存点被释放后就不能再回滚到该保存点了。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Pacemaker+Corosync+PostgreSQL集群故障测试和解决方案]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-Pacemaker+Corosync+2016-07-05-PostgreSQL%E9%9B%86%E7%BE%A4%E6%95%85%E9%9A%9C%E6%B5%8B%E8%AF%95%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1.环境"></a>1.环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-5-90 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 11:02:08 2015Last change: Tue Oct 20 10:57:37 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
</code></pre><a id="more"></a>
<p>  Full list of resources:<br>     Master/Slave Set: pgsql-master [pgsql]<br>         Masters: [ zhaopin-5-92 ]<br>         Slaves: [ zhaopin-5-90 zhaopin-5-91 ]<br>     Resource Group: master-group<br>         vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92<br>     Resource Group: slave-group<br>         vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-90<br>    PCSD Status:<br>      zhaopin-5-90 (172.17.5.90): Online<br>      zhaopin-5-91 (172.17.5.91): Online<br>      zhaopin-5-92 (172.17.5.92): Online<br>    Daemon Status:<br>      corosync: active/disabled<br>      pacemaker: active/disabled<br>      pcsd: active/enabled</p>
<p>node1: 172.17.5.90<br>node2: 172.17.5.91<br>node3: 172.17.5.92<br>node4: 172.17.5.93 应急</p>
<p>vip-master: 172.17.5.99<br>vip-slave:  172.17.5.98</p>
<h2 id="2-PostgreSQL故障"><a href="#2-PostgreSQL故障" class="headerlink" title="2.PostgreSQL故障"></a>2.PostgreSQL故障</h2><h3 id="1-单节点故障"><a href="#1-单节点故障" class="headerlink" title="(1) 单节点故障"></a>(1) 单节点故障</h3><p>1）master故障</p>
<p>在node3上执行:</p>
<pre><code>$ ps aux | grep postgres | grep -v grep
postgres  29482  0.0  0.2 103216  7900 ?        S    10:57   0:00 /usr/bin/postgres -D /data/postgresql/data -c config_file=/data/postgresql/data/postgresql.conf
postgres  29524  0.0  0.0 103216  1980 ?        Ss   10:57   0:00 postgres: checkpointer process
postgres  29525  0.0  0.0 103216  1988 ?        Ss   10:57   0:00 postgres: writer process
postgres  29526  0.0  0.0 103216  1896 ?        Ss   10:57   0:00 postgres: wal writer process
postgres  29527  0.0  0.0 104044  3008 ?        Ss   10:57   0:00 postgres: autovacuum launcher process
postgres  29528  0.0  0.0  88912  1792 ?        Ss   10:57   0:00 postgres: archiver process   last was 00000001000000000000000E
postgres  29529  0.0  0.0  89024  1920 ?        Ss   10:57   0:00 postgres: stats collector process
postgres  29790  0.0  0.0 104060  3208 ?        Ss   10:57   0:00 postgres: wal sender process postgres 172.17.5.90(35734) streaming 0/F0000E0
postgres  29791  0.0  0.0 104192  3392 ?        Ss   10:57   0:00 postgres: wal sender process postgres 172.17.5.91(60722) streaming 0/F0000E0
$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 29790 |       10 | postgres | zhaopin-5-90     | 172.17.5.90 |                 |       35734 | 2015-10-20 02:57:25.718047+00 | streaming | 0/F0000E0     | 0/F0000E0      | 0/F0000E0      | 0/F0000E0       |             1 | sync
 29791 |       10 | postgres | zhaopin-5-91     | 172.17.5.91 |                 |       60722 | 2015-10-20 02:57:25.718566+00 | streaming | 0/F0000E0     | 0/F0000E0      | 0/F0000E0      | 0/F0000E0       |             2 | potential
(2 rows)
postgres=# \q
</code></pre><p>杀掉PostgreSQL的进程</p>
<pre><code>$ sudo killall postgres
$ ps aux | grep postgres | grep -v grep
</code></pre><p>PostgreSQL的进程已经没有了</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 11:11:54 2015Last change: Tue Oct 20 11:10:31 2015 by root via crm_attribute on zhaopin-5-90
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-90 ]
     Slaves: [ zhaopin-5-91 ]
     Stopped: [ zhaopin-5-92 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-90
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
Failed Actions:
* pgsql_start_0 on zhaopin-5-92 &apos;unknown error&apos; (1): call=33, status=complete, exitreason=&apos;My data may be inconsistent. You have to remove /var/lib/pgsql/tmp/PGSQL.lock file to force start.&apos;,
    last-rc-change=&apos;Tue Oct 20 11:10:14 2015&apos;, queued=0ms, exec=389ms
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>查看集群状态可见,master和vip-master已经切换到node1上了</p>
<p>在node1上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 59835 |       10 | postgres | zhaopin-5-91     | 172.17.5.98 |                 |       46393 | 2015-10-20 03:10:24.392231+00 | streaming | 0/F000140     | 0/F000140      | 0/F000140      | 0/F000140       |             1 | sync
(1 row)
postgres=# create table test ( id bigint );
CREATE TABLE
postgres=# \dt test
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | test | table | postgres
(1 row)
postgres=# \q
</code></pre><p>可以正常提供服务</p>
<p>恢复node3为新master node1的slave,在node3上执行</p>
<pre><code>$ sudo su - postgres
Last login: Tue Oct 20 11:10:16 CST 2015
$ rm -f /var/lib/pgsql/tmp/PGSQL.lock
$ rm -fr /data/postgresql/data/*
$ pg_basebackup -h 172.17.5.90 -U postgres -D /data/postgresql/data/ -X stream -P
20243/20243 kB (100%), 1/1 tablespace
$ logout
$ sudo pcs resource cleanup pgsql-master
Resource: pgsql-master successfully cleaned up
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 11:19:45 2015Last change: Tue Oct 20 11:19:45 2015 by root via crm_attribute on zhaopin-5-90
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-90 ]
     Slaves: [ zhaopin-5-91 zhaopin-5-92 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-90
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>可见node3已经加到集群里面了。如果配置archive_command和restore_command的话,就不用删除数据目录,按照下面pacemaker故障的解决方法就行。</p>
<p>2）slave故障</p>
<p>在node2上执行:</p>
<pre><code>$ ps aux | grep postgres | grep -v grep
postgres  35611  0.0  0.2 103216  7876 ?        S    10:57   0:00 /usr/bin/postgres -D /data/postgresql/data -c config_file=/data/postgresql/data/postgresql.conf
postgres  35637  0.0  0.0 103280  2392 ?        Ss   10:57   0:00 postgres: startup process   recovering 000000010000000000000011
postgres  35656  0.0  0.0 103216  2144 ?        Ss   10:57   0:00 postgres: checkpointer process
postgres  35657  0.0  0.0 103216  1648 ?        Ss   10:57   0:00 postgres: writer process
postgres  35658  0.0  0.0  88912  1512 ?        Ss   10:57   0:00 postgres: stats collector process
postgres  55111  0.1  0.0 110228  3364 ?        Ss   11:10   0:00 postgres: wal receiver process   streaming 0/12000000
$ sudo killall postgres
$ ps aux | grep postgres | grep -v grep
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 11:22:57 2015Last change: Tue Oct 20 11:22:53 2015 by root via crm_attribute on zhaopin-5-90
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-90 ]
     Slaves: [ zhaopin-5-91 zhaopin-5-92 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-90
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
Failed Actions:
* pgsql_monitor_4000 on zhaopin-5-91 &apos;unknown error&apos; (1): call=47, status=complete, exitreason=&apos;none&apos;,
    last-rc-change=&apos;Tue Oct 20 11:22:51 2015&apos;, queued=0ms, exec=0ms
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>可见vip-slave自动切换到了node3</p>
<p>在node1上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# create table test ( id bigint );
ERROR:  relation &quot;test&quot; already exists
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |        backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 99082 |       10 | postgres | zhaopin-5-91     | 172.17.5.91 |                 |       60726 | 2015-10-20 03:22:55.84451+00 | streaming | 0/120000B8    | 0/120000B8     | 0/120000B8     | 0/120000B8      |             2 | potential
 88344 |       10 | postgres | zhaopin-5-92     | 172.17.5.92 |                 |       60891 | 2015-10-20 03:19:43.1618+00  | streaming | 0/120000B8    | 0/120000B8     | 0/120000B8     | 0/120000B8      |             1 | sync
(2 rows)
postgres=# \q
</code></pre><p>可见同步复制节点切换到了node3上</p>
<h3 id="2-两节点故障"><a href="#2-两节点故障" class="headerlink" title="(2) 两节点故障"></a>(2) 两节点故障</h3><p>1）master和slave故障</p>
<p>在node1和node2执行:</p>
<pre><code>$ sudo killall postgres
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 13:47:01 2015Last change: Tue Oct 20 13:47:00 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Stopped: [ zhaopin-5-90 zhaopin-5-91 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Stopped
Failed Actions:
* pgsql_start_0 on zhaopin-5-90 &apos;unknown error&apos; (1): call=50, status=complete, exitreason=&apos;My data may be inconsistent. You have to remove /var/lib/pgsql/tmp/PGSQL.lock file to force start.&apos;,
    last-rc-change=&apos;Tue Oct 20 13:46:53 2015&apos;, queued=0ms, exec=285ms
* pgsql_start_0 on zhaopin-5-91 &apos;unknown error&apos; (1): call=34, status=complete, exitreason=&apos;My data may be inconsistent. You have to remove /var/lib/pgsql/tmp/PGSQL.lock file to force start.&apos;,
    last-rc-change=&apos;Tue Oct 20 13:46:33 2015&apos;, queued=0ms, exec=343ms
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>可见node1和node2停机了,node3自动切成了master,vip-slave也失效了</p>
<p>在node3上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
 pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | state | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-----+----------+---------+------------------+-------------+-----------------+-------------+---------------+-------+---------------+----------------+----------------+-----------------+---------------+------------
(0 rows)
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | test | table | postgres
(1 row)
postgres=# drop table test ;
DROP TABLE
postgres=# \q
</code></pre><p>可见master可以正常提供服务</p>
<p>将两个节点变成新master node3的slave,解决方法如上面单节点故障-&gt;master节点的一样,结果如下:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 13:53:27 2015Last change: Tue Oct 20 13:53:16 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-90 zhaopin-5-91 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-90
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>2）两个slave故障</p>
<p>在node1和node2执行:</p>
<pre><code>$ sudo killall postgres
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 13:57:23 2015Last change: Tue Oct 20 13:57:20 2015 by hacluster via crmd on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-90 zhaopin-5-91 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-90
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>可见集群会自动恢复</p>
<h2 id="3-Pacemaker故障"><a href="#3-Pacemaker故障" class="headerlink" title="3.Pacemaker故障"></a>3.Pacemaker故障</h2><h3 id="1-单节点故障-1"><a href="#1-单节点故障-1" class="headerlink" title="(1) 单节点故障"></a>(1) 单节点故障</h3><p>1）停掉Pacemaker服务</p>
<p>在node1上执行:</p>
<pre><code>$ systemctl status pacemaker
pacemaker.service - Pacemaker High Availability Cluster Manager
   Loaded: loaded (/usr/lib/systemd/system/pacemaker.service; disabled)
   Active: active (running) since Tue 2015-10-20 13:45:28 CST; 15min ago
 Main PID: 35622 (pacemakerd)
   CGroup: /system.slice/pacemaker.service
           ├─35622 /usr/sbin/pacemakerd -f
           ├─35623 /usr/libexec/pacemaker/cib
           ├─35624 /usr/libexec/pacemaker/stonithd
           ├─35625 /usr/libexec/pacemaker/lrmd
           ├─35626 /usr/libexec/pacemaker/attrd
           ├─35627 /usr/libexec/pacemaker/pengine
           └─35628 /usr/libexec/pacemaker/crmd
$ sudo systemctl stop pacemaker
$ systemctl status pacemaker
pacemaker.service - Pacemaker High Availability Cluster Manager
   Loaded: loaded (/usr/lib/systemd/system/pacemaker.service; disabled)
   Active: inactive (dead)
$ sudo pcs status
Error: cluster is not currently running on this node
</code></pre><p>2）查看状态</p>
<p>在node2上执行:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:02:41 2015Last change: Tue Oct 20 14:01:06 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-91 zhaopin-5-92 ]
OFFLINE: [ zhaopin-5-90 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-91 ]
     Stopped: [ zhaopin-5-90 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  172.17.5.90: Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>可见node1的集群和PostgreSQL都宕机了,vip-slave切换到了node2上</p>
<p>3）验证可用性</p>
<p>在node3上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 78298 |       10 | postgres | zhaopin-5-91     | 172.17.5.91 |                 |       60755 | 2015-10-20 05:57:08.882432+00 | streaming | 0/16000150    | 0/16000150     | 0/16000150     | 0/16000150      |             1 | sync
(1 row)
postgres=# create table test ( id int );
CREATE TABLE
postgres=# \q
</code></pre><p>可见集群仍然可用</p>
<p>4）解决方案</p>
<p>在node1上执行:</p>
<pre><code>$ sudo pcs cluster start
Starting Cluster...
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:07:54 2015Last change: Tue Oct 20 14:07:51 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-90 zhaopin-5-91 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>在node3上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid   | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
--------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 110962 |       10 | postgres | zhaopin-5-90     | 172.17.5.90 |                 |       35910 | 2015-10-20 06:07:51.026655+00 | streaming | 0/16013738    | 0/16013738     | 0/16013738     | 0/16013738      |             2 | potential
  78298 |       10 | postgres | zhaopin-5-91     | 172.17.5.91 |                 |       60755 | 2015-10-20 05:57:08.882432+00 | streaming | 0/16013738    | 0/16013738     | 0/16013738     | 0/16013738      |             1 | sync
(2 rows)
postgres=# \q
</code></pre><p>可见已经恢复</p>
<h3 id="2-两节点故障-1"><a href="#2-两节点故障-1" class="headerlink" title="(2) 两节点故障"></a>(2) 两节点故障</h3><p>1）停掉Pacemaker服务</p>
<p>在node1和node3上执行:</p>
<pre><code>$ sudo systemctl stop pacemaker
</code></pre><p>2）查看状态</p>
<p>在node2上执行:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:11:34 2015Last change: Tue Oct 20 14:10:59 2015 by root via crm_attribute on zhaopin-5-91
Stack: corosync
Current DC: zhaopin-5-91 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-91 ]
OFFLINE: [ zhaopin-5-90 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-91 ]
     Stopped: [ zhaopin-5-90 zhaopin-5-92 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Stopped
PCSD Status:
  172.17.5.90: Online
  zhaopin-5-91 (172.17.5.91): Online
  172.17.5.92: Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>3）验证可用性</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
 pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | state | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-----+----------+---------+------------------+-------------+-----------------+-------------+---------------+-------+---------------+----------------+----------------+-----------------+---------------+------------
(0 rows)
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | test | table | postgres
(1 row)
postgres=# drop table test ;
DROP TABLE
postgres=# \q
</code></pre><p>可见集群仍然可用</p>
<p>4）解决方案</p>
<p>对于原来的slave节点故障如上面单节点故障处理方法一样,原master节点操作如下:</p>
<pre><code>$ sudo rm -f /var/lib/pgsql/tmp/PGSQL.lock
$ sudo pcs cluster start
Starting Cluster...
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:17:32 2015Last change: Tue Oct 20 14:17:31 2015 by root via crm_attribute on zhaopin-5-91
Stack: corosync
Current DC: zhaopin-5-91 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-91 ]
     Slaves: [ zhaopin-5-90 zhaopin-5-92 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-90
PCSD Status:
  zhaopin-5-90 (172.17.5.90): Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>在node2上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 54687 |       10 | postgres | zhaopin-5-90     | 172.17.5.98 |                 |       40297 | 2015-10-20 06:17:02.084825+00 | streaming | 0/17005B08    | 0/17005B08     | 0/17005B08     | 0/17005B08      |             1 | sync
 56147 |       10 | postgres | zhaopin-5-92     | 172.17.5.92 |                 |       32904 | 2015-10-20 06:17:29.874711+00 | streaming | 0/17005B08    | 0/17005B08     | 0/17005B08     | 0/17005B08      |             2 | potential
(2 rows)
postgres=# \q
</code></pre><h2 id="4-Corosync故障"><a href="#4-Corosync故障" class="headerlink" title="4.Corosync故障"></a>4.Corosync故障</h2><h3 id="1-单节点故障-2"><a href="#1-单节点故障-2" class="headerlink" title="(1) 单节点故障"></a>(1) 单节点故障</h3><p>1）停掉Corosync服务</p>
<p>在node1上执行:</p>
<pre><code>$ systemctl status corosync.service
corosync.service - Corosync Cluster Engine
   Loaded: loaded (/usr/lib/systemd/system/corosync.service; disabled)
   Active: active (running) since Tue 2015-10-20 13:45:28 CST; 35min ago
  Process: 35600 ExecStart=/usr/share/corosync/corosync start (code=exited, status=0/SUCCESS)
 Main PID: 35607 (corosync)
   CGroup: /system.slice/corosync.service
           └─35607 corosync
$ sudo systemctl stop corosync.service
$ systemctl status corosync.service
corosync.service - Corosync Cluster Engine
   Loaded: loaded (/usr/lib/systemd/system/corosync.service; disabled)
   Active: inactive (dead)
$ sudo pcs status
Error: cluster is not currently running on this node
</code></pre><p>2）查看状态</p>
<p>在node2上执行:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:22:36 2015Last change: Tue Oct 20 14:21:37 2015 by root via crm_attribute on zhaopin-5-91
Stack: corosync
Current DC: zhaopin-5-91 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-91 zhaopin-5-92 ]
OFFLINE: [ zhaopin-5-90 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-91 ]
     Slaves: [ zhaopin-5-92 ]
     Stopped: [ zhaopin-5-90 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
PCSD Status:
  172.17.5.90: Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>3）验证可用性</p>
<p>在node2上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 56147 |       10 | postgres | zhaopin-5-92     | 172.17.5.92 |                 |       32904 | 2015-10-20 06:17:29.874711+00 | streaming | 0/17005BA0    | 0/17005BA0     | 0/17005BA0     | 0/17005BA0      |             1 | sync
(1 row)
postgres=# \dt
No relations found.
postgres=# create table test ( id bigint );
CREATE TABLE
postgres=# \q
</code></pre><p>可见集群仍然可用</p>
<p>4）解决方案</p>
<p>如同Pacemaker单节点故障解决方案</p>
<h3 id="2-两节点故障-2"><a href="#2-两节点故障-2" class="headerlink" title="(2) 两节点故障"></a>(2) 两节点故障</h3><p>在node1和node2上执行:</p>
<pre><code>$ sudo systemctl stop corosync.service
$ systemctl status corosync.service
corosync.service - Corosync Cluster Engine
   Loaded: loaded (/usr/lib/systemd/system/corosync.service; disabled)
   Active: inactive (dead)
$ sudo pcs status
Error: cluster is not currently running on this node
</code></pre><p>2）查看状态</p>
<p>在node3上执行:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:27:17 2015Last change: Tue Oct 20 14:26:35 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition WITHOUT quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-92 ]
OFFLINE: [ zhaopin-5-90 zhaopin-5-91 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Stopped: [ zhaopin-5-90 zhaopin-5-91 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Stopped
PCSD Status:
  172.17.5.90: Online
  172.17.5.91: Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>3）验证可用性</p>
<p>在node3上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
 pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | state | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-----+----------+---------+------------------+-------------+-----------------+-------------+---------------+-------+---------------+----------------+----------------+-----------------+---------------+------------
(0 rows)
postgres=# \dt
        List of relations
 Schema | Name | Type  |  Owner
--------+------+-------+----------
 public | test | table | postgres
(1 row)
postgres=# drop table test ;
DROP TABLE
postgres=# \q
</code></pre><p>4）解决方案</p>
<p>如同Pacemaker故障中的两节点故障</p>
<h2 id="5-服务器故障"><a href="#5-服务器故障" class="headerlink" title="5.服务器故障"></a>5.服务器故障</h2><h3 id="1-停掉机器"><a href="#1-停掉机器" class="headerlink" title="(1) 停掉机器"></a>(1) 停掉机器</h3><p>在node1上执行:</p>
<pre><code>$ sudo shutdown -h now
</code></pre><h3 id="2-查看状态"><a href="#2-查看状态" class="headerlink" title="(2) 查看状态"></a>(2) 查看状态</h3><p>在node3上执行:</p>
<pre><code>$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 14:33:47 2015Last change: Tue Oct 20 14:33:22 2015 by root via crm_attribute on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-92 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 5 resources configured
Online: [ zhaopin-5-91 zhaopin-5-92 ]
OFFLINE: [ zhaopin-5-90 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-91 ]
     Stopped: [ zhaopin-5-90 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  172.17.5.90: Offline
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><h3 id="3-验证可用性"><a href="#3-验证可用性" class="headerlink" title="(3) 验证可用性"></a>(3) 验证可用性</h3><p>在node3上执行:</p>
<pre><code>$ psql -U postgres
psql (9.2.13)
Type &quot;help&quot; for help.
postgres=# select * from pg_stat_replication ;
  pid  | usesysid | usename  | application_name | client_addr | client_hostname | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
-------+----------+----------+------------------+-------------+-----------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
 15678 |       10 | postgres | zhaopin-5-91     | 172.17.5.91 |                 |       60771 | 2015-10-20 06:30:11.747191+00 | streaming | 0/18006120    | 0/18006120     | 0/18006120     | 0/18006120      |             1 | sync
(1 row)
postgres=# \dt
No relations found.
postgres=# create table test ( id bigint );
CREATE TABLE
postgres=# \q
</code></pre><p>可见集群仍然可用</p>
<h3 id="4-解决方案"><a href="#4-解决方案" class="headerlink" title="(4) 解决方案"></a>(4) 解决方案</h3><p>在node4上执行:</p>
<pre><code>$ sudo yum install -y -q pacemaker pcs psmisc policycoreutils-python
$ sudo setenforce 0
$ sudo sed -i.bak &quot;s/SELINUX=enforcing/SELINUX=permissive/g&quot; /etc/selinux/config
$ sudo systemctl disable firewalld.service
$ sudo systemctl stop firewalld.service
$ sudo iptables --flush
$ sudo systemctl start pcsd.service
$ sudo systemctl enable pcsd.service
ln -s &apos;/usr/lib/systemd/system/pcsd.service&apos; &apos;/etc/systemd/system/multi-user.target.wants/pcsd.service&apos;
$ sudo passwd hacluster
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.
$ sudo mkdir -p /data/postgresql/data
$ sudo mkdir -p /data/postgresql/xlog_archive
$ sudo chown -R postgres:postgres /data/postgresql/
$ sudo chmod 0700 /data/postgresql/data
$ sudo su - postgres
$ pg_basebackup -h 172.17.5.92 -U postgres -D /data/postgresql/data/ -X stream -P
</code></pre><p>20245/20245 kB (100%), 1/1 tablespace</p>
<p>在node2上执行:</p>
<pre><code>$ sudo pcs cluster auth 172.17.5.90 172.17.5.91 172.17.5.92 172.17.5.93 -u hacluster -p hacluster
172.17.5.90: Authorized
172.17.5.91: Authorized
172.17.5.92: Authorized
172.17.5.93: Authorized
$ sudo pcs cluster node add 172.17.5.93 --start
172.17.5.90: Corosync updated
172.17.5.91: Corosync updated
172.17.5.92: Corosync updated
172.17.5.93: Succeeded
172.17.5.93: Starting Cluster...
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 16:07:12 2015Last change: Tue Oct 20 16:01:06 2015 by hacluster via crmd on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-91 (version 1.1.13-a14efad) - partition with quorum
4 nodes and 5 resources configured
Online: [ zhaopin-5-91 zhaopin-5-92 zhaopin-5-93 ]
OFFLINE: [ zhaopin-5-90 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-92 ]
     Slaves: [ zhaopin-5-91 ]
     Stopped: [ zhaopin-5-90 zhaopin-5-93 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-92
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  172.17.5.90: Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
  zhaopin-5-93 (172.17.5.93): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><p>更新PostgreSQL集群,添加新加的节点,会有闪断</p>
<pre><code>$ sudo pcs resource update pgsql-master pgsql master-max=1 master-node-max=1 clone-max=4 clone-node-max=1 notify=true
$ sudo pcs resource update pgsql pgsql node_list=&quot;zhaopin-5-90 zhaopin-5-91 zhaopin-5-92 zhaopin-5-93&quot;
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 16:13:55 2015Last change: Tue Oct 20 16:13:32 2015 by hacluster via crmd on zhaopin-5-91
Stack: corosync
Current DC: zhaopin-5-91 (version 1.1.13-a14efad) - partition with quorum
4 nodes and 6 resources configured
Online: [ zhaopin-5-91 zhaopin-5-92 zhaopin-5-93 ]
OFFLINE: [ zhaopin-5-90 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-93 ]
     Slaves: [ zhaopin-5-91 zhaopin-5-92 ]
     Stopped: [ zhaopin-5-90 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-93
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  172.17.5.90: Online
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
  zhaopin-5-93 (172.17.5.93): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
$ sudo pcs cluster node remove 172.17.5.90
172.17.5.90: Stopping Cluster (pacemaker)...
172.17.5.90: Successfully destroyed cluster
172.17.5.91: Corosync updated
172.17.5.92: Corosync updated
172.17.5.93: Corosync updated
$ sudo crmadmin -N
(null) node: zhaopin-5-92 (3)
(null) node: zhaopin-5-91 (2)
(null) node: zhaopin-5-93 (4)
$ sudo crm_node -R zhaopin-5-90 --force
$ sudo crmadmin -N
(null) node: zhaopin-5-92 (3)
(null) node: zhaopin-5-91 (2)
(null) node: zhaopin-5-93 (4)
$ sudo pcs status
Cluster name: pgcluster
WARNING: corosync and pacemaker node names do not match (IPs used in setup?)
Last updated: Tue Oct 20 16:58:05 2015Last change: Tue Oct 20 16:56:06 2015 by root via crm_node on zhaopin-5-92
Stack: corosync
Current DC: zhaopin-5-91 (version 1.1.13-a14efad) - partition with quorum
3 nodes and 6 resources configured
Online: [ zhaopin-5-91 zhaopin-5-92 zhaopin-5-93 ]
Full list of resources:
 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ zhaopin-5-93 ]
     Slaves: [ zhaopin-5-91 zhaopin-5-92 ]
 Resource Group: master-group
     vip-master(ocf::heartbeat:IPaddr2):Started zhaopin-5-93
 Resource Group: slave-group
     vip-slave(ocf::heartbeat:IPaddr2):Started zhaopin-5-91
PCSD Status:
  zhaopin-5-91 (172.17.5.91): Online
  zhaopin-5-92 (172.17.5.92): Online
  zhaopin-5-93 (172.17.5.93): Online
Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
</code></pre><h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6.结论"></a>6.结论</h2><p>集群中还剩最后一台机器仍能提供服务</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[PostgreSQL表空间的使用]]></title>
      <url>http://panwenhang.github.io/2016/07/05/PostgreSQL/2016-07-05-PostgreSQL%E8%A1%A8%E7%A9%BA%E9%97%B4%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      <content type="html"><![CDATA[<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>表空间的作用就是允许数据库管理员定义一个其他非数据目录的位置存储数据库对象。</p>
<p>使用场景之一是,如果机器新加了ssd,但是又不够整个实例使用,可以将一些重要和使用频率高的表和索引放到ssd上,提高查询效率</p>
<a id="more"></a>
<h2 id="2-创建表空间"><a href="#2-创建表空间" class="headerlink" title="2. 创建表空间"></a>2. 创建表空间</h2><p>先创建要保存表空间的目录</p>
<pre><code># mkdir -p /export/tablespace1
# chown -R postgres:postgres /export/tablespace1/
</code></pre><p>进入数据库</p>
<pre><code>postgres=# \db
       List of tablespaces
    Name    |  Owner   | Location
------------+----------+----------
 pg_default | postgres |
 pg_global  | postgres |
(2 rows)
</code></pre><p>数据里面只有两个默认的表空间,还没有其他的表空间,现在创建一个表空间</p>
<pre><code>postgres=# CREATE TABLESPACE tbspl1 LOCATION &apos;/export/tablespace1&apos;;
CREATE TABLESPACE
postgres=# \db
             List of tablespaces
    Name    |  Owner   |      Location
------------+----------+---------------------
 pg_default | postgres |
 pg_global  | postgres |
 tbspl1     | postgres | /export/tablespace1
(3 rows)
</code></pre><h2 id="3-使用表空间"><a href="#3-使用表空间" class="headerlink" title="3. 使用表空间"></a>3. 使用表空间</h2><p>创建一个表table_a并查看其位置</p>
<pre><code>postgres=# create table table_a (id int);
CREATE TABLE
postgres=# select pg_relation_filepath(&apos;table_a&apos;);
 pg_relation_filepath
----------------------
 base/13003/17031
(1 row)
</code></pre><p>可以看出是在数据目录的base目录下,将其修改到表空间tbspl1</p>
<pre><code>postgres=# alter table table_a set tablespace tbspl1 ;
ALTER TABLE
postgres=# select pg_relation_filepath(&apos;table_a&apos;);
             pg_relation_filepath
----------------------------------------------
 pg_tblspc/17030/PG_9.4_201409291/13003/17037
(1 row)
</code></pre><p>可见表转移到了表空间所在的目录了</p>
<p>创建使用表空间的表:</p>
<pre><code>postgres=# create table table_b (id int) tablespace tbspl1;
CREATE TABLE
postgres=# select pg_relation_filepath(&apos;table_b&apos;);
             pg_relation_filepath
----------------------------------------------
 pg_tblspc/17030/PG_9.4_201409291/13003/17038
(1 row)
</code></pre><p>对于默认创建的索引是还是在数据目录下的,跟表没有关系</p>
<pre><code>postgres=# create index on table_b (id);
CREATE INDEX
postgres=# \d table_b
    Table &quot;public.table_b&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
Indexes:
    &quot;table_b_id_idx&quot; btree (id)
Tablespace: &quot;tbspl1&quot;

postgres=# select pg_relation_filepath(&apos;table_b_id_idx&apos;);
 pg_relation_filepath
----------------------
 base/13003/17041
(1 row)
</code></pre><p>创建使用表空间的索引</p>
<pre><code>postgres=# create index on table_a (id) tablespace tbspl1;
CREATE INDEX
postgres=# \d table_a
    Table &quot;public.table_a&quot;
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer |
Indexes:
    &quot;table_a_id_idx&quot; btree (id), tablespace &quot;tbspl1&quot;
Tablespace: &quot;tbspl1&quot;

postgres=# select pg_relation_filepath(&apos;table_a_id_idx&apos;);
             pg_relation_filepath
----------------------------------------------
 pg_tblspc/17030/PG_9.4_201409291/13003/17042
(1 row)
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux下打包]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-Linux%E4%B8%8B%E6%89%93%E5%8C%85/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><pre><code>打包是指将源码包编译成rpm包,在centos下通过rpmbuild命令和spec文件来实现。
相对于源码编译安装的好处是,不需要每次安装都编译一次,那样很慢,对于打好的rpm包,直接试用rpm命令就可以安装。
相对于一些官方提供的rpm包的优势在于,可以想源码编译一样定制化包的安装,比如修改安装的路径,添加一些自己的配置和文件等。
</code></pre><a id="more"></a>
<p>以下的打包以postgresql为例</p>
<h2 id="1-安装rpmbuild"><a href="#1-安装rpmbuild" class="headerlink" title="1. 安装rpmbuild"></a>1. 安装rpmbuild</h2><pre><code>$ sudo yum install -y rpm-build
</code></pre><h2 id="3-初始化打包目录"><a href="#3-初始化打包目录" class="headerlink" title="3. 初始化打包目录"></a>3. 初始化打包目录</h2><pre><code>$ sudo mkdir -p /data
$ cd /data/
$ sudo mkdir -p rpmbuild/{BUILD,BUILDROOT,RPMS,SRPMS,SOURCES,SPECS}
$ sudo chown -R $(whoami): rpmbuild
</code></pre><h2 id="4-准备spec文件"><a href="#4-准备spec文件" class="headerlink" title="4. 准备spec文件"></a>4. 准备spec文件</h2><pre><code>$ sudo mkdir -p rpmbuild/SPECS/postgresql/{9.4,conf}
$ cd rpmbuild/SPECS/postgresql/9.4/
$ vim pg94.el6.spec

    %define debug_package %{nil}

    Name: postgresql
    Summary: PostgreSQL server
    Version: 9.5.3
    Release: 1%{?dist}
    Source: postgresql-%{version}.tar.gz
    URL: http://www.postgresql.org/ftp/source/
    Group: Applications/Database
    Vendor: PostgreSQL
    License: Public Domain

    BuildRequires    : libxml2-devel
    BuildRequires    : libxslt-devel
    BuildRequires    : uuid-devel
    BuildRequires    : readline-devel
    BuildRequires    : openssl-devel
    BuildRequires    : python-devel
    BuildRequires    : pam-devel
    BuildRequires    : tcl-devel
    BuildRequires    : openldap-devel

    Requires    : tcl
    Requires    : perl-ExtUtils-Embed
    Requires    : libxml2
    Requires    : libxslt
    Requires    : readline
    Requires    : uuid
    Requires(pre)     : /usr/sbin/useradd

    %description
    Postgres is a truly awesome database. When we started working on Launchpad I wasn&apos;t sure if it would be up to the job. I was so wrong. It&apos;s been robust, fast, and professional in every regard.

    %prep
    %setup -q -n postgresql-%{version}
    env CFLAGS=&quot; -march=core2 -O2 &quot; ./configure --prefix=/opt/pg95 --with-python --with-perl --with-openssl --with-libxml --with-libxslt --with-ossp-uuid --with-pam --with-ldap --with-tcl --enable-thread-safety --with-segsize=128 --with-blocksize=32 --with-wal-segsize=64 --with-wal-blocksize=32

    %build
    make -s -j8 world

    %install
    rm -fr %{buildroot}
    make -s install DESTDIR=%{buildroot}
    make -s install-world DESTDIR=%{buildroot}
    install -d %{buildroot}/home/postgres
    install -d %{buildroot}/opt/pg95/etc
    install -d %{buildroot}/etc/ld.so.conf.d
    install -d %{buildroot}/etc/profile.d

    test -L %{buildroot}/opt/pgsql &amp;&amp; rm -f %{buildroot}/opt/pgsql
    ln -sf /opt/pg95 %{buildroot}/opt/pgsql

    echo &apos;/opt/pgsql/lib&apos; &gt; %{buildroot}/etc/ld.so.conf.d/pgsql.conf
    echo &apos;export PATH=/opt/pgsql/bin:$PATH&apos; &gt; %{buildroot}/etc/profile.d/pgsql.sh

    cat &gt; %{buildroot}/opt/pg95/etc/pg_hba.conf &lt;&lt;EOF
    host     all               postgres             127.0.0.1/32            md5
    host     all               monitor              127.0.0.1/32            md5
    host     all               postgres             0.0.0.0/0               reject
    host     all               monitor              0.0.0.0/0               reject
    local    all               all                                          md5
    host     replication       all                  0.0.0.0/0               md5
    host     all               all                  0.0.0.0/0               md5
    EOF

    cat &gt; %{buildroot}/opt/pg95/etc/recovery.conf &lt;&lt;EOF
    standby_mode = &apos;on&apos;
    primary_conninfo = &apos;host=localhost port=5432 user=postgres password=password application_name=$(hostname)&apos;
    ###restore_command = &apos;/bin/cp -n \$PGDATA/../arclog/%f %p&apos;
    ###restore_command = &apos;/usr/bin/test -f $PGDATA/../arclog/\$(date +%Y%m%d)/%f.zip &amp;&amp; unzip -o \$PGDATA/../arclog/\$(date +%Y%m%d)/%f.zip&apos;
    recovery_target_timeline = &apos;latest&apos;
    EOF

    cat &gt; %{buildroot}/opt/pg95/etc/sysctl.conf &lt;&lt;EOF
    # Database kernel optimization
    vm.swappiness = 0
    vm.overcommit_memory = 2
    vm.overcommit_ratio = 90
    vm.zone_reclaim_mode = 0
    net.core.somaxconn = 62144
    vm.dirty_background_ratio = 40
    vm.dirty_ratio = 80
    # vm.nr_hugepages = 49631
    EOF

    cp %{buildroot}/../../SPECS/pgsql/pg95.conf %{buildroot}/opt/pg95/etc/postgresql.conf

    %pre
    getent group postgres &gt; /dev/null || groupadd -r postgres
    getent passwd postgres &gt; /dev/null || \
        useradd -r -g postgres -s /sbin/nologin \
        -c &quot;PostgreSQL DataBase&quot; postgres

    %post
    /sbin/ldconfig

    %postun -p /sbin/ldconfig

    %clean
    rm -fr %{buildroot}

    %files
    %defattr(-, root, root)
    /opt/pgsql
    /opt/pg95/*
    /etc/ld.so.conf.d/pgsql.conf
    /etc/profile.d/pgsql.sh
    %defattr(0700, postgres, postgres)
    /home/postgres/
</code></pre><h2 id="5-打包"><a href="#5-打包" class="headerlink" title="5. 打包"></a>5. 打包</h2><p>先配置变量方便以后的使用:</p>
<pre><code>$ sudo vim /etc/profile.d/rpmbuild.sh
#!/bin/env bash
alias rpmbuild=&apos;rpmbuild --define &quot;_topdir /data/rpmbuild&quot; -bb&apos;
$ sudo source /etc/profile.d/rpmbuild.sh
</code></pre><p>打包就只需要执行:</p>
<pre><code>$ rpmbuild path/xxx.spec
</code></pre><h2 id="6-rpmmacros"><a href="#6-rpmmacros" class="headerlink" title="6. ~/.rpmmacros"></a>6. ~/.rpmmacros</h2><pre><code>%_topdir      %(echo $HOME)/rpmbuild
%_tmppath  %{_topdir}/tmp
%buildroot  %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)
%_rpmfilename  %%{NAME}-%%{VERSION}-%%{RELEASE}.%%{ARCH}.rpm
</code></pre><p>参考:<a href="fedoraproject.org/wiki/How_to_create_an_RPM_package">How to create an RPM package</a>和<a href="rpmbuildtut.wordpress.com/">Rpmbuild Tutorial</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[设置虚ip]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-%E8%AE%BE%E7%BD%AE%E8%99%9Aip/</url>
      <content type="html"><![CDATA[<h2 id="查看网卡设备"><a href="#查看网卡设备" class="headerlink" title="查看网卡设备"></a>查看网卡设备</h2><pre><code>$ ip a show up
</code></pre><a id="more"></a>
<h2 id="删除虚ip"><a href="#删除虚ip" class="headerlink" title="删除虚ip"></a>删除虚ip</h2><pre><code>$ sudo ip a d local v-ip/16 brd + dev eth0
</code></pre><h2 id="设置虚ip"><a href="#设置虚ip" class="headerlink" title="设置虚ip"></a>设置虚ip</h2><pre><code>$ sudo ip a a local v-ip/16 brd + dev eth0
$ sudo arping -q -c 3 -A -I eth0 v-ip
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[格式化大盘]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%A4%A7%E7%9B%98/</url>
      <content type="html"><![CDATA[<h2 id="设置分区为-GPT"><a href="#设置分区为-GPT" class="headerlink" title="设置分区为 GPT"></a>设置分区为 <a href="https://en.wikipedia.org/wiki/GUID_Partition_Table" target="_blank" rel="external">GPT</a></h2><p>因为<a href="https://en.wikipedia.org/wiki/Master_boot_record" target="_blank" rel="external">MBR</a>限制单盘的最大地址存储空间为2TB</p>
<a id="more"></a>
<pre><code># parted /dev/sdb
(parted) mklabel gpt
(parted) unit s
(parted) print free
(parted) mkpart primary 0% 10TB
(parted) print free
(parted) quit
</code></pre><h2 id="格式化为ext4"><a href="#格式化为ext4" class="headerlink" title="格式化为ext4"></a>格式化为ext4</h2><pre><code># mkfs.ext4 /dev/sdb1
</code></pre><h2 id="挂载"><a href="#挂载" class="headerlink" title="挂载"></a>挂载</h2><pre><code># mount -t ext4 /dev/sdb1 /mnt
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在Slackware64中安装rtx]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-%E5%9C%A8Slackware64%E4%B8%AD%E5%AE%89%E8%A3%85rtx/</url>
      <content type="html"><![CDATA[<h2 id="1-安装Multilib-of-Alien"><a href="#1-安装Multilib-of-Alien" class="headerlink" title="1. 安装Multilib of Alien"></a>1. 安装Multilib of Alien</h2><p>在slackware64下执行如下命令,下载和安装32位库,安装完后需要重启才能生效,因为修改了lib库:</p>
<pre><code>$ mkdir multlib
$ lftp -c &apos;open http://slackware.com/~alien/multilib/; mirror 14.1&apos;
$ cd 14.1
$ sudo upgradepkg --reinstall --install-new *.t?z
$ sudo upgradepkg --install-new slackware64-compat32/*-compat32/*.t?z
</code></pre><a id="more"></a>
<h2 id="2-安装Wine"><a href="#2-安装Wine" class="headerlink" title="2. 安装Wine"></a>2. 安装Wine</h2><pre><code>$ wget http://sourceforge.net/projects/wine/files/Slackware%20Packages/1.7.8/x86_64/wine-1.7.8-x86_64-1sg.txz
$ sudo installpkg wine-1.7.8-x86_64-1sg.txz
</code></pre><h2 id="3-安装Winetricks-and-Cabextract"><a href="#3-安装Winetricks-and-Cabextract" class="headerlink" title="3. 安装Winetricks and Cabextract"></a>3. 安装Winetricks and Cabextract</h2><p><strong>winetricks:</strong></p>
<pre><code>$ wget http://winetricks.org/winetricks
$ chmod +x winetricks
$ cp winetricks /usr/local/bin
</code></pre><p><strong>cabextract:</strong></p>
<pre><code>$ wget http://slackbuilds.org/slackbuilds/14.1/system/cabextract.tar.gz
$ tar zxvf cabextract.tar.gz
$ cd cabextract/
$ wget http://www.cabextract.org.uk/cabextract-1.4.tar.gz
$ ./cabextract.SlackBuild
$ sudo installpkg /tmp/cabextract-1.4-x86_64-1_SBo.tgz
</code></pre><h2 id="4-配置Wine"><a href="#4-配置Wine" class="headerlink" title="4. 配置Wine"></a>4. 配置Wine</h2><pre><code>$ winecfg
$ winetricks msxml3 gdiplus riched20 riched30 vcrun6 vcrun2005sp1
</code></pre><h2 id="5-安装RTX"><a href="#5-安装RTX" class="headerlink" title="5. 安装RTX"></a>5. 安装RTX</h2><pre><code>$ wget http://dldir1.qq.com/foxmail/rtx/rtxclient2013formal.exe
$ wine rtxclient2012formal.exe
</code></pre><h2 id="6-乱码解决"><a href="#6-乱码解决" class="headerlink" title="6. 乱码解决"></a>6. 乱码解决</h2><p> 把windows系统C:\Windows\Fonts\下的字体全部复制到linux的/usr/share/fonts/windows(这个目录需要自己创建)目录下,rtx的菜单栏应该可以正常显示中文了；如果字体乱码,选择菜单栏的编辑字体,选择宋体,然后重启rtx就可以了。</p>
<p>如果还是乱码的话,把下面的内容保存为rtx.reg, 执行regedit rtx.reg,然后把字体simsun.ttf复制到~/.wine/drive_c/windows/Fonts目录下。</p>
<pre><code>$ vim rtx.reg
    REGEDIT4
    [HKEY_LOCAL_MACHINE\Software\Microsoft\Windows NT\CurrentVersion\FontSubstitutes]
    &quot;Arial&quot;=&quot;simsun&quot;
    &quot;Arial CE,238&quot;=&quot;simsun&quot;
    &quot;Arial CYR,204&quot;=&quot;simsun&quot;
    &quot;Arial Greek,161&quot;=&quot;simsun&quot;
    &quot;Arial TUR,162&quot;=&quot;simsun&quot;
    &quot;Courier New&quot;=&quot;simsun&quot;
    &quot;Courier New CE,238&quot;=&quot;simsun&quot;
    &quot;Courier New CYR,204&quot;=&quot;simsun&quot;
    &quot;Courier New Greek,161&quot;=&quot;simsun&quot;
    &quot;Courier New TUR,162&quot;=&quot;simsun&quot;
    &quot;FixedSys&quot;=&quot;simsun&quot;
    &quot;Helv&quot;=&quot;simsun&quot;
    &quot;Helvetica&quot;=&quot;simsun&quot;
    &quot;MS Sans Serif&quot;=&quot;simsun&quot;
    &quot;MS Shell Dlg&quot;=&quot;simsun&quot;
    &quot;MS Shell Dlg 2&quot;=&quot;simsun&quot;
    &quot;System&quot;=&quot;simsun&quot;
    &quot;Tahoma&quot;=&quot;simsun&quot;
    &quot;Times&quot;=&quot;simsun&quot;
    &quot;Times New Roman CE,238&quot;=&quot;simsun&quot;
    &quot;Times New Roman CYR,204&quot;=&quot;simsun&quot;
    &quot;Times New Roman Greek,161&quot;=&quot;simsun&quot;
    &quot;Times New Roman TUR,162&quot;=&quot;simsun&quot;
    &quot;Tms Rmn&quot;=&quot;simsun&quot;

$ regedit rtx.reg
$ cp simsun.ttf ~/.wine/drive_c/windows/Fonts
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在CenOS6.4下安装Docker]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-%E5%9C%A8CenOS6.4%E4%B8%8B%E5%AE%89%E8%A3%85Docker/</url>
      <content type="html"><![CDATA[<h2 id="1-升级内核版本"><a href="#1-升级内核版本" class="headerlink" title="1.升级内核版本"></a>1.升级内核版本</h2><p>因为CentOS6.4自带内核版本是2.6.32-358.23.2.el6.x86_64,而Docker要求内核版本大于3.0,推荐3.8以上的内核:</p>
<pre><code>$ sudo rpm -ivh http://elrepo.org/linux/kernel/el6/x86_64/RPMS/kernel-lt-3.10.101-1.el6.elrepo.x86_64.rpm
</code></pre><a id="more"></a>
<h2 id="2-安装Docker"><a href="#2-安装Docker" class="headerlink" title="2.安装Docker"></a>2.安装Docker</h2><pre><code>$ sudo yum install -y libcgroup.x86_64 glibc.i686 sqlite.i68
$ sudo rpm -ivh https://get.docker.com/rpm/1.7.0/centos-6/RPMS/x86_64/docker-engine-1.7.0-1.el6.x86_64.rpm
</code></pre><h2 id="3-配置"><a href="#3-配置" class="headerlink" title="3.配置"></a>3.配置</h2><pre><code>$ sudo vim /etc/sysconfig/docker
    other_args=&quot;$other_args --registry-mirror=&quot;http://556b149e.m.daocloud.io&quot;
    other_args=&quot;$other_args --log-driver=none&quot;
</code></pre><h2 id="4-启动"><a href="#4-启动" class="headerlink" title="4.启动"></a>4.启动</h2><pre><code>$ sudo service docker start
</code></pre><h3 id="启动时可能出现的错误"><a href="#启动时可能出现的错误" class="headerlink" title="启动时可能出现的错误"></a>启动时可能出现的错误</h3><pre><code>docker: relocation error: docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference
</code></pre><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><pre><code>$ sudo cd /etc/yum.repos.d
$ sudo wget public-yum.oracle.com/public-yum-ol6.repo
$ sudo wget public-yum.oracle.com/RPM-GPG-KEY-oracle-ol6 -O /etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
$ sudo yum update -y device-mapper-libs
$ sudo yum update -y libcgroup
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[ssd优化]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-ssd%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<h2 id="1-Linux"><a href="#1-Linux" class="headerlink" title="1. Linux"></a>1. Linux</h2><p>参考: <a href="wiki.archlinux.org/index.php/Fstab#tmpfs">archlinux fstab</a>和<a href="wiki.archlinux.org/index.php/Solid_State_Drives#SSD_Memory_Cell_Clearing">Solid State Drives</a></p>
<a id="more"></a>
<h3 id="1-修改fstab开启Trim"><a href="#1-修改fstab开启Trim" class="headerlink" title="(1) 修改fstab开启Trim)"></a>(1) 修改fstab开启<a href="en.wikipedia.org/wiki/Trim_(computing">Trim</a>)</h3><pre><code>/dev/sda1       /           ext4        noatime,nodiratime,discard                          0   1
tmpfs           /tmp        tmpfs       nodev,nosuid,noexec,relatime,mode=1777,size=15%     0   0
tmpfs           /var/tmp    tmpfs       defaults,size=5%                                    0   0
</code></pre><p>如果正在使用的话需要先umount,然后执行mount -a,如果不行的话,重启一下</p>
<h3 id="2-打开写缓存"><a href="#2-打开写缓存" class="headerlink" title="(2) 打开写缓存"></a>(2) 打开写缓存</h3><pre><code>$ hdparm -W1 /dev/sda
</code></pre><h3 id="3-IO-schedule"><a href="#3-IO-schedule" class="headerlink" title="(3) IO schedule"></a>(3) IO schedule</h3><pre><code>$ sudo echo deadline &gt; /sys/block/sda/queue/scheduler
$ sudo echo 1 &gt; /sys/block/sda/queue/iosched/fifo_batch
</code></pre><h3 id="4-内核配置"><a href="#4-内核配置" class="headerlink" title="(4) 内核配置"></a>(4) 内核配置</h3><pre><code>$ sudo echo 0 &gt; /proc/sys/vm/swappiness
$ sudo echo 30 &gt; /proc/sys/vm/vfs_cache_pressure
</code></pre><h2 id="2-windows"><a href="#2-windows" class="headerlink" title="2. windows"></a>2. windows</h2><p>参考:SSD Performance Tweaks</p>
<h3 id="1-系统服务"><a href="#1-系统服务" class="headerlink" title="(1) 系统服务"></a>(1) 系统服务</h3><pre><code>win+r运行services.msc,禁用Superfetch和Windows Search
右键磁盘属性,禁用压缩驱动器以节约磁盘空间和允许索引服务编制该磁盘的索引以便快速搜索文件
关闭上次访问时间:使用管理员启动cmd,运行fsutil behavior set disablelastaccess 1
关闭NTFS日志:使用管理员启动cmd,运行fsutil usn deletejournal /n c:
关闭休眠:使用管理员启动cmd,运行powercfg /h off
开启trim:使用管理员启动cmd,运行fsutil behavior set DisableDeleteNotify 0
禁用Rac Task:右键我的电脑-&gt;管理-&gt;系统工具-&gt;任务调度-&gt;任务调度库-&gt;Windows-&gt;RAC
</code></pre><h3 id="2-注册表"><a href="#2-注册表" class="headerlink" title="(2) 注册表"></a>(2) 注册表</h3><p>关闭Prefetch:</p>
<pre><code>[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session Manager\Memory Management\PrefetchParameters]
&quot;EnableSuperfetch&quot;=dword:00000000
&quot;EnablePrefetcher&quot;=dword:00000000
&quot;EnableBootTrace&quot;=dword:00000000
</code></pre><p>关闭Logging WMI Activity:</p>
<pre><code>[HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\WBEM\CIMOM]
&quot;Logging&quot;=&quot;0&quot;
</code></pre><p>减少MFT碎片:</p>
<pre><code>[HKEY_LOCAL_MACHINE/SYSTEM/CurrentControlSet/Control/FileSystem]
&quot;NtfsMftZoneReservation&quot;=&quot;2&quot;
</code></pre><p>关闭CLEAR PAGEFILE AT SHUTDOWN:</p>
<pre><code>[Hkey_local_machine\SYSTEM\CurrentControlSet\Control\Session Manager\Memory Management]
&quot;ClearPageFilesAtShutdown&quot;=&quot;0&quot;
</code></pre><p>关闭系统缓存:</p>
<pre><code>[Hkey_local_machine\SYSTEM\CurrentControlSet\Control\Session Manager\Memory Management]
&quot;LargeSystemCache&quot;=&quot;1&quot;
</code></pre><p>减少NTFS内存使用:</p>
<pre><code>[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem]
&quot;NtfsMemoryUsage&quot;=&quot;2&quot;
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[crontab中PATH的坑]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-crontab%E4%B8%ADPATH%E7%9A%84%E5%9D%91/</url>
      <content type="html"><![CDATA[<h2 id="1-现象"><a href="#1-现象" class="headerlink" title="1. 现象"></a>1. 现象</h2><p>在命令行里面能正常运行脚本,在crontab里面报错:</p>
<p>lsof: No such file or directory</p>
<p>看报错信息, 应该是找不到lsof这个命令</p>
<a id="more"></a>
<h2 id="2-排查"><a href="#2-排查" class="headerlink" title="2. 排查"></a>2. 排查</h2><p>查看lsof命令的路径:</p>
<pre><code>$ type -a lsof
lsof is /usr/bin/lsof
</code></pre><p>查看系统的PATH路径:</p>
<pre><code>$ echo $PATH
/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/bin
</code></pre><p>里面是包含/usr/sbin的,这能解释为什么命令行运行正常了,那就是crontab的PATH的问题了</p>
<p>在crontab运行一个echo $PATH的脚本,结果如下</p>
<pre><code>/usr/bin:/bin
</code></pre><p>果真是crontab的原因</p>
<h2 id="3-原因"><a href="#3-原因" class="headerlink" title="3. 原因"></a>3. 原因</h2><pre><code>$ man 5 crontab
</code></pre><p>在man page里面找到</p>
<p>Several  environment variables are set up automatically by the cron(8) daemon.  SHELL is set to /bin/sh, and LOGNAME and HOME are set from the /etc/passwd line of the crontab’s owner. PATH is set to “/usr/bin:/bin”.  HOME, SHELL, and PATH may be overridden by settings in the crontab; LOGNAME is the user that the job is running from, and may not be changed.</p>
<h2 id="4-解决办法"><a href="#4-解决办法" class="headerlink" title="4. 解决办法"></a>4. 解决办法</h2><p>在脚本里面export PATH=/usr/sbin:$PATH, 或者把crontab改成</p>
<pre><code>* * * * * source /etc/profile; your command
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[部署Docker下打包环境]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-%E9%83%A8%E7%BD%B2Docker%E4%B8%8B%E6%89%93%E5%8C%85%E7%8E%AF%E5%A2%83/</url>
      <content type="html"><![CDATA[<h2 id="1-镜像"><a href="#1-镜像" class="headerlink" title="1. 镜像"></a>1. 镜像</h2><h3 id="1-Dockerfile"><a href="#1-Dockerfile" class="headerlink" title="(1) Dockerfile"></a>(1) Dockerfile</h3><p>由于Docker Client会默认发送Dockerfile同级目录下的所有文件到Dockerdaemon中,所以建议把Dockerfile放在一个空文件夹或者比较小的文件夹中。</p>
<pre><code>FROM centos:6
MAINTAINER panwenhang &quot;panwenhang92@gmail.com&quot;
</code></pre><a id="more"></a>
<pre><code>RUN yum -q -y update
RUN yum install -q -y rpm-build  git subversion tar gcc g++ make vim-common vim-enhanced wget
RUN yum install -q -y libxml2-devel libxslt-devel uuid-devel readline-devel
RUN yum install -q -y perl-ExtUtils-Embed libxml2 libxslt readline uuid
</code></pre><h3 id="2-创建镜像"><a href="#2-创建镜像" class="headerlink" title="(2) 创建镜像"></a>(2) 创建镜像</h3><pre><code>$ sudo docker build --rm -t rpmbuild:centos6 .
</code></pre><h3 id="3-确认"><a href="#3-确认" class="headerlink" title="(3) 确认"></a>(3) 确认</h3><pre><code>$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
rpmbuild            centos6             2f14e12b1008        14 minutes ago      296.5 MB
</code></pre><h2 id="2-容器"><a href="#2-容器" class="headerlink" title="2. 容器"></a>2. 容器</h2><h3 id="1-创建"><a href="#1-创建" class="headerlink" title="(1) 创建"></a>(1) 创建</h3><pre><code>$ sudo docker run -itd -h rpmbuild_centos6 --name=rpmbuild_centos6 -v /data/rpmbuild/:/data/rpmbuild/:rw rpmbuild:centos6 /bin/bash
</code></pre><h3 id="2-确认"><a href="#2-确认" class="headerlink" title="(2) 确认"></a>(2) 确认</h3><pre><code>$ sudo docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
b1041bc9086c        rpmbuild:centos6    &quot;/bin/bash&quot;         About an hour ago   Up 22 minutes                           rpmbuild_centos6
</code></pre><h2 id="3-运行"><a href="#3-运行" class="headerlink" title="3. 运行"></a>3. 运行</h2><pre><code>$ sudo docker start rpmbuild_centos6
$ sudo docker exec -it rpmbuild_centos6 /bin/bash
</code></pre><p>然后就可以正常打包了</p>
<h2 id="4-备份和恢复"><a href="#4-备份和恢复" class="headerlink" title="4. 备份和恢复"></a>4. 备份和恢复</h2><h3 id="1-备份"><a href="#1-备份" class="headerlink" title="(1) 备份"></a>(1) 备份</h3><pre><code>sudo -u root sh -c &apos;docker export rpmbuild_centos6 &gt; rpmbuild_centos6.ctr&apos;
</code></pre><h3 id="2-恢复"><a href="#2-恢复" class="headerlink" title="(2) 恢复"></a>(2) 恢复</h3><pre><code>cat rpmbuild_centos6.ctr | sudo docker import - rpmbuild:centos6
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux查看硬件信息]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-Linux%E6%9F%A5%E7%9C%8B%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF/</url>
      <content type="html"><![CDATA[<h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="1-lscpu"><a href="#1-lscpu" class="headerlink" title="1. lscpu"></a>1. lscpu</h3><p>Lists the basic information of cpu.</p>
<a id="more"></a>
<h3 id="2-cat-proc-cpuinfo"><a href="#2-cat-proc-cpuinfo" class="headerlink" title="2. cat /proc/cpuinfo"></a>2. cat /proc/cpuinfo</h3><p>Lists the information of each core of cpu.</p>
<h2 id="MEMORY"><a href="#MEMORY" class="headerlink" title="MEMORY"></a>MEMORY</h2><h3 id="1-free-m"><a href="#1-free-m" class="headerlink" title="1. free -m"></a>1. free -m</h3><p>Lists usage information of memory.</p>
<h3 id="2-cat-proc-meminfo"><a href="#2-cat-proc-meminfo" class="headerlink" title="2. cat /proc/meminfo"></a>2. cat /proc/meminfo</h3><p>Lists the detail usage information of memory.</p>
<h3 id="3-dmidecode-t-memory"><a href="#3-dmidecode-t-memory" class="headerlink" title="3. dmidecode -t memory"></a>3. dmidecode -t memory</h3><p>Lists the hardware information of memory.</p>
<h2 id="DISK"><a href="#DISK" class="headerlink" title="DISK"></a>DISK</h2><h3 id="1-lsblk"><a href="#1-lsblk" class="headerlink" title="1. lsblk"></a>1. lsblk</h3><p>Lists information about all or the specified block devices.</p>
<h3 id="2-fdisk-l"><a href="#2-fdisk-l" class="headerlink" title="2. fdisk -l"></a>2. fdisk -l</h3><p>Partition table manipulator for Linux.</p>
<h3 id="3-df-Th"><a href="#3-df-Th" class="headerlink" title="3. df -Th"></a>3. df -Th</h3><p>Report file system disk space usage.</p>
<h2 id="NETWORK-CARD"><a href="#NETWORK-CARD" class="headerlink" title="NETWORK CARD"></a>NETWORK CARD</h2><h3 id="1-ifconfig-a"><a href="#1-ifconfig-a" class="headerlink" title="1. ifconfig -a"></a>1. ifconfig -a</h3><p>Lists configuration of each network interface.</p>
<h3 id="2-ip-link-show"><a href="#2-ip-link-show" class="headerlink" title="2. ip link show"></a>2. ip link show</h3><p>Show routing, devices, policy routing and tunnels.</p>
<h2 id="BIOS"><a href="#BIOS" class="headerlink" title="BIOS"></a>BIOS</h2><h3 id="dmidecode-t-bios"><a href="#dmidecode-t-bios" class="headerlink" title="dmidecode -t bios"></a>dmidecode -t bios</h3><h2 id="PCI"><a href="#PCI" class="headerlink" title="PCI"></a>PCI</h2><h3 id="lspci"><a href="#lspci" class="headerlink" title="lspci"></a>lspci</h3>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[HP机器RAID和LVM创建]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-HP%E6%9C%BA%E5%99%A8RAID%E5%92%8C2016-06-30-LVM%E5%88%9B%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1.环境"></a>1.环境</h2><pre><code>[root@localhost ~]# uname -a
Linux localhost.localdomain 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/Linux
[root@localhost ~]# cat /etc/issue
CentOS release 6.7 (Final)
Kernel \r on an \m
[root@localhost ~]# dmidecode -t bios
# dmidecode 2.12
SMBIOS 2.7 present.
</code></pre><a id="more"></a>
<pre><code>Handle 0x0000, DMI type 0, 24 bytes
BIOS Information
        Vendor: HP
        Version: P71
        Release Date: 03/01/2013
        Address: 0xF0000
        Runtime Size: 64 kB
        ROM Size: 8192 kB
</code></pre><h2 id="2-安装HP磁盘管理工具"><a href="#2-安装HP磁盘管理工具" class="headerlink" title="2.安装HP磁盘管理工具"></a>2.安装HP磁盘管理工具</h2><pre><code>[root@localhost ~]# rpm -ivh http://mirror.nforce.com/pub/software/raidtools/hpacucli-tool/hpacucli-9.40-12.0.x86_64.rpm
</code></pre><h2 id="3-配置RAID"><a href="#3-配置RAID" class="headerlink" title="3.配置RAID"></a>3.配置RAID</h2><h3 id="1-产看当前RAID状态"><a href="#1-产看当前RAID状态" class="headerlink" title="(1) 产看当前RAID状态"></a>(1) 产看当前RAID状态</h3><pre><code>[root@localhost ~]# hpacucli ctrl all show status

Smart Array P420i in Slot 0 (Embedded)
   Controller Status: OK
   Cache Status: OK
   Battery/Capacitor Status: OK

[root@localhost ~]# hpacucli ctrl all show config

Smart Array P420i in Slot 0 (Embedded)    (sn: 500143802626A660)

   array A (SAS, Unused Space: 0  MB)


      logicaldrive 1 (558.9 GB, RAID 1, OK)

      physicaldrive 1I:1:1 (port 1I:box 1:bay 1, SAS, 600 GB, OK)
      physicaldrive 1I:1:2 (port 1I:box 1:bay 2, SAS, 600 GB, OK)

   unassigned

      physicaldrive 1I:1:3 (port 1I:box 1:bay 3, SAS, 600 GB, OK)
      physicaldrive 1I:1:4 (port 1I:box 1:bay 4, SAS, 600 GB, OK)
      physicaldrive 2I:1:5 (port 2I:box 1:bay 5, SAS, 600 GB, OK)
      physicaldrive 2I:1:6 (port 2I:box 1:bay 6, SAS, 600 GB, OK)

   SEP (Vendor ID PMCSIERA, Model SRCv8x6G) 380 (WWID: 500143802626A66F)
</code></pre><p>发现有两块盘做了RAID 1, 剩下4块盘没有做RAID。。</p>
<h2 id="2-创建RAID"><a href="#2-创建RAID" class="headerlink" title="(2) 创建RAID"></a>(2) 创建RAID</h2><pre><code>[root@localhost ~]# hpacucli ctrl slot=0 create type=ld drives=1I:1:3,1I:1:4,2I:1:5,2I:1:6 raid=1+0
[root@localhost ~]# hpacucli ctrl all show config

Smart Array P420i in Slot 0 (Embedded)    (sn: 500143802626A660)

   array A (SAS, Unused Space: 0  MB)


      logicaldrive 1 (558.9 GB, RAID 1, OK)

      physicaldrive 1I:1:1 (port 1I:box 1:bay 1, SAS, 600 GB, OK)
      physicaldrive 1I:1:2 (port 1I:box 1:bay 2, SAS, 600 GB, OK)

   array B (SAS, Unused Space: 0  MB)


      logicaldrive 2 (1.1 TB, RAID 1+0, OK)

      physicaldrive 1I:1:3 (port 1I:box 1:bay 3, SAS, 600 GB, OK)
      physicaldrive 1I:1:4 (port 1I:box 1:bay 4, SAS, 600 GB, OK)
      physicaldrive 2I:1:5 (port 2I:box 1:bay 5, SAS, 600 GB, OK)
      physicaldrive 2I:1:6 (port 2I:box 1:bay 6, SAS, 600 GB, OK)

   SEP (Vendor ID PMCSIERA, Model SRCv8x6G) 380 (WWID: 500143802626A66F)
</code></pre><p>为剩下的4块盘创建了一个RAID 1+0</p>
<h2 id="4-配置LVM"><a href="#4-配置LVM" class="headerlink" title="4.配置LVM"></a>4.配置LVM</h2><h3 id="1-创建分区"><a href="#1-创建分区" class="headerlink" title="(1) 创建分区"></a>(1) 创建分区</h3><pre><code>[root@localhost ~]# cfdisk /dev/sdb
</code></pre><h3 id="2-创建物理卷"><a href="#2-创建物理卷" class="headerlink" title="(2) 创建物理卷"></a>(2) 创建物理卷</h3><pre><code>[root@localhost ~]# pvs
  PV         VG       Fmt  Attr PSize   PFree
  /dev/sda2  VolGroup lvm2 a--  558.39g    0
[root@localhost ~]# pvcreate /dev/sdb1
  Physical volume &quot;/dev/sdb1&quot; successfully created
[root@localhost ~]# pvs
  PV         VG       Fmt  Attr PSize   PFree
  /dev/sda2  VolGroup lvm2 a--  558.39g    0
  /dev/sdb1           lvm2 ---    1.09t 1.09t
</code></pre><h3 id="3-将创建的物理卷添加到卷组"><a href="#3-将创建的物理卷添加到卷组" class="headerlink" title="(3) 将创建的物理卷添加到卷组"></a>(3) 将创建的物理卷添加到卷组</h3><pre><code>[root@localhost ~]# vgs
  VG       #PV #LV #SN Attr   VSize   VFree
  VolGroup   1   3   0 wz--n- 558.39g    0
[root@localhost ~]# vgextend VolGroup /dev/sdb1
  Volume group &quot;VolGroup&quot; successfully extended
[root@localhost ~]# vgs
  VG       #PV #LV #SN Attr   VSize VFree
  VolGroup   2   3   0 wz--n- 1.64t 1.09t
</code></pre><h3 id="4-创建逻辑卷"><a href="#4-创建逻辑卷" class="headerlink" title="(4) 创建逻辑卷"></a>(4) 创建逻辑卷</h3><pre><code>[root@localhost ~]# lvs
  LV      VG       Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv_home VolGroup -wi-ao---- 380.25g
  lv_root VolGroup -wi-ao----  50.00g
  lv_swap VolGroup -wi-ao---- 128.14g
[root@localhost ~]# lvcreate -L 1T -n lv_data VolGroup
  Logical volume &quot;lv_data&quot; created.
[root@localhost ~]# lvs
  LV      VG       Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv_data VolGroup -wi-a-----   1.00t
  lv_home VolGroup -wi-ao---- 380.25g
  lv_root VolGroup -wi-ao----  50.00g
  lv_swap VolGroup -wi-ao---- 128.14g
</code></pre><h2 id="5-格式化使用"><a href="#5-格式化使用" class="headerlink" title="5.格式化使用"></a>5.格式化使用</h2><h3 id="1-格式化"><a href="#1-格式化" class="headerlink" title="(1) 格式化"></a>(1) 格式化</h3><pre><code>[root@localhost ~]#  mkfs.ext4 /dev/VolGroup/lv_data
mke2fs 1.41.12 (17-May-2010)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=64 blocks, Stripe width=128 blocks
67108864 inodes, 268435456 blocks
13421772 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=4294967296
8192 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968,
        102400000, 214990848

Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 28 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.
[root@localhost ~]#  e2fsck -f /dev/VolGroup/lv_data
e2fsck 1.41.12 (17-May-2010)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/VolGroup/lv_data: 11/67108864 files (0.0% non-contiguous), 4262937/268435456 blocks
</code></pre><h3 id="2-挂载"><a href="#2-挂载" class="headerlink" title="(2) 挂载"></a>(2) 挂载</h3><pre><code>[root@localhost ~]# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/VolGroup-lv_root
                       50G  5.1G   42G  11% /
tmpfs                  63G     0   63G   0% /dev/shm
/dev/sda1             485M   67M  393M  15% /boot
/dev/mapper/VolGroup-lv_home
                      375G  192G  164G  55% /home
[root@localhost ~]# mkdir -p /data
[root@localhost ~]# mount -t ext4 /dev/VolGroup/lv_data /data
[root@localhost ~]# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/VolGroup-lv_root
                       50G  5.1G   42G  11% /
tmpfs                  63G     0   63G   0% /dev/shm
/dev/sda1             485M   67M  393M  15% /boot
/dev/mapper/VolGroup-lv_home
                      375G  192G  164G  55% /home
/dev/mapper/VolGroup-lv_data
                     1008G  200M  957G   1% /data
</code></pre><h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6.参考"></a>6.参考</h2><p><a href="http://www.datadisk.co.uk/html_docs/redhat/hpacucli.htm" target="_blank" rel="external">http://www.datadisk.co.uk/html_docs/redhat/hpacucli.htm</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux代理设置]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-Linux%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>由于公司必须通过代理才能上外网,所以搞了下代理的设置</p>
<h2 id="1-浏览器设置代理"><a href="#1-浏览器设置代理" class="headerlink" title="1. 浏览器设置代理"></a>1. 浏览器设置代理</h2><a id="more"></a>
<p>将如下内容添加到你的.bashrc或者.bash_profile里面</p>
<pre><code>export all_proxy=&apos;sockets://ip:port&apos;
export ftp_proxy=&apos;http://ip:port&apos;
export http_proxy=&apos;http://ip:port&apos;
export https_proxy=&apos;http://ip:port&apos;
export no_proxy=&quot;localhost,127.0.0.0/8,::1&quot;
</code></pre><p>然后执行:</p>
<pre><code>$ source ~/.bashrc
</code></pre><p>这样浏览器就可以使用代理了。</p>
<h2 id="2-apt-get设置代理"><a href="#2-apt-get设置代理" class="headerlink" title="2. apt-get设置代理"></a>2. apt-get设置代理</h2><p>在Ubuntu 10.10及以后版本中,apt-get不再读取$http_proxy变量,解决方法是修改apt-get的配置文件:</p>
<pre><code>$ sudo vim /etc/apt/apt.conf
Acquire::http::proxy &quot;http://ip:port&quot;;
Acquire::ftp::proxy &quot;ftp://ip:port&quot;;
Acquire::https::proxy &quot;https://ip:port&quot;;
</code></pre><p>apt-get还支持-c模式指定配置文件和-o指定配置项:</p>
<p>如将上面的配置加入到~/apt_proxy.conf中,可以使用,这样不会修改默认的配置,使用代理是执行</p>
<pre><code>$ sudo apt-get -c ~/apt_proxy.conf update
</code></pre><p>或者使用配置项:</p>
<pre><code>$ sudo apt-get -o Acquire::http::proxy=&quot;http://ip:port&quot; update
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux中时间同步]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-Linux%E4%B8%AD%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/</url>
      <content type="html"><![CDATA[<h2 id="1-安装ntpdate"><a href="#1-安装ntpdate" class="headerlink" title="1. 安装ntpdate"></a>1. 安装ntpdate</h2><pre><code>$ sudo yum install -y ntpdate
</code></pre><a id="more"></a>
<h2 id="2-同步时间"><a href="#2-同步时间" class="headerlink" title="2. 同步时间"></a>2. 同步时间</h2><pre><code>$ sudo /usr/sbin/ntpdate $ntp_server
</code></pre><h2 id="3-刷新到硬件时钟"><a href="#3-刷新到硬件时钟" class="headerlink" title="3. 刷新到硬件时钟"></a>3. 刷新到硬件时钟</h2><pre><code>$ sudo /sbin/hwclock --systohc
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux之间ssh免密码登录]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-Linux%E4%B9%8B%E9%97%B4ssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/</url>
      <content type="html"><![CDATA[<h2 id="1-生成公私钥对"><a href="#1-生成公私钥对" class="headerlink" title="1.生成公私钥对"></a>1.生成公私钥对</h2><pre><code>$ ssh-keygen -t rsa -P &apos;&apos;
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
</code></pre><a id="more"></a>
<p>   The key fingerprint is:<br>    88:64:ed:a1:a7:a2:c0:b4:ee:77:38:2f:c7:39:c6:c1 root@QA-5-45<br>    The key’s randomart image is:<br>    +–[ RSA 2048 ]—-+<br>    |                 |<br>    |     .           |<br>    |    o o          |<br>    |   o + o         |<br>    | .  + + S        |<br>    |o .  E           |<br>    |.o. = o          |<br>    |o. * O           |<br>    |oo. B..          |<br>    +—————–+<br>     -P phrase   Provide old passphrase. 这个是指使用私钥的密码<br>     -t type     Specify type of key to create. 这个是创建公私钥的类型</p>
<pre><code>$ ll .ssh/
total 8
-rw-------. 1 root root 1675 Sep  6 04:12 id_rsa
-rw-r--r--. 1 root root  394 Sep  6 04:12 id_rsa.pub
</code></pre><h2 id="2-复制公钥到免密码登录的机器"><a href="#2-复制公钥到免密码登录的机器" class="headerlink" title="2.复制公钥到免密码登录的机器"></a>2.复制公钥到免密码登录的机器</h2><pre><code>$ ssh-copy-id root@172.17.5.46
The authenticity of host &apos;172.17.5.46 (172.17.5.46)&apos; can&apos;t be established.
RSA key fingerprint is 8e:ac:89:5d:10:be:e7:d8:a2:34:0b:bf:70:cd:ce:33.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &apos;172.17.5.46&apos; (RSA) to the list of known hosts.
root@172.17.5.46&apos;s password:
Now try logging into the machine, with &quot;ssh &apos;root@172.17.5.46&apos;&quot;, and check in:

  .ssh/authorized_keys

to make sure we haven&apos;t added extra keys that you weren&apos;t expecting.
</code></pre><p>免登录机器上:</p>
<pre><code>$ ll .ssh/
total 4
-rw-------. 1 root root 394 Sep  6 04:16 authorized_keys
</code></pre><h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3.测试"></a>3.测试</h2><pre><code># ssh 172.17.5.46
Last login: Sun Sep  6 04:06:43 2015 from 172.17.5.45
[root@node2 ~]#
</code></pre><p>成功免密码登录。</p>
<h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h2><pre><code>免密码登录不用再每次都输入密码,方便了许多；

rsync也可以使用ssh认证在linux服务器之间免密码传输文件；

最好别配置root用户的免密码登录
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[DELL磁盘阵列控制卡（RAID卡）MegaCli常用管理命令汇总]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-DELL%E7%A3%81%E7%9B%98%E9%98%B5%E5%88%97%E6%8E%A7%E5%88%B6%E5%8D%A1%EF%BC%88RAID%E5%8D%A1%EF%BC%89MegaCli%E5%B8%B8%E7%94%A8%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB/</url>
      <content type="html"><![CDATA[<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>  新版本的 MegaCli-1.01.24-0.i386.rpm （下载地址:<a href="http://www.lsi.com/downloads/Public/MegaRAID%20Common%20Files/8.02.16_MegaCLI.zip" target="_blank" rel="external">http://www.lsi.com/downloads/Public/MegaRAID%20Common%20Files/8.02.16_MegaCLI.zip</a> ）该包会把程序安装在/opt下,当然也可以自定义安装目录,例如:rpm –relocate /opt/=/usr/sbin/ -i MegaCli-1.01.24-0.i386.rpm （即把安装目录 /opt 替换成 /usr/sbin）。</p>
<p>(linux文件夹下有个MegaCli文件里面有I386 RPM）</p>
<a id="more"></a>
<p>相关命令及其解析:</p>
<pre><code>查看机器型号    # dmidecode | grep &quot;Product&quot;
查看厂商    # dmidecode| grep  &quot;Manufacturer&quot;
查看序列号    # dmidecode | grep  &quot;Serial Number&quot;
查看CPU信息    # dmidecode | grep  &quot;CPU&quot;
查看CPU个数    # dmidecode | grep  &quot;Socket Designation: CPU&quot; |wc –l
查看出厂日期    # dmidecode | grep &quot;Date&quot;
查看充电状态    # MegaCli -AdpBbuCmd -GetBbuStatus -aALL |grep &quot;Charger Status&quot;
显示BBU状态信息    # MegaCli -AdpBbuCmd -GetBbuStatus –aALL
显示BBU容量信息    # MegaCli -AdpBbuCmd -GetBbuCapacityInfo –aALL
显示BBU设计参数    # MegaCli -AdpBbuCmd -GetBbuDesignInfo –aALL
显示当前BBU属性    # MegaCli -AdpBbuCmd -GetBbuProperties –aALL
查看充电进度百分比    # MegaCli -AdpBbuCmd -GetBbuStatus -aALL |grep &quot;Relative State of Charge&quot;
查询Raid阵列数    # MegaCli -cfgdsply -aALL |grep &quot;Number of DISK GROUPS:&quot;
显示Raid卡型号,Raid设置,Disk相关信息      # MegaCli -cfgdsply –aALL
显示所有物理信息    # MegaCli -PDList -aALL
显示所有逻辑磁盘组信息    # MegaCli -LDInfo -LALL –aAll
查看物理磁盘重建进度(重要)    # MegaCli -PDRbld -ShowProg -PhysDrv [1:5] -a0
查看适配器个数    #MegaCli –adpCount
查看适配器时间    #MegaCli -AdpGetTime –aALL
显示所有适配器信息    #MegaCli -AdpAllInfo –aAll
查看Cache 策略设置    # MegaCli -cfgdsply -aALL |grep Polic
</code></pre><h2 id="1-查看所有物理磁盘信息"><a href="#1-查看所有物理磁盘信息" class="headerlink" title="1.查看所有物理磁盘信息"></a>1.查看所有物理磁盘信息</h2><pre><code># MegaCli -PDList -aALL
Adapter #0
Enclosure Number: 1
Slot Number: 5
Device Id: 5
Sequence Number: 2
Media Error Count: 0
Other Error Count: 0
Predictive Failure Count: 0
Last Predictive Failure Event Seq Number: 0
Raw Size: 140014MB [0x11177328 Sectors]
Non Coerced Size: 139502MB [0x11077328 Sectors]
Coerced Size: 139392MB [0x11040000 Sectors]
Firmware state:
Hotspare SAS Address(0): 0x5000c50008e5cca9
SAS Address(1): 0x0
Inquiry Data: SEAGATE ST3146855SS     S5273LN4Y1X0 .....
</code></pre><h2 id="2-查看磁盘缓存策略"><a href="#2-查看磁盘缓存策略" class="headerlink" title="2.查看磁盘缓存策略"></a>2.查看磁盘缓存策略</h2><pre><code>MegaCli -LDGetProp -Cache -L0 -a0
Adapter 0-VD 0: Cache Policy:WriteBack, ReadAheadNone, Direct
or MegaCli -LDGetProp -Cache -L1 -a0
Adapter 0-VD 1: Cache Policy:WriteBack, ReadAheadNone, Direct
or MegaCli -LDGetProp -Cache -LALL -a0
Adapter 0-VD 0: Cache Policy:WriteBack, ReadAheadNone, Direct Adapter 0-VD 1: Cache Policy:WriteBack, ReadAheadNone, Direct
or MegaCli -LDGetProp -Cache -LALL -aALL
Adapter 0-VD 0: Cache Policy:WriteBack, ReadAheadNone, Direct Adapter 0-VD 1: Cache Policy:WriteBack, ReadAheadNone, Direct
or MegaCli -LDGetProp -DskCache -LALL -aALL
Adapter 0-VD 0: Disk Write Cache : Disk&apos;s Default Adapter 0-VD 1: Disk Write Cache : Disk&apos;s Default
</code></pre><h2 id="3-设置磁盘缓存策略"><a href="#3-设置磁盘缓存策略" class="headerlink" title="3.设置磁盘缓存策略"></a>3.设置磁盘缓存策略</h2><p>缓存策略解释:</p>
<pre><code>WT    (Write through
WB    (Write back)
NORA  (No read ahead)
RA    (Read ahead)
ADRA  (Adaptive read ahead)
Cached
Direct
</code></pre><p>例子:</p>
<pre><code>MegaCli -LDSetProp WT|WB|NORA|RA|ADRA -L0 -a0
or
MegaCli -LDSetProp -Cached|-Direct -L0 -a0
or
enable / disable disk cache
MegaCli -LDSetProp -EnDskCache|-DisDskCache -L0 -a0
</code></pre><h2 id="4-创建-删除-阵列"><a href="#4-创建-删除-阵列" class="headerlink" title="4.创建/删除 阵列"></a>4.创建/删除 阵列</h2><h3 id="4-1-创建一个-raid5-阵列-由物理盘-2-3-4-构成-该阵列的热备盘是物理盘-5"><a href="#4-1-创建一个-raid5-阵列-由物理盘-2-3-4-构成-该阵列的热备盘是物理盘-5" class="headerlink" title="4.1 创建一个 raid5 阵列,由物理盘 2,3,4 构成,该阵列的热备盘是物理盘 5"></a>4.1 创建一个 raid5 阵列,由物理盘 2,3,4 构成,该阵列的热备盘是物理盘 5</h3><pre><code>MegaCli -CfgLdAdd -r5 [1:2,1:3,1:4] WB Direct -Hsp[1:5] –a0
</code></pre><h3 id="4-2-创建阵列-不指定热备"><a href="#4-2-创建阵列-不指定热备" class="headerlink" title="4.2 创建阵列,不指定热备"></a>4.2 创建阵列,不指定热备</h3><pre><code>MegaCli -CfgLdAdd -r5 [1:2,1:3,1:4] WB Direct –a0
</code></pre><h3 id="4-3-删除阵列"><a href="#4-3-删除阵列" class="headerlink" title="4.3 删除阵列"></a>4.3 删除阵列</h3><pre><code>MegaCli -CfgLdDel -L1 –a0
</code></pre><h3 id="4-4-在线添加磁盘"><a href="#4-4-在线添加磁盘" class="headerlink" title="4.4 在线添加磁盘"></a>4.4 在线添加磁盘</h3><pre><code>MegaCli -LDRecon -Start -r5 -Add -PhysDrv[1:4] -L1 -a0
</code></pre><p>意思是,重建逻辑磁盘组1,raid级别是5,添加物理磁盘号:1:4。重建完后,新添加的物理磁盘会自动处于重建(同步)状态,这个 时候 fdisk -l是看不到阵列的空间变大的,只有在系统重启后才能看见。</p>
<h2 id="5-查看阵列初始化信息"><a href="#5-查看阵列初始化信息" class="headerlink" title="5.查看阵列初始化信息"></a>5.查看阵列初始化信息</h2><h3 id="5-1-阵列创建完后-会有一个初始化同步块的过程-可以看看其进度。"><a href="#5-1-阵列创建完后-会有一个初始化同步块的过程-可以看看其进度。" class="headerlink" title="5.1 阵列创建完后,会有一个初始化同步块的过程,可以看看其进度。"></a>5.1 阵列创建完后,会有一个初始化同步块的过程,可以看看其进度。</h3><pre><code>MegaCli -LDInit -ShowProg -LALL -aALL
</code></pre><p>或者以动态可视化文字界面显示</p>
<pre><code>MegaCli -LDInit -ProgDsply -LALL –aALL
</code></pre><h3 id="5-2-查看阵列后台初始化进度"><a href="#5-2-查看阵列后台初始化进度" class="headerlink" title="5.2 查看阵列后台初始化进度"></a>5.2 查看阵列后台初始化进度</h3><pre><code>MegaCli -LDBI -ShowProg -LALL -aALL
</code></pre><p>或者以动态可视化文字界面显示</p>
<pre><code>MegaCli -LDBI -ProgDsply -LALL -aALL
</code></pre><h2 id="6-创建全局热备"><a href="#6-创建全局热备" class="headerlink" title="6.创建全局热备"></a>6.创建全局热备</h2><p>指定第 5 块盘作为全局热备</p>
<pre><code>MegaCli -PDHSP -Set [-EnclAffinity] [-nonRevertible] -PhysDrv[1:5] -a0
</code></pre><p>也可以指定为某个阵列的专用热备</p>
<pre><code>MegaCli -PDHSP -Set [-Dedicated [-Array1]] [-EnclAffinity] [-nonRevertible] -PhysDrv[1:5] -a0
</code></pre><h2 id="7-删除全局热备"><a href="#7-删除全局热备" class="headerlink" title="7.删除全局热备"></a>7.删除全局热备</h2><pre><code>MegaCli -PDHSP -Rmv -PhysDrv[1:5] -a0
</code></pre><h2 id="8-将某块物理盘下线-上线"><a href="#8-将某块物理盘下线-上线" class="headerlink" title="8.将某块物理盘下线/上线"></a>8.将某块物理盘下线/上线</h2><pre><code>MegaCli -PDOffline -PhysDrv [1:4] -a0
MegaCli -PDOnline -PhysDrv [1:4] -a0
</code></pre><h2 id="9-查看物理磁盘重建进度"><a href="#9-查看物理磁盘重建进度" class="headerlink" title="9.查看物理磁盘重建进度"></a>9.查看物理磁盘重建进度</h2><pre><code>MegaCli -PDRbld -ShowProg -PhysDrv [1:5] -a0
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Shell脚本-良好的习惯]]></title>
      <url>http://panwenhang.github.io/2016/06/30/Linux/2016-06-30-Shell%E8%84%9A%E6%9C%AC-%E8%89%AF%E5%A5%BD%E7%9A%84%E4%B9%A0%E6%83%AF/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>大多数编程语言都有一系列使用该语言编码需要遵循良好的编程习惯。然而,对于shell脚本我没有找到一个比较全面的,所以我决定编写一个我自己的基于我多年编写shell经验的编程习惯。</p>
<p>移植性的注意:自从主要编写shell脚本在安装了Bash 4.2的系统上运行,我从来不担心可移植性,你也不需要担心！下面的列表都是使用Bash 4.2（和其他现代化的shell）编写的。如果你要编写一个可移植的脚本,有些点可能不适用。无需多说,在你按照这个列表改变之后,你应该进行充足的测试。</p>
<p>下面是我关于shell脚本的良好的编程习惯（没有特殊的顺序）:</p>
<h2 id="1-使用函数"><a href="#1-使用函数" class="headerlink" title="1. 使用函数"></a>1. 使用函数</h2><p>除非你编写非常小的脚本,使用函数将你的代码模块化,并使它易读、可重复使用和好维护。我的所有脚本使用的模板如下。如你所见,所有代码写在函数里面。脚本以main的调用开头。</p>
<pre><code>#!/bin/bash
set -e

usage() {
}

my_function() {
}

main() {
}

main &quot;$@&quot;
</code></pre><h2 id="2-为函数编写注释"><a href="#2-为函数编写注释" class="headerlink" title="2. 为函数编写注释"></a>2. 为函数编写注释</h2><p>为你的函数添加充足的文档,指定它们是敢什么的,调用它们需要哪些参数。</p>
<p>下面是一个例子:</p>
<pre><code># Processes a file.
# $1 - the name of the input file
# $2 - the name of the output file
process_file(){
}
</code></pre><h2 id="3-使用shift读取函数参数"><a href="#3-使用shift读取函数参数" class="headerlink" title="3. 使用shift读取函数参数"></a>3. 使用shift读取函数参数</h2><p>不是使用$1,$2来获取函数参数,使用shift如下所示。这会更容易记录参数,如果你后来改变了想法。</p>
<pre><code># Processes a file.
# $1 - the name of the input file
# $2 - the name of the output file
process_file(){
    local -r input_file=&quot;$1&quot;;  shift
    local -r output_file=&quot;$1&quot;; shift
}
</code></pre><h2 id="4-声明你的变量"><a href="#4-声明你的变量" class="headerlink" title="4. 声明你的变量"></a>4. 声明你的变量</h2><p>如果你的变量是整型,像下面这样声明。还有,使你的变量只读,除非后面你想要在你的脚本里面修改它的值。对函数里面声明的变量使用local。这会有助于表达你的意图。如果考虑可移植性,使用typeset而不是delcare。下面是一些例子:</p>
<pre><code>declare -r -i port_number=8080
declare -r -a my_array=( apple orange )

my_function() {
    local -r name=apple
}
</code></pre><h2 id="5-为所有变量扩展加双引号"><a href="#5-为所有变量扩展加双引号" class="headerlink" title="5. 为所有变量扩展加双引号"></a>5. 为所有变量扩展加双引号</h2><p>为了防止word-splitting和文件扩展,你必须为所有变量扩展加上双引号。尤其是当你需要处理包含空白字符（或其他特殊字符）的文件名时,你必须这么做。考虑这个例子；</p>
<pre><code># create a file containing a space in its name
touch &quot;foo bar&quot;

declare -r my_file=&quot;foo bar&quot;

# try rm-ing the file without quoting the variable
rm  $my_file
# it fails because rm sees two arguments: &quot;foo&quot; and &quot;bar&quot;
# rm: cannot remove `foo&apos;: No such file or directory
# rm: cannot remove `bar&apos;: No such file or directory

# need to quote the variable
rm &quot;$my_file&quot;

# file globbing example:
mesg=&quot;my pattern is *.txt&quot;
echo $mesg
# this is not quoted so *.txt will undergo expansion
# will print &quot;my pattern is foo.txt bar.txt&quot;

# need to quote it for correct output
echo &quot;$msg&quot;
</code></pre><p>为你所有的变量加上双引号是一个好习惯。如果你需要word-splitting,考虑数组替换。参考下一个点。</p>
<h2 id="6-适当的时候使用数组"><a href="#6-适当的时候使用数组" class="headerlink" title="6. 适当的时候使用数组"></a>6. 适当的时候使用数组</h2><p>不要在字符串里面存储元素的集合。使用一个数组替换。例如:</p>
<pre><code># using a string to hold a collection
declare -r hosts=&quot;host1 host2 host3&quot;
for host in $hosts  # not quoting $hosts here, since we want word splitting
do
    echo &quot;$host&quot;
done

# use an array instead!
declare -r -a host_array=( host1 host2 host3 )
for host in &quot;${host_array[@]}&quot;
do
    echo &quot;$host&quot;
done
</code></pre><h2 id="7-使用”-”引用所有变量"><a href="#7-使用”-”引用所有变量" class="headerlink" title="7. 使用”$@”引用所有变量"></a>7. 使用”$@”引用所有变量</h2><p>不要使用$<em>。参考我之前的文章 Difference between $</em>, $@, “$*” and “$@”。这里是一个例子:</p>
<pre><code>main() {
    # print each argument
    for i in &quot;$@&quot;
    do
        echo &quot;$i&quot;
    done
}
# pass all arguments to main
main &quot;$@&quot;
</code></pre><h2 id="8-仅对环境变量使用大写"><a href="#8-仅对环境变量使用大写" class="headerlink" title="8. 仅对环境变量使用大写"></a>8. 仅对环境变量使用大写</h2><p>我的个人偏好是所有变量都使用小写,除非是环境变量。例如:</p>
<pre><code>declare -i port_number=8080

# JAVA_HOME and CLASSPATH are environment variables
&quot;$JAVA_HOME&quot;/bin/java -cp &quot;$CLASSPATH&quot; app.Main &quot;$port_number&quot;
</code></pre><h2 id="9-倾向于使用shell自带命令而不是扩展程序"><a href="#9-倾向于使用shell自带命令而不是扩展程序" class="headerlink" title="9. 倾向于使用shell自带命令而不是扩展程序"></a>9. 倾向于使用shell自带命令而不是扩展程序</h2><p>shell有能力去处理字符串和简单的算术,所以你不必调用像cut和sed这样的程序。这里是一些例子:</p>
<pre><code>declare -r my_file=&quot;/var/tmp/blah&quot;

# instead of dirname, use:
declare -r file_dir=&quot;{my_file%/*}&quot;

# instead of basename, use:
declare -r file_base=&quot;{my_file###*/}&quot;

# instead of sed &apos;s/blah/hello&apos;, use:
declare -r new_file=&quot;${my_file/blah/hello}&quot;

# instead of bc &lt;&lt;&lt; &quot;2+2&quot;, use:
echo $(( 2+2 ))

# instead of grepping a pattern in a string, use:
[[ $line =~ .*blah$ ]]

# instead of cut -d:, use an array:
IFS=: read -a arr &lt;&lt;&lt; &quot;one:two:three&quot;
</code></pre><p>注意,当出来大文件或输入的时候,扩展程序将会表现的更好。</p>
<h2 id="10-避免非必须管道"><a href="#10-避免非必须管道" class="headerlink" title="10. 避免非必须管道"></a>10. 避免非必须管道</h2><p>管道会增加而外的开销,所以尽量保持你的管道数量最少。通常没用的例子是cat和echo,如下所示:</p>
<h3 id="1-避免非必须的cat"><a href="#1-避免非必须的cat" class="headerlink" title="(1) 避免非必须的cat"></a>(1) 避免非必须的cat</h3><p>如果你不熟悉没用cat的判定,看这里。cat命令只应该被使用于一系列的文件,而不是把输出发送给另一个命令。</p>
<pre><code># instead of
cat file | command
# use
command &lt; file
</code></pre><h3 id="2-避免使用非必须的echo"><a href="#2-避免使用非必须的echo" class="headerlink" title="(2) 避免使用非必须的echo"></a>(2) 避免使用非必须的echo</h3><p>仅当你想要输出文本到stdout、stderr、文件等。如果你想要发送文本到另外一个命令,不要使用echo通过管道发送。使用一个here-string替换。注意here-string不是可移植的（但是大多数现代shell支持它们）。所以使用一个注释,如果编写一个可移植的脚本。（参考我之前的文章:Useless Use of Echo。）</p>
<pre><code># instead of
echo text | command
# use
command &lt;&lt;&lt; text

# for portability, use a heredoc
command &lt;&lt; END
text
END
</code></pre><h3 id="3-避免非必须的grep"><a href="#3-避免非必须的grep" class="headerlink" title="(3) 避免非必须的grep"></a>(3) 避免非必须的grep</h3><p>从grep到awk或sed的管道是没必要的。自从awk和sed都能grep,你就不必再用管道来grep了。（参考我之前的文章:Useless Use of Grep）</p>
<pre><code># instead of
grep pattern file | awk &apos;{print $1}&apos;
# use
awk &apos;/pattern/{print $1}&apos;

# instead of
grep pattern file | sed &apos;s/foo/bar/g&apos;
# use
sed -n &apos;/pattern/{s/foo/bar/p}&apos; file
</code></pre><h3 id="4-其他非必要的管道"><a href="#4-其他非必要的管道" class="headerlink" title="(4) 其他非必要的管道"></a>(4) 其他非必要的管道</h3><p>这里是一些例子:</p>
<pre><code># instead of
command | sort | uniq
# use
command | sort -u

# instead of
command | grep pattern | wc -l
# use
command | grep -c pattern
</code></pre><h2 id="11-避免解析ls"><a href="#11-避免解析ls" class="headerlink" title="11. 避免解析ls"></a>11. 避免解析ls</h2><p>ls的问题是以新的行输出文件名,所以如果你的文件名包含一个换行字符,你将不能正确的解析它。如果ls能够输出没有分隔符的文件名就更好了,不幸的是,它不能。除了ls,使用文件扩展或者一个能输出没有分隔符的替换命令,比如find -print0.</p>
<h2 id="12-使用globbing"><a href="#12-使用globbing" class="headerlink" title="12. 使用globbing"></a>12. 使用globbing</h2><p>Globbing（或文件扩展）是shell产生匹配模式的一系列文件的方法。在bash,你可以通过打开扩展模式匹配符使用extglob选项来使globbing更加强大。还有,你可以打开nullglob,来得到一个空列表如果没有匹配找到。在一些场合可以使用globbing而不是find,再次声明,不要解析ls！这里是一些例子:</p>
<pre><code>shopt -s nullglob
shopt -s extglob

# get all files with a .yyyymmdd.txt suffix
declare -a dated_files=( *.[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].txt )

# get all non-zip files
declare -a non_zip_files=( !(*.zip) )
</code></pre><h2 id="13-尽可能使用无分隔输出"><a href="#13-尽可能使用无分隔输出" class="headerlink" title="13. 尽可能使用无分隔输出"></a>13. 尽可能使用无分隔输出</h2><p>为了正确处理包含空白字符和换行符的文件名, 你需要使用无分隔输出,每一行以NUL (00)分隔而不是换行符。大多数程序支持这个。例如,find -pirnt0 输出以null字符结尾的文件名,还有xargs -0读取以null字符分隔的参数。</p>
<pre><code># instead of
find . -type f -mtime +5 | xargs rm -f
# use
find . -type f -mtime +5 -print0 | xargs -0 rm -f

# looping over files
find . -type f -print0 | while IFS= read -r -d $&apos;&apos; filename; do
    echo &quot;$filename&quot;
done
</code></pre><h2 id="14-不要使用反斜杠"><a href="#14-不要使用反斜杠" class="headerlink" title="14. 不要使用反斜杠"></a>14. 不要使用反斜杠</h2><p>使用$(command)而不是<code>command</code>,因为更容易嵌套多个命令和更易读。这是一个简单的例子:</p>
<pre><code># ugly escaping required when using nested backticks
a=`command1 \`command2\``

# $(...) is cleaner
b=$(command1 $(command2))
</code></pre><h2 id="15-使用进程替换而不是使用临时文件"><a href="#15-使用进程替换而不是使用临时文件" class="headerlink" title="15. 使用进程替换而不是使用临时文件"></a>15. 使用进程替换而不是使用临时文件</h2><p>大多数情况下,如果命令使用一个文件作为输入,文件可以被替代为另一个命令使用:&lt;(command).这将使你避免写到一个临时文件,传递临时文件到一个命令,最后删除临时文件。如下所示:</p>
<pre><code># using temp files
command1 &gt; file1
command2 &gt; file2
diff file1 file2
rm file1 file2

# using process substitution
diff &lt;(command1) &lt;(command2)
</code></pre><h2 id="16-使用mktemp如果必须创建临时文件"><a href="#16-使用mktemp如果必须创建临时文件" class="headerlink" title="16. 使用mktemp如果必须创建临时文件"></a>16. 使用mktemp如果必须创建临时文件</h2><p>尽量避免创建临时文件。如果必须的话,使用mktemp来创建临时文件夹,然后把文件写进里面。保证在你执行完后目录被移除。</p>
<pre><code># set up a trap to delete the temp dir when the script exits
unset temp_dir
trap &apos;[[ -d &quot;$temp_dir&quot; ]] &amp;&amp; rm -rf &quot;$temp_dir&quot;&apos; EXIT

# create the temp dir
declare -r temp_dir=$(mktemp -dt myapp.XXXXXX)

# write to the temp dir
command &gt; &quot;$temp_dir&quot;/foo
</code></pre><h2 id="17-对于判断条件使用-和"><a href="#17-对于判断条件使用-和" class="headerlink" title="17. 对于判断条件使用((和[["></a>17. 对于判断条件使用((和[[</h2><p>使用[[ … ]]而不是[ … ],因为它更安全,并且提供更丰富的特性。对于算术条件使用(( … )),因为它运行你使用更相似的数学操作符比如&lt;和&gt;,而不是-lt和-gt. 注意,如果你想要为可移植设计,你必须保留旧的方式[ … ].这是一些例子:</p>
<pre><code>[[ $foo == &quot;foo&quot; ]] &amp;&amp; echo &quot;match&quot;  # don&apos;t need to quote variable inside [[
[[ $foo == &quot;a&quot; &amp;&amp; $bar == &quot;a&quot; ]] &amp;&amp; echo &quot;match&quot;

declare -i num=5
(( num &lt; 10 )) &amp;&amp; echo &quot;match&quot;       # don&apos;t need the $ on $num in ((
</code></pre><h2 id="18-在判断条件里面使用命令-而不是使用退出状态"><a href="#18-在判断条件里面使用命令-而不是使用退出状态" class="headerlink" title="18. 在判断条件里面使用命令,而不是使用退出状态"></a>18. 在判断条件里面使用命令,而不是使用退出状态</h2><p>如果你想要检查一个命令在做其他事情之前是否返回成功,直接在判断条件里面使用命令,而不是检查命令的退出状态。</p>
<pre><code># don&apos;t use exit status
grep -q pattern file
if (( $? == 0 ))
then
    echo &quot;pattern was found&quot;
fi

# use the command as the condition
if grep -q pattern file
then
    echo &quot;pattern was found&quot;
fi
</code></pre><h2 id="19-使用set-e"><a href="#19-使用set-e" class="headerlink" title="19. 使用set -e"></a>19. 使用set -e</h2><p>把这个放在你脚本的最上面。这个是告诉shell脚本尽快地退出如果任何语句返回非0的退出码</p>
<h2 id="20-将错误信息输出到stderr"><a href="#20-将错误信息输出到stderr" class="headerlink" title="20. 将错误信息输出到stderr"></a>20. 将错误信息输出到stderr</h2><p>错误信息属于stderr而不是stdout</p>
<pre><code>echo &quot;An error message&quot; &gt;&amp;2
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MongoDB分片搭建]]></title>
      <url>http://panwenhang.github.io/2016/06/23/MongoDB/2016-06-23-MongoDB%E5%88%86%E7%89%87%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-201 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
</code></pre><a id="more"></a>
<p>  $ mongo –version<br>    MongoDB shell version: 3.0.6</p>
<pre><code>node1: 172.30.2.201
node2: 172.30.2.202
node3: 172.30.2.203
</code></pre><h2 id="2-配置-Shard-Server"><a href="#2-配置-Shard-Server" class="headerlink" title="2. 配置 Shard Server"></a>2. 配置 Shard Server</h2><p>在 3 个节点分别执行:</p>
<h3 id="1-创建目录"><a href="#1-创建目录" class="headerlink" title="(1) 创建目录"></a>(1) 创建目录</h3><pre><code>$ sudo mkdir -p /data/mongodb/{sh0,sh1}/{data,backup,log,conf}
</code></pre><h3 id="2-准备配置文件"><a href="#2-准备配置文件" class="headerlink" title="(2) 准备配置文件"></a>(2) 准备配置文件</h3><p>第一个分片:</p>
<pre><code>$ sudo vim /data/mongodb/sh0/conf/mongodb.conf
# base
port = 27010
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/sh0/data
pidfilepath = /data/mongodb/sh0/data/mongodb.pid
oplogSize = 10
journal = true
# security
nohttpinterface = true
rest = false
# log
logpath = /data/mongodb/sh0/log/mongodb.log
logRotate = rename
logappend = true
slowms = 50
replSet = sh0
shardsvr = true
</code></pre><p>第二个分片:</p>
<pre><code>$ sudo vim /data/mongodb/sh1/conf/mongodb.conf
# base
port = 27011
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/sh1/data
pidfilepath = /data/mongodb/sh1/data/mongodb.pid
oplogSize = 10
journal = true
# security
nohttpinterface = true
rest = false
# log
logpath = /data/mongodb/sh1/log/mongodb.log
logRotate = rename
logappend = true
slowms = 50
replSet = sh1
shardsvr = true
</code></pre><h3 id="3-启动-Shard-Server"><a href="#3-启动-Shard-Server" class="headerlink" title="(3) 启动 Shard Server"></a>(3) 启动 Shard Server</h3><pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 41492
child process started successfully, parent exiting
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 41509
child process started successfully, parent exiting
$ ps aux | grep mongo | grep -v grep
root     41492  0.5  0.0 518016 54604 ?        Sl   10:09   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/conf/sh0/mongodb.conf
root     41509  0.5  0.0 516988 51824 ?        Sl   10:09   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/conf/sh1/mongodb.conf
$ mongo --port 27010
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:27010/test
&gt;
bye
$ mongo --port 27011
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:27011/test
&gt;
bye
</code></pre><h2 id="3-配置-Config-Server"><a href="#3-配置-Config-Server" class="headerlink" title="3.配置 Config Server"></a>3.配置 Config Server</h2><p>在 3 个节点分别执行:</p>
<h3 id="1-创建目录-1"><a href="#1-创建目录-1" class="headerlink" title="(1) 创建目录"></a>(1) 创建目录</h3><pre><code>$ sudo mkdir -p /data/mongodb/cf0/{data,backup,log,conf}
</code></pre><h3 id="2-准备配置文件-1"><a href="#2-准备配置文件-1" class="headerlink" title="(2) 准备配置文件"></a>(2) 准备配置文件</h3><pre><code>$ sudo vim /data/mongodb/cf0/conf/config.conf
# base
port = 27000
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/cf0/data
pidfilepath = /data/mongodb/cf0/data/config.pid
oplogSize = 10
journal = true
# security
nohttpinterface = true
rest = false
# log
logpath = /data/mongodb/cf0/log/config.log
logRotate = rename
logappend = true
slowms = 50
configsvr = true
</code></pre><h3 id="3-启动"><a href="#3-启动" class="headerlink" title="(3) 启动"></a>(3) 启动</h3><pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
about to fork child process, waiting until server is ready for connections.
forked process: 41759
child process started successfully, parent exiting
$ ps aux | grep mongo | grep -v grep
root     41492  0.3  0.0 518016 54728 ?        Sl   10:09   0:06 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.3  0.0 518016 54760 ?        Sl   10:09   0:06 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.4  0.0 467828 51684 ?        Sl   10:25   0:03 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
</code></pre><h3 id="4-配置-Query-Routers"><a href="#4-配置-Query-Routers" class="headerlink" title="4. 配置 Query Routers"></a>4. 配置 Query Routers</h3><p>在 3 个节点分别执行:</p>
<h3 id="1-创建目录-2"><a href="#1-创建目录-2" class="headerlink" title="(1) 创建目录"></a>(1) 创建目录</h3><pre><code>$ sudo mkdir -p /data/mongodb/ms0/{data,backup,log,conf}
</code></pre><h3 id="2-准备配置文件-2"><a href="#2-准备配置文件-2" class="headerlink" title="(2) 准备配置文件"></a>(2) 准备配置文件</h3><pre><code>$ sudo vim /data/mongodb/ms0/conf/mongos.conf
# base
port = 30000
maxConns = 800
filePermissions = 0700
fork = true
pidfilepath = /data/mongodb/ms0/data/mongos.pid
# log
logpath = /data/mongodb/ms0/log/mongos.log
logRotate = rename
logappend = true
configdb = 172.30.2.201:27000,172.30.2.202:27000,172.30.2.203:27000
</code></pre><h3 id="3-启动-1"><a href="#3-启动-1" class="headerlink" title="(3) 启动"></a>(3) 启动</h3><pre><code>$ sudo /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
about to fork child process, waiting until server is ready for connections.
forked process: 42233
child process started successfully, parent exiting
$ ps aux | grep mongo | grep -v grep
root     41492  0.3  0.0 518016 54728 ?        Sl   10:09   0:06 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.3  0.0 518016 54760 ?        Sl   10:09   0:07 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.4  0.0 546724 37812 ?        Sl   10:25   0:03 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root     42233  0.5  0.0 233536 10188 ?        Sl   10:38   0:00 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
</code></pre><h2 id="5-初始化副本集"><a href="#5-初始化副本集" class="headerlink" title="5. 初始化副本集"></a>5. 初始化副本集</h2><p>配置副本集的好处是为了高可用,配置单节点是我自己为了节省时间,后续添加节点和副本集的操作一样,分片的配置不需要修改,在任何一个节点执行,这里在 node1 上执行</p>
<p>分片一:</p>
<pre><code>$ mongo --port 27010
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:27010/test
&gt; use admin
switched to db admin
&gt; cfg={_id:&quot;sh0&quot;, members:[ {_id:0,host:&quot;172.30.2.201:27010&quot;}, {_id:1,host:&quot;172.30.2.202:27010&quot;}, {_id:2,host:&quot;172.30.2.203:27010&quot;} ] }
{
        &quot;_id&quot; : &quot;sh0&quot;,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;host&quot; : &quot;172.30.2.201:27010&quot;
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;host&quot; : &quot;172.30.2.202:27010&quot;
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;host&quot; : &quot;172.30.2.203:27010&quot;
                }
        ]
}
&gt; rs.initiate( cfg );
{ &quot;ok&quot; : 1 }
sh0:OTHER&gt; rs.status()
{
        &quot;set&quot; : &quot;sh0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-10-23T05:33:31.920Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27010&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 270,
                        &quot;optime&quot; : Timestamp(1445578404, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-10-23T05:33:24Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1445578408, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-10-23T05:33:28Z&quot;),
                        &quot;configVersion&quot; : 1,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.202:27010&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 5,
                        &quot;stateStr&quot; : &quot;STARTUP2&quot;,
                        &quot;uptime&quot; : 7,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-10-23T05:33:30.289Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-10-23T05:33:30.295Z&quot;),
                        &quot;pingMs&quot; : 1,
                        &quot;configVersion&quot; : 1
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.203:27010&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 5,
                        &quot;stateStr&quot; : &quot;STARTUP2&quot;,
                        &quot;uptime&quot; : 7,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-10-23T05:33:30.289Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-10-23T05:33:30.293Z&quot;),
                        &quot;pingMs&quot; : 1,
                        &quot;configVersion&quot; : 1
                }
        ],
        &quot;ok&quot; : 1
}
sh0:PRIMARY&gt;
bye
</code></pre><p>分片二:</p>
<pre><code>$ mongo --port 27011
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:27011/test
&gt; use admin
switched to db admin
&gt; cfg={_id:&quot;sh1&quot;, members:[ {_id:0,host:&quot;172.30.2.201:27011&quot;}, {_id:1,host:&quot;172.30.2.202:27011&quot;}, {_id:2,host:&quot;172.30.2.203:27011&quot;} ] }
{
        &quot;_id&quot; : &quot;sh1&quot;,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;host&quot; : &quot;172.30.2.201:27011&quot;
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;host&quot; : &quot;172.30.2.202:27011&quot;
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;host&quot; : &quot;172.30.2.203:27011&quot;
                }
        ]
}
&gt; rs.initiate( cfg );
{ &quot;ok&quot; : 1 }
sh1:OTHER&gt; rs.status();
{
        &quot;set&quot; : &quot;sh1&quot;,
        &quot;date&quot; : ISODate(&quot;2015-10-23T05:36:02.365Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27011&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 406,
                        &quot;optime&quot; : Timestamp(1445578557, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-10-23T05:35:57Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1445578561, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-10-23T05:36:01Z&quot;),
                        &quot;configVersion&quot; : 1,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.202:27011&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 5,
                        &quot;stateStr&quot; : &quot;STARTUP2&quot;,
                        &quot;uptime&quot; : 5,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-10-23T05:36:01.168Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-10-23T05:36:01.175Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 1
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.203:27011&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 5,
                        &quot;stateStr&quot; : &quot;STARTUP2&quot;,
                        &quot;uptime&quot; : 5,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-10-23T05:36:01.167Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-10-23T05:36:01.172Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 1
                }
        ],
        &quot;ok&quot; : 1
}
sh1:PRIMARY&gt;
bye
</code></pre><h2 id="6-配置分片"><a href="#6-配置分片" class="headerlink" title="6. 配置分片"></a>6. 配置分片</h2><pre><code>$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
mongos&gt; use admin;
switched to db admin
mongos&gt; sh.addShard(&quot;sh0/172.30.2.201:27010,172.30.2.202:27010,172.30.2.203:27010&quot;);
{ &quot;shardAdded&quot; : &quot;sh0&quot;, &quot;ok&quot; : 1 }
mongos&gt; sh.addShard(&quot;sh1/172.30.2.201:27011,172.30.2.202:27011,172.30.2.203:27011&quot;);
{ &quot;shardAdded&quot; : &quot;sh1&quot;, &quot;ok&quot; : 1 }
mongos&gt; use mydb;
switched to db mydb
mongos&gt; db.createCollection(&quot;test&quot;);
{
        &quot;ok&quot; : 1,
        &quot;$gleStats&quot; : {
                &quot;lastOpTime&quot; : Timestamp(1444358911, 1),
                &quot;electionId&quot; : ObjectId(&quot;56172a4bc03d9b1667f8e928&quot;)
        }
}
mongos&gt; sh.enableSharding(&quot;mydb&quot;);
{ &quot;ok&quot; : 1 }
mongos&gt; sh.shardCollection(&quot;mydb.test&quot;, {&quot;_id&quot;:1});
{ &quot;collectionsharded&quot; : &quot;mydb.test&quot;, &quot;ok&quot; : 1 }
mongos&gt; sh.status();
--- Sharding Status ---
  sharding version: {
        &quot;_id&quot; : 1,
        &quot;minCompatibleVersion&quot; : 5,
        &quot;currentVersion&quot; : 6,
        &quot;clusterId&quot; : ObjectId(&quot;561728b4030ea038bcb57fa0&quot;)
}
  shards:
        {  &quot;_id&quot; : &quot;sh0&quot;,  &quot;host&quot; : &quot;sh0/172.30.2.201:27010,172.30.2.202:27010,172.30.2.203:27010&quot; }
        {  &quot;_id&quot; : &quot;sh1&quot;,  &quot;host&quot; : &quot;sh1/172.30.2.201:27011,172.30.2.202:27011,172.30.2.203:27011&quot; }
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours:
                No recent migrations
  databases:
        {  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
        {  &quot;_id&quot; : &quot;mydb&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;sh0&quot; }
                mydb.test
                        shard key: { &quot;_id&quot; : 1 }
                        chunks:
                                sh0     1
                        { &quot;_id&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;_id&quot; : { &quot;$maxKey&quot; : 1 } } on : sh0 Timestamp(1, 0)
</code></pre><p>可见分片已经配置完成了</p>
<h2 id="7-添加开机启动项"><a href="#7-添加开机启动项" class="headerlink" title="7. 添加开机启动项"></a>7. 添加开机启动项</h2><pre><code>$ sudo vim /etc/rc.local
ulimit -SHn 65535
/opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
/opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
/opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
/opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
</code></pre><h2 id="8-备注"><a href="#8-备注" class="headerlink" title="8. 备注"></a>8. 备注</h2><pre><code>虽然也是 3 台机器,使用分片的好处是可以把两个分片的 primary 设置在不同的节点,这个可以分摊单节点的压力,当然有更多机器就可以把分片放到不同机器上。
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MongoDB分片故障测试和解决方案]]></title>
      <url>http://panwenhang.github.io/2016/06/23/MongoDB/2016-06-23-MongoDB%E5%88%86%E7%89%87%E6%95%85%E9%9A%9C%E6%B5%8B%E8%AF%95%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-201 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
$ mongo --port 30000
MongoDB shell version: 3.0.6
</code></pre><a id="more"></a>
<p>  connecting to: 127.0.0.1:30000/test<br>    mongos&gt; sh.status();<br>    — Sharding Status —<br>      sharding version: {<br>            “_id” : 1,<br>            “minCompatibleVersion” : 5,<br>            “currentVersion” : 6,<br>            “clusterId” : ObjectId(“561728b4030ea038bcb57fa0”)<br>    }<br>      shards:<br>            {  “_id” : “sh0”,  “host” : “sh0/172.30.2.201:27010” }<br>            {  “_id” : “sh1”,  “host” : “sh1/172.30.2.201:27011” }<br>      balancer:<br>            Currently enabled:  yes<br>            Currently running:  no<br>            Failed balancer rounds in last 5 attempts:  0<br>            Migration Results for the last 24 hours:<br>                    No recent migrations<br>      databases:<br>            {  “_id” : “admin”,  “partitioned” : false,  “primary” : “config” }<br>            {  “_id” : “mydb”,  “partitioned” : true,  “primary” : “sh0” }</p>
<h2 id="2-准备"><a href="#2-准备" class="headerlink" title="2. 准备"></a>2. 准备</h2><p>插入测试数据</p>
<pre><code>$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
mongos&gt; use mydb
switched to db mydb
mongos&gt; sh.shardCollection(&quot;mydb.test&quot;,{&quot;Uid&quot;:1});
{ &quot;collectionsharded&quot; : &quot;mydb.test&quot;, &quot;ok&quot; : 1 }
mongos&gt; for (var i = 1; i &lt;= 100000; i++) db.test.save({&quot;Uid&quot;:i,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt; use config;
switched to db config
mongos&gt; db.chunks.find().limit(3);
{ &quot;_id&quot; : &quot;mydb.test-Uid_MinKey&quot;, &quot;lastmod&quot; : Timestamp(10, 1), &quot;lastmodEpoch&quot; : ObjectId(&quot;56178dad030ea038bcb592c2&quot;), &quot;ns&quot; : &quot;mydb.test&quot;, &quot;min&quot; : { &quot;Uid&quot; : { &quot;$minKey&quot; : 1 } }, &quot;max&quot; : { &quot;Uid&quot; : 2 }, &quot;shard&quot; : &quot;sh0&quot; }
{ &quot;_id&quot; : &quot;mydb.test-Uid_2.0&quot;, &quot;lastmod&quot; : Timestamp(1, 2), &quot;lastmodEpoch&quot; : ObjectId(&quot;56178dad030ea038bcb592c2&quot;), &quot;ns&quot; : &quot;mydb.test&quot;, &quot;min&quot; : { &quot;Uid&quot; : 2 }, &quot;max&quot; : { &quot;Uid&quot; : 10 }, &quot;shard&quot; : &quot;sh0&quot; }
{ &quot;_id&quot; : &quot;mydb.test-Uid_10.0&quot;, &quot;lastmod&quot; : Timestamp(11, 1), &quot;lastmodEpoch&quot; : ObjectId(&quot;56178dad030ea038bcb592c2&quot;), &quot;ns&quot; : &quot;mydb.test&quot;, &quot;min&quot; : { &quot;Uid&quot; : 10 }, &quot;max&quot; : { &quot;Uid&quot; : 4691 }, &quot;shard&quot; : &quot;sh1&quot; }
</code></pre><h2 id="3-Shard-Server-节点故障"><a href="#3-Shard-Server-节点故障" class="headerlink" title="3. Shard Server 节点故障"></a>3. Shard Server 节点故障</h2><h3 id="1-分片中副本集的节点故障"><a href="#1-分片中副本集的节点故障" class="headerlink" title="(1) 分片中副本集的节点故障"></a>(1) 分片中副本集的节点故障</h3><p>参考 MongoDB 副本集故障测试和解决方案,Query Routers 会自动更新副本集的配置,只需要在副本集里面解决故障</p>
<h3 id="2-分片故障"><a href="#2-分片故障" class="headerlink" title="(2) 分片故障"></a>(2) 分片故障</h3><p>分片故障是指分片中的节点全部不可用了 ( 如果分片中配置了副本集的话,基本是不会出现这种情况的 )</p>
<p>1) 将 sh0 停机</p>
<pre><code>$ ps aux | grep mongo
wenhang+  1493  0.0  0.0 112640   976 pts/0    S+   10:42   0:00 grep --color=auto mongo
root     41492  0.5  0.1 1045696 118336 ?      Sl   Oct09   8:12 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108720 ?      Sl   Oct09   7:30 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.6  0.0 566248 55208 ?        Sl   Oct09   9:14 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root     41870  0.5  0.0 565220 56180 ?        Sl   Oct09   8:29 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.0 565216 56872 ?        Sl   Oct09   8:25 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 260180 13200 ?        Sl   Oct09   5:40 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 259148 11564 ?        Sl   Oct09   4:31 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
$ ps aux | grep mongo | grep -v grep
root     41492  0.5  0.1 1045696 118336 ?      Sl   Oct09   8:12 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108720 ?      Sl   Oct09   7:31 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.6  0.0 566248 54988 ?        Sl   Oct09   9:14 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root     41870  0.5  0.1 565220 72392 ?        Sl   Oct09   8:29 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.0 565216 56776 ?        Sl   Oct09   8:25 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 260180 13200 ?        Sl   Oct09   5:40 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 259148 11564 ?        Sl   Oct09   4:31 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
$ sudo kill 41492
$ ps aux | grep mongo | grep -v grep
root     41509  0.5  0.1 1028196 108736 ?      Sl   Oct09   7:31 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.6  0.1 566248 87832 ?        Sl   Oct09   9:14 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root     41870  0.5  0.1 565220 89116 ?        Sl   Oct09   8:29 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.0 565216 56852 ?        Sl   Oct09   8:26 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 260180 13200 ?        Sl   Oct09   5:40 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 259148 11564 ?        Sl   Oct09   4:31 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
</code></pre><p>2) 查看集群状态</p>
<pre><code>$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
mongos&gt; sh.status();
--- Sharding Status ---
  sharding version: {
        &quot;_id&quot; : 1,
        &quot;minCompatibleVersion&quot; : 5,
        &quot;currentVersion&quot; : 6,
        &quot;clusterId&quot; : ObjectId(&quot;561728b4030ea038bcb57fa0&quot;)
}
  shards:
        {  &quot;_id&quot; : &quot;sh0&quot;,  &quot;host&quot; : &quot;sh0/172.30.2.201:27010&quot; }
        {  &quot;_id&quot; : &quot;sh1&quot;,  &quot;host&quot; : &quot;sh1/172.30.2.201:27011&quot; }
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  5
        Last reported error:  socket exception [CONNECT_ERROR] for sh0/172.30.2.201:27010
        Time of Reported error:  Sat Oct 10 2015 10:44:56 GMT+0800 (CST)
        Migration Results for the last 24 hours:
                19 : Success
                2 : Failed with error &apos;migration already in progress&apos;, from sh0 to sh1
  databases:
        {  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
        {  &quot;_id&quot; : &quot;mydb&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;sh0&quot; }
                mydb.test
                        shard key: { &quot;Uid&quot; : 1 }
                        chunks:
                                sh0     11
                                sh1     10
                        too many chunks to print, use verbose if you want to force print
</code></pre><p>3) 测试可用性</p>
<pre><code>mongos&gt; db.test.find({Uid:12});
{ &quot;_id&quot; : ObjectId(&quot;56178e473af16d338338d3bf&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:07.205Z&quot;) }
mongos&gt; db.test.find({Uid:8});
Error: error: {
        &quot;$err&quot; : &quot;socket exception [CONNECT_ERROR] for sh0/172.30.2.201:27010&quot;,
        &quot;code&quot; : 11002,
        &quot;shard&quot; : &quot;sh0&quot;
}
mongos&gt; db.test.insert({&quot;Uid&quot;:12,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt; db.test.insert({&quot;Uid&quot;:8,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({
        &quot;nInserted&quot; : 0,
        &quot;writeError&quot; : {
                &quot;code&quot; : 7,
                &quot;errmsg&quot; : &quot;could not contact primary for replica set sh0&quot;
        }
})
mongos&gt;
bye
</code></pre><p>sh0 上的操作都会报错,其他分片的可以正常执行</p>
<p>重新启动 sh0</p>
<pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 2534
child process started successfully, parent exiting
$ ps aux | grep mongo | grep -v grep
root      2534  1.1  0.0 971800 64060 ?        Sl   10:53   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108804 ?      Sl   Oct09   7:33 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.6  0.0 566248 55572 ?        Sl   Oct09   9:17 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root     41870  0.5  0.0 565220 56480 ?        Sl   Oct09   8:33 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.0 565216 57240 ?        Sl   Oct09   8:29 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 260180 13388 ?        Sl   Oct09   5:42 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 259148 11572 ?        Sl   Oct09   4:32 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
mongos&gt; use mydb;
switched to db mydb
mongos&gt; db.test.find({Uid:8});
{ &quot;_id&quot; : ObjectId(&quot;56178e453af16d338338d3bb&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:05.539Z&quot;) }
mongos&gt; db.test.find({Uid:12});
{ &quot;_id&quot; : ObjectId(&quot;56178e473af16d338338d3bf&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:07.205Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187d6743c2e9badb0ca53b&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:52:23.497Z&quot;) }
mongos&gt; db.test.insert({&quot;Uid&quot;:8,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
</code></pre><p>两个分片都可读写</p>
<h2 id="4-Config-Server-故障"><a href="#4-Config-Server-故障" class="headerlink" title="4. Config Server 故障"></a>4. Config Server 故障</h2><h3 id="1-单节点故障"><a href="#1-单节点故障" class="headerlink" title="(1) 单节点故障"></a>(1) 单节点故障</h3><p>1) 停掉 cf0</p>
<pre><code>$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 67608 ?        Sl   10:53   0:03 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108808 ?      Sl   Oct09   7:37 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41855  0.6  0.0 566248 55444 ?        Sl   Oct09   9:23 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root     41870  0.5  0.0 565220 56456 ?        Sl   Oct09   8:38 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.0 565216 56820 ?        Sl   Oct09   8:34 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 261204 13408 ?        Sl   Oct09   5:45 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 259148 11596 ?        Sl   Oct09   4:35 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
[wenhang.pan@zhaopin-2-201 ~]$ sudo kill 41855
[wenhang.pan@zhaopin-2-201 ~]$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 67636 ?        Sl   10:53   0:03 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108808 ?      Sl   Oct09   7:37 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41870  0.5  0.0 565220 56324 ?        Sl   Oct09   8:38 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.0 565216 56600 ?        Sl   Oct09   8:34 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 261204 13408 ?        Sl   Oct09   5:45 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 259148 11596 ?        Sl   Oct09   4:35 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
</code></pre><p>2) 查看状态</p>
<pre><code>$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
mongos&gt; sh.status();
--- Sharding Status ---
  sharding version: {
        &quot;_id&quot; : 1,
        &quot;minCompatibleVersion&quot; : 5,
        &quot;currentVersion&quot; : 6,
        &quot;clusterId&quot; : ObjectId(&quot;561728b4030ea038bcb57fa0&quot;)
}
  shards:
        {  &quot;_id&quot; : &quot;sh0&quot;,  &quot;host&quot; : &quot;sh0/172.30.2.201:27010&quot; }
        {  &quot;_id&quot; : &quot;sh1&quot;,  &quot;host&quot; : &quot;sh1/172.30.2.201:27011&quot; }
  balancer:
        Currently enabled:  yes
        Currently running:  yes
                Balancer lock taken at Sat Oct 10 2015 11:08:14 GMT+0800 (CST) by zhaopin-2-201:30000:1444358323:1804289383:Balancer:1681692777
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours:
                19 : Success
                2 : Failed with error &apos;migration already in progress&apos;, from sh0 to sh1
  databases:
        {  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
        {  &quot;_id&quot; : &quot;mydb&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;sh0&quot; }
                mydb.test
                        shard key: { &quot;Uid&quot; : 1 }
                        chunks:
                                sh0     11
                                sh1     10
                        too many chunks to print, use verbose if you want to force print
</code></pre><p>3) 测试可用性</p>
<pre><code>mongos&gt; use mydb;
switched to db mydb
mongos&gt; db.test.find({Uid:12});
{ &quot;_id&quot; : ObjectId(&quot;56178e473af16d338338d3bf&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:07.205Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187d6743c2e9badb0ca53b&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:52:23.497Z&quot;) }
mongos&gt; db.test.find({Uid:8});
{ &quot;_id&quot; : ObjectId(&quot;56178e453af16d338338d3bb&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:05.539Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187e117ec37f9bb7244496&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:55:13.529Z&quot;) }
mongos&gt; db.test.insert({&quot;Uid&quot;:8,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt; db.test.insert({&quot;Uid&quot;:12,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt;
bye
</code></pre><p>集群可正常使用</p>
<h3 id="2-双节点故障"><a href="#2-双节点故障" class="headerlink" title="(2) 双节点故障"></a>(2) 双节点故障</h3><p>1) 停掉 cf1</p>
<pre><code>$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 68128 ?        Sl   10:53   0:05 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108892 ?      Sl   Oct09   7:39 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     41870  0.5  0.1 565220 92328 ?        Sl   Oct09   8:40 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     42170  0.5  0.1 565216 90392 ?        Sl   Oct09   8:36 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 261204 13508 ?        Sl   Oct09   5:46 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 260172 11716 ?        Sl   Oct09   4:37 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
$ sudo kill 41870
$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 68128 ?        Sl   10:53   0:05 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root     41509  0.5  0.1 1028196 108892 ?      Sl   Oct09   7:39 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     42170  0.5  0.1 565216 90408 ?        Sl   Oct09   8:36 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 261204 13508 ?        Sl   Oct09   5:46 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 260172 11716 ?        Sl   Oct09   4:37 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
</code></pre><p>2) 查看集群状态</p>
<pre><code>$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
mongos&gt; sh.status();
--- Sharding Status ---
  sharding version: {
        &quot;_id&quot; : 1,
        &quot;minCompatibleVersion&quot; : 5,
        &quot;currentVersion&quot; : 6,
        &quot;clusterId&quot; : ObjectId(&quot;561728b4030ea038bcb57fa0&quot;)
}
  shards:
        {  &quot;_id&quot; : &quot;sh0&quot;,  &quot;host&quot; : &quot;sh0/172.30.2.201:27010&quot; }
        {  &quot;_id&quot; : &quot;sh1&quot;,  &quot;host&quot; : &quot;sh1/172.30.2.201:27011&quot; }
  balancer:
        Currently enabled:  yes
        Currently running:  yes
                Balancer lock taken at Sat Oct 10 2015 11:08:14 GMT+0800 (CST) by zhaopin-2-201:30000:1444358323:1804289383:Balancer:1681692777
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours:
                19 : Success
                2 : Failed with error &apos;migration already in progress&apos;, from sh0 to sh1
  databases:
        {  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
        {  &quot;_id&quot; : &quot;mydb&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;sh0&quot; }
                mydb.test
                        shard key: { &quot;Uid&quot; : 1 }
                        chunks:
                                sh0     11
                                sh1     10
                        too many chunks to print, use verbose if you want to force print
</code></pre><p>3) 测试可用性</p>
<pre><code>mongos&gt; use mydb;
switched to db mydb
mongos&gt; db.test.find({Uid:8});
{ &quot;_id&quot; : ObjectId(&quot;56178e453af16d338338d3bb&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:05.539Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187e117ec37f9bb7244496&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:55:13.529Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;5618824d7bd66d65a90fb01c&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T03:13:17.564Z&quot;) }
mongos&gt; db.test.find({Uid:12});
{ &quot;_id&quot; : ObjectId(&quot;56178e473af16d338338d3bf&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:07.205Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187d6743c2e9badb0ca53b&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:52:23.497Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;561882527bd66d65a90fb01d&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T03:13:22.594Z&quot;) }
mongos&gt; db.test.insert({&quot;Uid&quot;:12,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt; db.test.insert({&quot;Uid&quot;:8,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt;
bye
</code></pre><p>集群可以正常使用</p>
<p>4) 重新启动 cf0 和 cf1</p>
<pre><code>$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 68228 ?        Sl   10:53   0:05 /opt/mongodb/bin/mongod --config /data/mongodb/conf/sh0/mongodb.conf
root     41509  0.5  0.1 1028196 107976 ?      Sl   Oct09   7:39 /opt/mongodb/bin/mongod --config /data/mongodb/conf/sh1/mongodb.conf
root     42170  0.5  0.1 565216 93140 ?        Sl   Oct09   8:37 /opt/mongodb/bin/mongod --config /data/mongodb/conf/cf2/config.conf
root     42233  0.3  0.0 261204 13540 ?        Sl   Oct09   5:47 /opt/mongodb/bin/mongos --config /data/mongodb/conf/ms0/mongos.conf
root     42286  0.3  0.0 260172 11752 ?        Sl   Oct09   4:37 /opt/mongodb/bin/mongos --config /data/mongodb/conf/ms1/mongos.conf
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/conf/cf0/config.conf
about to fork child process, waiting until server is ready for connections.
forked process: 7309
child process started successfully, parent exiting
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/conf/cf1/config.conf
about to fork child process, waiting until server is ready for connections.
forked process: 7355
child process started successfully, parent exiting
$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 68228 ?        Sl   10:53   0:06 /opt/mongodb/bin/mongod --config /data/mongodb/conf/sh0/mongodb.conf
root      7309  0.6  0.0 545696 37504 ?        Sl   11:17   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/conf/cf0/config.conf
root      7355  0.5  0.0 542616 33748 ?        Sl   11:17   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/conf/cf1/config.conf
root     41509  0.5  0.1 1028196 108188 ?      Sl   Oct09   7:40 /opt/mongodb/bin/mongod --config /data/mongodb/conf/sh1/mongodb.conf
root     42170  0.5  0.0 565216 57992 ?        Sl   Oct09   8:37 /opt/mongodb/bin/mongod --config /data/mongodb/conf/cf2/config.conf
root     42233  0.3  0.0 261204 13540 ?        Sl   Oct09   5:47 /opt/mongodb/bin/mongos --config /data/mongodb/conf/ms0/mongos.conf
root     42286  0.3  0.0 260172 11768 ?        Sl   Oct09   4:38 /opt/mongodb/bin/mongos --config /data/mongodb/conf/ms1/mongos.conf
</code></pre><p>5) 解决方案</p>
<p>参考:</p>
<p><a href="http://docs.mongodb.org/manual/tutorial/replace-config-server/" target="_blank" rel="external">http://docs.mongodb.org/manual/tutorial/replace-config-server/</a></p>
<p><a href="http://docs.mongodb.org/manual/tutorial/migrate-config-servers-with-different-hostnames/" target="_blank" rel="external">http://docs.mongodb.org/manual/tutorial/migrate-config-servers-with-different-hostnames/</a></p>
<h2 id="5-Query-Routers故障"><a href="#5-Query-Routers故障" class="headerlink" title="5. Query Routers故障"></a>5. Query Routers故障</h2><p>1) 将 ms0 停机</p>
<pre><code>$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 68228 ?        Sl   10:53   0:06 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root      7309  0.6  0.0 545696 37504 ?        Sl   11:17   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root      7355  0.5  0.0 542616 33748 ?        Sl   11:17   0:00 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     41509  0.5  0.1 1028196 108188 ?      Sl   Oct09   7:40 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     42170  0.5  0.0 565216 57992 ?        Sl   Oct09   8:37 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42233  0.3  0.0 261204 13540 ?        Sl   Oct09   5:47 /opt/mongodb/bin/mongos --config /data/mongodb/ms0/conf/mongos.conf
root     42286  0.3  0.0 260172 11768 ?        Sl   Oct09   4:38 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
$ sudo kill 42233
$ ps axu | grep mongo | grep -v grep
root      2534  0.4  0.1 982052 68408 ?        Sl   10:53   0:06 /opt/mongodb/bin/mongod --config /data/mongodb/sh0/conf/mongodb.conf
root      7309  0.6  0.0 547752 39096 ?        Sl   11:17   0:01 /opt/mongodb/bin/mongod --config /data/mongodb/cf0/conf/config.conf
root      7355  0.6  0.0 544672 34256 ?        Sl   11:17   0:01 /opt/mongodb/bin/mongod --config /data/mongodb/cf1/conf/config.conf
root     41509  0.5  0.1 1028196 108164 ?      Sl   Oct09   7:40 /opt/mongodb/bin/mongod --config /data/mongodb/sh1/conf/mongodb.conf
root     42170  0.5  0.0 565216 57796 ?        Sl   Oct09   8:38 /opt/mongodb/bin/mongod --config /data/mongodb/cf2/conf/config.conf
root     42286  0.3  0.0 260172 11768 ?        Sl   Oct09   4:38 /opt/mongodb/bin/mongos --config /data/mongodb/ms1/conf/mongos.conf
</code></pre><p>2) 查看集群状态</p>
<pre><code>$ mongo --port 30000
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30000/test
2015-10-10T11:21:04.681+0800 W NETWORK  Failed to connect to 127.0.0.1:30000, reason: errno:111 Connection refused
2015-10-10T11:21:04.683+0800 E QUERY    Error: couldn&apos;t connect to server 127.0.0.1:30000 (127.0.0.1), connection attempt failed
    at connect (src/mongo/shell/mongo.js:181:14)
    at (connect):1:6 at src/mongo/shell/mongo.js:181
exception: connect failed
$ mongo --port 30001
MongoDB shell version: 3.0.6
connecting to: 127.0.0.1:30001/test
mongos&gt; sh.status();
--- Sharding Status ---
  sharding version: {
        &quot;_id&quot; : 1,
        &quot;minCompatibleVersion&quot; : 5,
        &quot;currentVersion&quot; : 6,
        &quot;clusterId&quot; : ObjectId(&quot;561728b4030ea038bcb57fa0&quot;)
}
  shards:
        {  &quot;_id&quot; : &quot;sh0&quot;,  &quot;host&quot; : &quot;sh0/172.30.2.201:27010&quot; }
        {  &quot;_id&quot; : &quot;sh1&quot;,  &quot;host&quot; : &quot;sh1/172.30.2.201:27011&quot; }
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours:
                19 : Success
                2 : Failed with error &apos;migration already in progress&apos;, from sh0 to sh1
  databases:
        {  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
        {  &quot;_id&quot; : &quot;mydb&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;sh0&quot; }
                mydb.test
                        shard key: { &quot;Uid&quot; : 1 }
                        chunks:
                                sh0     11
                                sh1     10
                        too many chunks to print, use verbose if you want to force print
mongos&gt;
bye
</code></pre><p>可见 30000 端口的 mongos 已经宕机,不能提供服务了,但 30001 端口的 mongos 可以使用</p>
<p>3) 测试可用性</p>
<pre><code>mongos&gt; use mydb;
switched to db mydb
mongos&gt; db.test.find({Uid:12});
{ &quot;_id&quot; : ObjectId(&quot;56178e473af16d338338d3bf&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:07.205Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187d6743c2e9badb0ca53b&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:52:23.497Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;561882527bd66d65a90fb01d&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T03:13:22.594Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;561882f9ff80495f50bec62f&quot;), &quot;Uid&quot; : 12, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T03:16:09.930Z&quot;) }
mongos&gt; db.test.find({Uid:8});
{ &quot;_id&quot; : ObjectId(&quot;56178e453af16d338338d3bb&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-09T09:52:05.539Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;56187e117ec37f9bb7244496&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T02:55:13.529Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;5618824d7bd66d65a90fb01c&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T03:13:17.564Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;561882fdff80495f50bec630&quot;), &quot;Uid&quot; : 8, &quot;Name&quot; : &quot;zhanjindong&quot;, &quot;Age&quot; : 13, &quot;Date&quot; : ISODate(&quot;2015-10-10T03:16:13.788Z&quot;) }
mongos&gt; db.test.insert({&quot;Uid&quot;:8,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
mongos&gt; db.test.insert({&quot;Uid&quot;:12,&quot;Name&quot;:&quot;zhanjindong&quot;,&quot;Age&quot;:13,&quot;Date&quot;:new Date()});
WriteResult({ &quot;nInserted&quot; : 1 })
</code></pre><p>可见集群可以正常使用</p>
<p>4) 解决方案</p>
<p>可以在代码中配置连接多台 mongo 实现 failover,java 代码如下:</p>
<pre><code>List&lt;ServerAddress&gt; mongoHostList = new ArrayList&lt;ServerAddress&gt;();
mongoHostList.add(new ServerAddress(&quot;ip&quot;,port));
mongoHostList.add(new ServerAddress(&quot;ip&quot;,port));
mongoHostList.add(new ServerAddress(&quot;ip&quot;,port));
MongoClient mg = new MongoClient(mongoHostList);
</code></pre><h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h2><h3 id="1-Shard-Server-故障"><a href="#1-Shard-Server-故障" class="headerlink" title="(1) Shard Server 故障"></a>(1) Shard Server 故障</h3><p>1) 分片节点故障,可以按照 MongoDB 副本集故障测试和解决方案中解决,仍可以实现可用性</p>
<p>2) 整个分片故障,如果可以重新启动的话,仍然可以使用,否则只能从备份中恢复,必然会丢数据</p>
<h3 id="2-Config-Server-故障"><a href="#2-Config-Server-故障" class="headerlink" title="(2) Config Server 故障"></a>(2) Config Server 故障</h3><p>只要有可用的配置服务器,整个集群都可以正常使用</p>
<h3 id="3-Query-Routers-故障"><a href="#3-Query-Routers-故障" class="headerlink" title="(3) Query Routers 故障"></a>(3) Query Routers 故障</h3><p>只要有可用的 mongos 服务器,整个集群就可以正常使用,要客户端也实现 failover,需要将代码写成连接多台 mongo 的方式</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MongoDB副本集搭建]]></title>
      <url>http://panwenhang.github.io/2016/06/23/MongoDB/2016-06-23-MongoDB%E5%89%AF%E6%9C%AC%E9%9B%86%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-201 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
$ mongo --version
MongoDB shell version: 3.0.6
</code></pre><a id="more"></a>
<h2 id="2-准备"><a href="#2-准备" class="headerlink" title="2. 准备"></a>2. 准备</h2><h3 id="1-创建目录"><a href="#1-创建目录" class="headerlink" title="(1) 创建目录"></a>(1) 创建目录</h3><pre><code>$ sudo mkdir -p /data/mongodb/db0/{data,backup,log,conf}
</code></pre><h3 id="2-编写配置文件"><a href="#2-编写配置文件" class="headerlink" title="(2) 编写配置文件"></a>(2) 编写配置文件</h3><pre><code>$ sudo vim /data/mongodb/db0/conf/mongodb.conf
# base
port = 27017
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/db0/data
pidfilepath = /data/mongodb/db0/data/mongodb.pid
journal = true

# security
nohttpinterface = true
rest = false

# log
logpath = /data/mongodb/db0/log/mongodb.log
logRotate = rename
logappend = true
slowms = 50
replSet = rs0
</code></pre><h2 id="3-配置副本集"><a href="#3-配置副本集" class="headerlink" title="3. 配置副本集"></a>3. 配置副本集</h2><h3 id="1-配置-primary"><a href="#1-配置-primary" class="headerlink" title="(1) 配置 primary"></a>(1) 配置 primary</h3><p>1) 启动</p>
<pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 48583
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; rs.status();
{
        &quot;info&quot; : &quot;run rs.initiate(...) if not yet done for the set&quot;,
        &quot;ok&quot; : 0,
        &quot;errmsg&quot; : &quot;no replset config has been received&quot;,
        &quot;code&quot; : 94
}
</code></pre><p>2) 初始化</p>
<pre><code>&gt; cfg={_id:&quot;rs0&quot;, members:[ {_id:0,host:&quot;172.30.2.201:27017&quot;}] }
{
        &quot;_id&quot; : &quot;rs0&quot;,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;host&quot; : &quot;172.30.2.201:27017&quot;
                }
        ]
}
&gt; rs.initiate( cfg );
{ &quot;ok&quot; : 1 }
rs0:OTHER&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-25T08:31:36.354Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 112,
                        &quot;optime&quot; : Timestamp(1443169891, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-25T08:31:31Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443169891, 2),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-25T08:31:31Z&quot;),
                        &quot;configVersion&quot; : 1,
                        &quot;self&quot; : true
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><h3 id="2-添加节点"><a href="#2-添加节点" class="headerlink" title="(2) 添加节点"></a>(2) 添加节点</h3><p>1) 启动新的节点</p>
<pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 41794
child process started successfully, parent exiting
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 3761
child process started successfully, parent exiting
</code></pre><p>2) 添加新的节点</p>
<p>在 primary 上执行:</p>
<pre><code>rs0:PRIMARY&gt; rs.add( &quot;172.30.2.202:27017&quot; );
{ &quot;ok&quot; : 1 }
rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-25T08:34:48.161Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 304,
                        &quot;optime&quot; : Timestamp(1443170060, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-25T08:34:20Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443169891, 2),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-25T08:31:31Z&quot;),
                        &quot;configVersion&quot; : 2,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 27,
                        &quot;optime&quot; : Timestamp(1443170060, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-25T08:34:20Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-25T08:34:46.884Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-25T08:34:46.896Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 2
                }
        ],
        &quot;ok&quot; : 1
}
rs0:PRIMARY&gt; rs.add( &quot;172.30.2.203:27017&quot; );
{ &quot;ok&quot; : 1 }
rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-25T08:36:22.579Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 398,
                        &quot;optime&quot; : Timestamp(1443170158, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-25T08:35:58Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443169891, 2),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-25T08:31:31Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 121,
                        &quot;optime&quot; : Timestamp(1443170158, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-25T08:35:58Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-25T08:36:22.268Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-25T08:36:20.949Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;syncingTo&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;configVersion&quot; : 3
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 24,
                        &quot;optime&quot; : Timestamp(1443170158, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-25T08:35:58Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-25T08:36:22.267Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-25T08:36:22.275Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 3
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><h3 id="3-设置开机启动"><a href="#3-设置开机启动" class="headerlink" title="(3) 设置开机启动"></a>(3) 设置开机启动</h3><pre><code>$ sudo vim /etc/rc.local
/opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
</code></pre><h2 id="4-验证"><a href="#4-验证" class="headerlink" title="4.验证"></a>4.验证</h2><h3 id="1-数据同步"><a href="#1-数据同步" class="headerlink" title="(1) 数据同步"></a>(1) 数据同步</h3><p>1) primary:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:PRIMARY&gt; use aaa;
switched to db aaa
rs0:PRIMARY&gt; db.createCollection( &quot;test&quot; );
{ &quot;ok&quot; : 1 }
rs0:PRIMARY&gt; show collections;
system.indexes
test
</code></pre><p>2) secondary:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; rs.slaveOk();
rs0:SECONDARY&gt; use aaa;
switched to db aaa
rs0:SECONDARY&gt; show collections;
system.indexes
test
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; rs.slaveOk();
rs0:SECONDARY&gt; use aaa;
switched to db aaa
rs0:SECONDARY&gt; show collections;
system.indexes
test
</code></pre><h3 id="2-读写验证"><a href="#2-读写验证" class="headerlink" title="(2) 读写验证"></a>(2) 读写验证</h3><p>由上可知,primary 是可读写的</p>
<p>在 secondary 上写测试:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; use aaa;
switched to db aaa
&gt; db.createCollection(&quot;test2&quot;);
{ &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; }
&gt; show collections;
2015-09-23T15:57:49.240+0800 E QUERY    Error: listCollections failed: { &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; }
    at Error (&lt;anonymous&gt;)
    at DB._getCollectionInfosCommand (src/mongo/shell/db.js:646:15)
    at DB.getCollectionInfos (src/mongo/shell/db.js:658:20)
    at DB.getCollectionNames (src/mongo/shell/db.js:669:17)
    at shellHelper.show (src/mongo/shell/utils.js:625:12)
    at shellHelper (src/mongo/shell/utils.js:524:36)
    at (shellhelp2):1:1 at src/mongo/shell/db.js:646
&gt; rs.slaveOk();
&gt; show collections;
system.indexes
test
</code></pre><p>证明 secondary 节点是不能写,默认也是不可读的,需要执行 rs.slaveOk(); 或者 db.getMongo().setSlaveOk(); 并且只对当前 session 有效,所以每次连接从库都需要执行。</p>
<h2 id="4-说明"><a href="#4-说明" class="headerlink" title="4. 说明"></a>4. 说明</h2><p>mongodb 的副本集至少需要3台以上才能实现高可用,并且节点的个数最好是基数。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[C语言side effect和sequence point]]></title>
      <url>http://panwenhang.github.io/2016/06/23/C/2016-06-23-C%E8%AF%AD%E8%A8%80side%20effect%E5%92%8Csequence%20point/</url>
      <content type="html"><![CDATA[<p>  C 语言中副作用 ( side effect ) : 是指对数据对象或者文件的修改。例如,语句 var = 99 ; 的副作用是把 var 的值修改成 99 。对表达式求值也可能产生副作用,例如,对表达式求 se = 100;<br>求值所产生的副作用就是 se 的值被修改成 100。<br>  序列点 ( sequence point ) : 是指程序运行中的一个特殊的时间点,在该点之前的所有副作用已经结束,并且后续的副作用还没发生。C 语句结束标志——分号 ( ; ) 是序列点。标准规定,在两个序列点之间,一个对象所保存的值最多只能被修改一次。</p>
<a id="more"></a>
<p>  C 语句中由赋值、自增或者自减等引起的副作用在分号之前必须结束。我们以后会说到一些包含序列点的运算符。任何完整表达式 ( full expression ) 运算结束的那个时间点也是序列点。所谓完整表达式,就是说这个表达式不是子表达式。而所谓的子表达式,则是指表达式中的表达式。例如:f = ++e % 3 这整个表达式就是一个完整表达式。这个表达式中的 ++e、3 和 ++e % 3 都是它的子表达式。</p>
<p>有了序列点的概念,我们下面来分析一下一个很常见的错误 :</p>
<pre><code>int x = 1, y ;
y = x++ + x++ ;
</code></pre><p>这里 y = x++ + x++ 是完整表达式,而 x++ 是它的子表达式。这个完整表达式运算结束的那一点是一个序列点,int x = 1, y ; 中的 ; 也是一个序列点。也就是说 x++ + x++ 位于两个序列点之间。标准规定,在两个序列点之间,一个对象所保存的值最多只能被修改一次。但是我们清楚可以看到,上面这个例子中,x 的值在两个序列点之间被修改了两次。这显然是错误的！这段代码在不同的编译器上编译可能会导致 y 的值有所不同。比较常见的结果是 y 的值最后被修改为 2 或者 3 。在此,我不打算就这个问题作更深入的分析,各位只要记住这是错误的,别这么用就可以了。有兴趣的话,可以看看以下列出的相关资料。</p>
<p>C 语言标准对副作用和序列点的定义如下:<br>Accessing a volatile object, modifying an object, modifying a file, or calling a function that does any of those operations are all side effects, which are changes in the state of the execution environment. Evaluation of an expression may produce side effects. At certain specified points in the execution sequence called sequence points, all side effects of previous evaluations shall be complete and no side effects of subsequent evaluations shall have taken place.<br>翻译如下:</p>
<p>访问易变对象,修改对象或文件,或者调用包含这些操作的函数都是副作用,它们都会改变执行环境的状态。计算表达式也会引起副作用。执行序列中某些特定的点被称为序列点。在序列点上,该点之前所有运算的副作用都应该结束,并且后继运算的副作用还没发生。</p>
<p>顺序点有:</p>
<ol>
<li>The point of calling a function, after evaluating its arguments.</li>
<li>The end of the first operand of the &amp;&amp; operator.</li>
<li>The end of the first operand of the || operator.</li>
<li>The end of the first operand of the ?: conditional operator.</li>
<li>The end of the each operand of the comma operator.</li>
<li>Completing the evaluation of a full expression. They are the following:</li>
<li>Evaluating the initializer of an auto object.</li>
<li>The expression in an ‘ordinary’ statement—an expression followed by semicolon.</li>
<li>The controlling expressions in do , while , if , switch or for statements.</li>
<li>The other two expressions in a for statement.</li>
<li>The expression in a return statement.</li>
</ol>
<p>编译时可以加上 “ -Wsequence-point “ 让编译器帮我们检查可能的关于检查点的错误。</p>
<pre><code>$ vim test_sequence_point.c
＃include &lt;stdio.h&gt;
int main() {
    int i = 12 ;
    i = i-- ;
    printf( &quot;the i is %d/n&quot; , i ) ;
    return 0 ;
}

$ gcc -Wsequence-point test_sequence_point.c
test_sequence_point.c: In function `main&apos;:
test_sequence_point.c:10: warning: operation on `i&apos; may be undefined
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MongoDB副本集故障测试和解决方案]]></title>
      <url>http://panwenhang.github.io/2016/06/23/MongoDB/2016-06-23-MongoDB%E5%89%AF%E6%9C%AC%E9%9B%86%E6%95%85%E9%9A%9C%E6%B5%8B%E8%AF%95%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-201 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
</code></pre><a id="more"></a>
<p>  rs0:PRIMARY&gt; rs.status();<br>    {<br>            “set” : “rs0”,<br>            “date” : ISODate(“2015-09-28T07:00:05.507Z”),<br>            “myState” : 1,<br>            “members” : [<br>                    {<br>                            “_id” : 0,<br>                            “name” : “172.30.2.201:27017”,<br>                            “health” : 1,<br>                            “state” : 1,<br>                            “stateStr” : “PRIMARY”,<br>                            “uptime” : 83,<br>                            “optime” : Timestamp(1443423600, 1),<br>                            “optimeDate” : ISODate(“2015-09-28T07:00:00Z”),<br>                            “electionTime” : Timestamp(1443423535, 2),<br>                            “electionDate” : ISODate(“2015-09-28T06:58:55Z”),<br>                            “configVersion” : 3,<br>                            “self” : true<br>                    },<br>                    {<br>                            “_id” : 1,<br>                            “name” : “172.30.2.203:27017”,<br>                            “health” : 1,<br>                            “state” : 2,<br>                            “stateStr” : “SECONDARY”,<br>                            “uptime” : 44,<br>                            “optime” : Timestamp(1443423600, 1),<br>                            “optimeDate” : ISODate(“2015-09-28T07:00:00Z”),<br>                            “lastHeartbeat” : ISODate(“2015-09-28T07:00:04.918Z”),<br>                            “lastHeartbeatRecv” : ISODate(“2015-09-28T07:00:05.042Z”),<br>                            “pingMs” : 0,<br>                            “syncingTo” : “172.30.2.201:27017”,<br>                            “configVersion” : 3<br>                    },<br>                    {<br>                            “_id” : 2,<br>                            “name” : “172.30.2.202:27017”,<br>                            “health” : 1,<br>                            “state” : 5,<br>                            “stateStr” : “STARTUP2”,<br>                            “uptime” : 4,<br>                            “optime” : Timestamp(0, 0),<br>                            “optimeDate” : ISODate(“1970-01-01T00:00:00Z”),<br>                            “lastHeartbeat” : ISODate(“2015-09-28T07:00:04.918Z”),<br>                            “lastHeartbeatRecv” : ISODate(“2015-09-28T07:00:04.926Z”),<br>                            “pingMs” : 0,<br>                            “configVersion” : 3<br>                    }<br>            ],<br>            “ok” : 1<br>    }</p>
<h2 id="2-单节点故障"><a href="#2-单节点故障" class="headerlink" title="2. 单节点故障"></a>2. 单节点故障</h2><h3 id="1-primary-节点故障"><a href="#1-primary-节点故障" class="headerlink" title="(1) primary 节点故障"></a>(1) primary 节点故障</h3><p>1) 关闭 primay 节点</p>
<pre><code>rs0:PRIMARY&gt; use admin;
switched to db admin
rs0:PRIMARY&gt; db.shutdownServer();
2015-09-28T15:00:51.828+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-28T15:00:51.830+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:00:51.831+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) ok
2015-09-28T15:00:51.831+0800 I NETWORK  DBClientCursor::init call() failed
&gt;
bye
</code></pre><p>2) 查看集群状态</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:01:28.818Z&quot;),
        &quot;myState&quot; : 2,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 0,
                        &quot;state&quot; : 8,
                        &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;,
                        &quot;uptime&quot; : 0,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:01:27.006Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:00:50.935Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 172.30.2.201:27017; couldn&apos;t connect to server 172.30.2.201:27017 (172.30.2.201), connection attempt failed&quot;,
                        &quot;configVersion&quot; : -1
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 87,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:01:26.963Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:01:27.078Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;electionTime&quot; : Timestamp(1443423653, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:00:53Z&quot;),
                        &quot;configVersion&quot; : 3
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 90,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>发现集群进行了自动切换,把 172.30.2.202:27017 变为了 primary</p>
<p>3) 启动原来的 primary</p>
<pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 25738
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:02:24.312Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 13,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:02:23.189Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:02:22.873Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 3
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 185,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443423653, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:00:53Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 143,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:02:23.103Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:02:22.990Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 3
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>发现原来的 primary 自动切为了 secondary</p>
<h3 id="2-secondary-节点故障"><a href="#2-secondary-节点故障" class="headerlink" title="(2) secondary 节点故障"></a>(2) secondary 节点故障</h3><p>1) 关闭 secondary 节点</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; use admin;
switched to db admin
rs0:SECONDARY&gt; db.shutdownServer();
2015-09-28T15:04:39.064+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-28T15:04:39.066+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:04:39.067+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-28T15:04:39.067+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
2015-09-28T15:04:39.070+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:04:39.070+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-28T15:04:39.070+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
&gt;
bye
</code></pre><p>2) 查看集群状态</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:05:12.140Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 180,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:05:11.265Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:05:10.951Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 3
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 353,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443423653, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:00:53Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 0,
                        &quot;state&quot; : 8,
                        &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;,
                        &quot;uptime&quot; : 0,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:05:11.226Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:04:37.055Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 172.30.2.202:27017; couldn&apos;t connect to server 172.30.2.202:27017 (172.30.2.202), connection attempt failed&quot;,
                        &quot;configVersion&quot; : -1
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>可见单个 secondary 节点故障对集群没有影响</p>
<p>3) 再启动 secondary</p>
<pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 49507
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:06:41.733Z&quot;),
        &quot;myState&quot; : 2,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 12,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:06:40.999Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:06:41.233Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;could not find member to sync from&quot;,
                        &quot;configVersion&quot; : 3
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 12,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:06:40.999Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:06:41.360Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;electionTime&quot; : Timestamp(1443423653, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:00:53Z&quot;),
                        &quot;configVersion&quot; : 3
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 13,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>重新启动后又重新连上了集群</p>
<h2 id="3-多节点故障"><a href="#3-多节点故障" class="headerlink" title="3. 多节点故障"></a>3. 多节点故障</h2><h3 id="1-primary-和-secondary-节点同时故障"><a href="#1-primary-和-secondary-节点同时故障" class="headerlink" title="(1) primary 和 secondary 节点同时故障"></a>(1) primary 和 secondary 节点同时故障</h3><p>1) 停掉一个 secondary 节点</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; use admin;
switched to db admin
rs0:SECONDARY&gt; db.shutdownServer();
2015-09-28T15:10:43.049+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-28T15:10:43.051+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:10:43.052+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-28T15:10:43.052+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
2015-09-28T15:10:43.055+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:10:43.055+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-28T15:10:43.055+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
</code></pre><p>2) 停掉 primary 节点</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:PRIMARY&gt; use admin;
switched to db admin
rs0:PRIMARY&gt; db.shutdownServer();
2015-09-28T15:10:53.069+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-28T15:10:53.072+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:10:53.073+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) ok
2015-09-28T15:10:53.073+0800 I NETWORK  DBClientCursor::init call() failed
2015-09-28T15:10:53.076+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:10:53.076+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) ok
2015-09-28T15:10:53.888+0800 I NETWORK  Socket recv() errno:104 Connection reset by peer 127.0.0.1:27017
2015-09-28T15:10:53.888+0800 I NETWORK  SocketException: remote: 127.0.0.1:27017 error: 9001 socket exception [RECV_ERROR] server [127.0.0.1:27017]
2015-09-28T15:10:53.888+0800 I NETWORK  DBClientCursor::init call() failed
</code></pre><p>3) 查看集群状态</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:12:10.946Z&quot;),
        &quot;myState&quot; : 2,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 600,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 0,
                        &quot;state&quot; : 8,
                        &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;,
                        &quot;uptime&quot; : 0,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:12:10.008Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:10:51.422Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 172.30.2.203:27017; couldn&apos;t connect to server 172.30.2.203:27017 (172.30.2.203), connection attempt failed&quot;,
                        &quot;configVersion&quot; : -1
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 0,
                        &quot;state&quot; : 8,
                        &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;,
                        &quot;uptime&quot; : 0,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:12:09.477Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:10:41.112Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 172.30.2.202:27017; couldn&apos;t connect to server 172.30.2.202:27017 (172.30.2.202), connection attempt failed&quot;,
                        &quot;configVersion&quot; : -1
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>只剩下一个 secondary 节点,集群变得不可用了</p>
<p>4) 解决方案</p>
<p>重新配置:</p>
<pre><code>rs0:SECONDARY&gt; cfg={_id:&quot;rs0&quot;, members:[ {_id:0,host:&quot;172.30.2.201:27017&quot;}] }
{
        &quot;_id&quot; : &quot;rs0&quot;,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;host&quot; : &quot;172.30.2.201:27017&quot;
                }
        ]
}
rs0:SECONDARY&gt; rs.reconfig(cfg, {force:true});
{ &quot;ok&quot; : 1 }
rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:14:09.350Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 719,
                        &quot;optime&quot; : Timestamp(1443423600, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:00:00Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443424428, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:13:48Z&quot;),
                        &quot;configVersion&quot; : 71840,
                        &quot;self&quot; : true
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>此时就变成了单 primary 节点,可以提供读写服务,然后再制作 secondary 节点</p>
<h3 id="2-两个-secondary-节点故障"><a href="#2-两个-secondary-节点故障" class="headerlink" title="(2) 两个 secondary 节点故障"></a>(2) 两个 secondary 节点故障</h3><p>1) 故障前状态</p>
<pre><code>rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:16:06.571Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 836,
                        &quot;optime&quot; : Timestamp(1443424538, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:15:38Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443424534, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:15:34Z&quot;),
                        &quot;configVersion&quot; : 71842,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 31,
                        &quot;optime&quot; : Timestamp(1443424538, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:15:38Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:16:06.230Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:16:06.089Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 71842
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 26,
                        &quot;optime&quot; : Timestamp(1443424538, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:15:38Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:16:06.229Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:16:06.233Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;configVersion&quot; : 71842
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>2) 停掉两个 secondary 节点</p>
<p>在两个 secondary 节点分别执行:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
rs0:SECONDARY&gt; use admin;
switched to db admin
rs0:SECONDARY&gt; db.shutdownServer();
2015-09-28T15:18:11.114+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-28T15:18:11.117+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:18:11.118+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-28T15:18:11.118+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
2015-09-28T15:18:11.121+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-28T15:18:11.121+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-28T15:18:11.121+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed
&gt;
bye
</code></pre><p>3) 查看集群状态</p>
<pre><code>rs0:SECONDARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:19:09.196Z&quot;),
        &quot;myState&quot; : 2,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 1019,
                        &quot;optime&quot; : Timestamp(1443424538, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:15:38Z&quot;),
                        &quot;configVersion&quot; : 71842,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;172.30.2.202:27017&quot;,
                        &quot;health&quot; : 0,
                        &quot;state&quot; : 8,
                        &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;,
                        &quot;uptime&quot; : 0,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:19:08.371Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:18:10.147Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 172.30.2.202:27017; couldn&apos;t connect to server 172.30.2.202:27017 (172.30.2.202), connection attempt failed&quot;,
                        &quot;configVersion&quot; : -1
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;172.30.2.203:27017&quot;,
                        &quot;health&quot; : 0,
                        &quot;state&quot; : 8,
                        &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;,
                        &quot;uptime&quot; : 0,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-09-28T07:19:08.350Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-09-28T07:18:34.298Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 172.30.2.203:27017; couldn&apos;t connect to server 172.30.2.203:27017 (172.30.2.203), connection attempt failed&quot;,
                        &quot;configVersion&quot; : -1
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>可见剩下的 primary 节点自动变为了 secondary 节点,集群变得不可用了</p>
<p>4) 解决方案</p>
<pre><code>rs0:SECONDARY&gt; cfg={_id:&quot;rs0&quot;, members:[ {_id:0,host:&quot;172.30.2.201:27017&quot;}] }
{
        &quot;_id&quot; : &quot;rs0&quot;,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;host&quot; : &quot;172.30.2.201:27017&quot;
                }
        ]
}
rs0:SECONDARY&gt; rs.reconfig(cfg, {force:true});
{ &quot;ok&quot; : 1 }
rs0:PRIMARY&gt; rs.status();
{
        &quot;set&quot; : &quot;rs0&quot;,
        &quot;date&quot; : ISODate(&quot;2015-09-28T07:20:08.099Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;172.30.2.201:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 1078,
                        &quot;optime&quot; : Timestamp(1443424538, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-09-28T07:15:38Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1443424795, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-09-28T07:19:55Z&quot;),
                        &quot;configVersion&quot; : 127342,
                        &quot;self&quot; : true
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre><p>处理方法和上面的相同,也是强制将剩下的 secondary 节点配置为单 primary 节点</p>
<h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h2><h3 id="1-三节点的副本集集群-任何一台故障-集群都会进行自动切换-不影响服务"><a href="#1-三节点的副本集集群-任何一台故障-集群都会进行自动切换-不影响服务" class="headerlink" title="(1) 三节点的副本集集群,任何一台故障,集群都会进行自动切换,不影响服务"></a>(1) 三节点的副本集集群,任何一台故障,集群都会进行自动切换,不影响服务</h3><h3 id="2-三节点的副本集集群-故障任何两个节点-集群就会变的不可用-需要手动处理"><a href="#2-三节点的副本集集群-故障任何两个节点-集群就会变的不可用-需要手动处理" class="headerlink" title="(2) 三节点的副本集集群,故障任何两个节点,集群就会变的不可用,需要手动处理"></a>(2) 三节点的副本集集群,故障任何两个节点,集群就会变的不可用,需要手动处理</h3>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[C语言类型转换]]></title>
      <url>http://panwenhang.github.io/2016/06/23/C/2016-06-23-C%E8%AF%AD%E8%A8%80%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/</url>
      <content type="html"><![CDATA[<h2 id="1-Integer-Promotion"><a href="#1-Integer-Promotion" class="headerlink" title="1. Integer Promotion"></a>1. Integer Promotion</h2><p>  在一个表达式中,凡是可以使用 int 或 unsigned int 类型做右值的地方也都可以使用有符号或无符号的 char 型、short 型和 Bit-field 。如果原始类型的取值范围都能用 int 型表示,则其值被提升为 int 型,如果表示不了就提升为 unsigned int 型,这称为 Integer Promotion 。做 IntegerPromotion 只影响上述几种类型的值,对其它类型无影响。C99 规定 Integer Promotion 适用于以</p>
<a id="more"></a>
<p>下几种情况:</p>
<p>(1) 如果一个函数的形参类型未知,例如使用了 Old Style C 风格的函数声明,或者函数的参数列表中有…,那么调用函数时要对相应的实参做 Integer Promotion ,此外,相应的实参如果是 float 型的也要被提升为 double 型,这条规则称为 Default Argument Promotion 。我们知道 printf 的参数列表中有…,除了第一个形参之外,其它形参的类型都是未知的,因此我们在调用 printf(“%c”,’A’) 时,’A’ 其实被提升为 int 型之后才传给了 printf 。</p>
<p>(2) 算术运算中的类型转换。两个算术类型的操作数做算术运算,比如 a * b ,如果两边操作数的类型不同,编译器会自动做类型换,使两边类型相同之后才做运算,这称为 Usual Arithmetic Conversion ,转换过程中有一步就是 Integer Promotion ,我们先举个例子来理解这一步,至于 Usual Arithmetic Conversion 的完整规则将在下面详细解释。</p>
<pre><code>unsigned char c1 = 255, c2 = 2;
int n = c1 + c2;
</code></pre><p>计算表达式 c1 + c2 的过程其实是先把 c1 和 c2 提升为int类型然后相加 ( unsigned char 的取值范围是 0 ~ 255 ,完全可以用 int 表示,所以不需要提升为 unsigned int ) ,整个表达式的值也是 int 型,最后的结果是 257 。假如没有这个提升的过程,c1 + c2 就溢出了,最后的结果应该是 1 。</p>
<p>显然,+-*/% 这些算术运算以及 &gt; &lt; &gt;= &lt;= == != 这些比较运算都需要做 Usual Arithmetic Conversion ,因为都要求两边操作数的类型一致,此外还有哪些运算也需要做 Usual Arithmetic Conversion 呢？我们将在下一章做个总结。</p>
<p>(3) 单目运算符 ++、–、~ 只有一个操作数,移位运算符 &lt;&lt; 、&gt;&gt; 两边的操作数类型不要求一致,因此这些运算不需要做 Usual Arithmetic Conversion ,但也需要做 Integer Promotion 。</p>
<h2 id="2-Usual-Arithmetic-Conversion"><a href="#2-Usual-Arithmetic-Conversion" class="headerlink" title="2. Usual Arithmetic Conversion"></a>2. Usual Arithmetic Conversion</h2><p>  现在详细解释一下 Usual Arithmetic Conversion 的规则:</p>
<p>(1) 如果有一边的类型是 long double ,则把另一边也转成 long double 。</p>
<p>(2) 否则,如果有一边的类型是 double ,则把另一边也转成 double 。</p>
<p>(3) 否则,如果有一边的类型是 float ,则把另一边也转成 float 。</p>
<p>(4) 否则,两边应该都是整数类型,首先按上一小节讲过的规则对 a 和 b 做 Integer Promotion ,然后如果类型仍不相同,则需要继续转换。首先规定 char、short、int、long、long long 的转换级别 ( Integer Conversion Rank ) 一个比一个高,同一类型的有符号和无符号数具有相同的 Rank ,然后有如下转换规则:</p>
<p>a. 如果两边都是有符号数,或者都是无符号数,那么较低 Rank 的类型转换成较高 Rank 的类型。例如 unsigned int 和 unsigned long 做算术运算时都转成 unsigned long 。</p>
<p>b. 否则,如果一边是无符号数另一边是有符号数,无符号数的 Rank 不低于有符号数的 Rank ,则把有符号数转成另一边的无符号类型。例如 unsigned long 和 int 做算术运算时都转成 unsigned long ,unsigned long 和 long 做算术运算时也都转成 unsigned long 。</p>
<p>c. 剩下的情况就是:一边是无符号数另一边是有符号数,并且无符号数的 Rank 低于有符号数的 Rank 。这时又分为两种情况,如果这个有符号数类型能够覆盖这个无符号数类型的取值范围,则把无符号数转成另一边的有符号类型。例如遵循 LP64 的平台上 unsigned int 和 long 在做算术运算时都转成 long 。</p>
<p>d. 否则,也就是这个符号数类型不足以覆盖这个无符号数类型的取值范围,则把两边都转成两者之中较高 Rank 的无符号类型。例如遵循 ILP32 的平台上 unsigned int 和 long 在做算术运算时都转成 unsigned long 。可见有符号和无符号整数的转换规则是十分复杂的,虽然这是有明确定义的,不属于阴暗角落,但为了程序的可读性,不应该依赖这些规则来写代码。我讲这些规则,不是为了让你用的,而是为了让你在出错时更容易分析错误原因,所以这些规则不需要记住,但要知道有这么回事,以便用到的时候能找到这一段。</p>
<h2 id="3-由赋值产生的类型转换"><a href="#3-由赋值产生的类型转换" class="headerlink" title="3. 由赋值产生的类型转换"></a>3. 由赋值产生的类型转换</h2><p>  如果赋值或初始化时等号两边的类型不相同,则编译器会把等号右边的类型转换成等号左边的类型再做赋值。例如 int c = 3.14 ;,编译器会把右边的 double 型转成 int 型再赋给变量 c 。我们知道,函数调用传参的过程相当于定义形参并且用实参对其做初始化,函数返回的过程相当于定义一个临时变量并且用 return 的表达式对其做初始化,所以由赋值产生的类型转换也适用于这两种情况。例如一个函数的原型是 int foo( int, int ) ;,则调用 foo( 3.1, 4.2 )时会自动把两个 double 型的实参转成 int 型赋给形参,如果这个函数定义中有返回语句 return 1.2 ;,则返回值 1.2 会自动转成 int 型再返回。在函数调用和返回过程中发生的类型转换往往容易被忽视,因为函数原型和函数调用并没有写在一起。例如 char c = getchar() ; ,看到这一句,往往想当然地认为 getchar 的返回值是 char 型的,而事实上 getchar 的返回值是 int 型的,这样赋值会引起一个类型转换,我们以后会详细解释使用这个函数需要注意的问题。</p>
<h2 id="4-强制类型转换"><a href="#4-强制类型转换" class="headerlink" title="4. 强制类型转换"></a>4. 强制类型转换</h2><p>  以上三种情况通称为隐式类型转换 ( Implicit Conversion ,或者叫 Coercion ) ,编译器根据它自己的一套规则将一种类型自动转换为另一种类型。除此之外,程序员也可以通过类型转换运算符 ( Cast Operator ) 自己规定某个值要转换成何种类型,这称为显式类型转换 ( Explicit Conversion ) 或强制类型转换 ( Type Cast ) 。例如计算表达式 ( double ) 3 + i ,首先将整数 3 强制转换成 double 型 3.0 ,然后和整型变量 i 相加,这时适用 Usual Arithmetic Conversion 规则,首先把 i 也转成 double 型,然后两者相加,最后整个表达式的值也是 double 型的。这里的 ( double ) 就是一个类型转换运算符,这种运算符由一个类型名加 () 括号组成,后面的3是这个运算符的操作数。</p>
<h2 id="5-编译器如何处理类型转换"><a href="#5-编译器如何处理类型转换" class="headerlink" title="5. 编译器如何处理类型转换"></a>5. 编译器如何处理类型转换</h2><p>  以上几小节介绍了哪些情况会产生类型转换,并且明确了每种情况下应该把什么类型转成什么类型,至于这两种类型之间的转换具体怎么做则是本节的内容。本节的主要内容出自 [ Standard C ] 。在两种类型之间做转换,转换结果将取决于两种类型的精度。那么类型的精度怎么定义呢？我们分三种情况讨论:</p>
<p>(1) 精度是 N 的有符号整数类型应该用 N 个 Bit 表示,取值范围至少应该覆盖 ( -2N-1, 2N-1 ) 。我们用 () 括号表示开区间,不包含端点,用 [] 括号表示闭区间,包含端点。例如 signed char 型用 8 个 Bit 表示,按 2’s Complement 表示法的取值范围是 [ -128, 127 ] ,也可以说是覆盖了 ( -128, 128 ) ,所以这种类型的精度是 8 。</p>
<p>(2) 精度是 N 的无符号整数类型应该用 N 个 Bit 表示,取值范围是 [ 0, 2N-1 ] 。</p>
<p>(3) 精度是 N 的浮点数类型的取值范围至少应该覆盖 ( -2N-1, 2N-1 ) 的整数值。现在要把一个精度是M的类型 ( 值为 X ) 转换成一个精度是 N 的类型,所有可能的情况如下表所示。</p>
<p>表 1. 如何做类型转换</p>
<table>
<thead>
<tr>
<th style="text-align:center">待转换的类型</th>
<th style="text-align:center">N &lt; M的情况</th>
<th style="text-align:center">N == M的情况</th>
<th style="text-align:center">N &gt; M的情况</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">signed integer to signed integer</td>
<td style="text-align:center">discard m.s. M-N bits (can overflow)</td>
<td style="text-align:center">same value</td>
<td style="text-align:center">same value</td>
</tr>
<tr>
<td style="text-align:center">unsigned integer to signed integer</td>
<td style="text-align:center">if (X &lt; 2N-1) same value else impl.-def. (can overflow)</td>
<td style="text-align:center">if (X &lt; 2N-1) same value else impl.-def. (can overflow)</td>
<td style="text-align:center">same value</td>
</tr>
<tr>
<td style="text-align:center">floating-point to signed integer</td>
<td style="text-align:center">if (\</td>
<td style="text-align:center">X\</td>
<td style="text-align:center">&lt; 2N-1) trunc(X) else imple.-def. (can overflow)</td>
<td>if (\</td>
<td>X\</td>
<td>&lt; 2N-1) trunc(X) else imple.-def. (can overflow)</td>
<td>if (\</td>
<td>X\</td>
<td>&lt; 2N-1) trunc(X) else imple.-def. (can overflow)</td>
</tr>
<tr>
<td style="text-align:center">signed integer to unsigned integer</td>
<td style="text-align:center">if (0 &lt;= X) X % 2N else impl.-def.</td>
<td style="text-align:center">if (0 &lt;= X) same value else X + 2N</td>
<td style="text-align:center">if (0 &lt;= X) same value else X + 2N</td>
</tr>
<tr>
<td style="text-align:center">unsigned integer to unsigned integer</td>
<td style="text-align:center">X % 2N</td>
<td style="text-align:center">same value</td>
<td style="text-align:center">same value</td>
</tr>
<tr>
<td style="text-align:center">floating-point to unsigned integer</td>
<td style="text-align:center">if (0 &lt;= X &lt; 2N) trunc(X) else imple.-def. (can overflow)</td>
<td style="text-align:center">if (0 &lt;= X &lt; 2N) trunc(X) else imple.-def. (can overflow)</td>
<td style="text-align:center">if (0 &lt;= X &lt; 2N) trunc(X) else imple.-def. (can overflow)</td>
</tr>
<tr>
<td style="text-align:center">signed integer to floating-point</td>
<td style="text-align:center">keep sign, keep m.s. N-1 bits</td>
<td style="text-align:center">same value</td>
<td style="text-align:center">same value</td>
</tr>
<tr>
<td style="text-align:center">unsigned integer to floating-point</td>
<td style="text-align:center">+ sign, keep m.s. N-1 bits</td>
<td style="text-align:center">+ sign, keep m.s. N-1 bits</td>
<td style="text-align:center">same value</td>
</tr>
<tr>
<td style="text-align:center">floating-point to floating-point</td>
<td style="text-align:center">keep m.s. N-1 bits (can overflow)</td>
<td style="text-align:center">same value</td>
<td style="text-align:center">same value</td>
</tr>
</tbody>
</table>
<p>上表中的一些缩写说明如下:impl.-def.表示 Implementation-defined ；m.s. bit 表示 MostSignificant Bit ；trunc(X) 表示取 X 的整数部分,即 Truncate Toward Zero ；X % Y 就是取模,上表中用到取模运算时 X 和 Y 都是正整数。同样地,这个表不是为了让你故意去用的,而是为了让你出错时分析错误原因的。下面举几个例子说明这个表的用法。比如要把 float 型转 short 型,对应表中的 floating-point to signed integer 一行,可以看到,不管两种类型的精度如何,处理方式是一样的,如果 float 类型的值在 ( -32768.0, 32768.0 ) 之间,则截掉小数部分就可以了,如果 float 类型的值超出了这个范围,则转换结果是未明确定义的,有可能会产生溢出,例如对于 short s = 32768.4 ; 这个语句 gcc 会报警告。再比如把 int 类型转换成 unsigned short 类型,对应表中的 unsigned integer to unsigned integer 一行,如果 int 类型的值是正的,则把它除以 216 取模,其实就是取它的低 16 位,如果 int 类型的值是负的,则转换结果是未明确定义的。再比如把 int 类型转换成 short 类型,对应表中的第一行 signed integer to signed integer ,把 int 类型值的高 16 位丢掉 ( 这里的 m.s.包括符号位在内,上表中另外几处提到的 m.s. 应该是不算符号位在内 ),只留低 16 位,这种情况也有可能溢出,例如对于 short s = -32769 ; 这个语句 gcc 会报警告,而对于 short s = -32768 ; 则不会报警告。最后一个例子,把 short 型转换成 int 型,仍然对应表中第一行,转换之后应该是 same value 。那怎么维持值不变呢？ 是不是在高位补 16 个 0 就行了呢？ 如果 short 型的值是 -1 ,按补码表示就是十六进制 ffff ,要转成 int 型的 -1 需要变成 ffffffff ,因此需要在高位补 16 个 1 而不是 16 个 0。换句话说,要维持值不变,在高位补 1 还是补 0 取决于原来的符号位,这称为符号扩展 ( Sign Extension ) 。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux下MongoDB安装和配置]]></title>
      <url>http://panwenhang.github.io/2016/06/23/MongoDB/2016-06-23-Linux%E4%B8%8B2016-06-23-MongoDB%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-201 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
</code></pre><a id="more"></a>
<h2 id="2-准备"><a href="#2-准备" class="headerlink" title="2. 准备"></a>2. 准备</h2><h3 id="1-下载包"><a href="#1-下载包" class="headerlink" title="(1) 下载包"></a>(1) 下载包</h3><pre><code>$ sudo wget -c -P /opt/ https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.0.6.tgz
--2015-09-23 10:18:24--  https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.0.6.tgz
Resolving fastdl.mongodb.org (fastdl.mongodb.org)... 54.230.108.68, 54.230.108.110, 54.230.108.116, ...
Connecting to fastdl.mongodb.org (fastdl.mongodb.org)|54.230.108.68|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 50210575 (48M) [application/x-gzip]
Saving to: ‘/opt/mongodb-linux-x86_64-rhel70-3.0.6.tgz’
100%[======================================================================================================================================================================================================================&gt;] 50,210,575  1.20MB/s   in 50s
2015-09-23 10:19:19 (978 KB/s) - ‘/opt/mongodb-linux-x86_64-rhel70-3.0.6.tgz’ saved [50210575/50210575]
</code></pre><h3 id="2-环境"><a href="#2-环境" class="headerlink" title="(2) 环境"></a>(2) 环境</h3><pre><code>$ cd /opt/
$ sudo tar -zxf mongodb-linux-x86_64-rhel70-3.0.6.tgz
$ ll
total 49052
drwxr-xr-x. 3 root root     4096 Sep 23 10:20 mongodb-linux-x86_64-rhel70-3.0.6
-rw-r--r--. 1 root root 50210575 Aug 24 11:38 mongodb-linux-x86_64-rhel70-3.0.6.tgz
drwxr-xr-x. 8 root root     4096 Sep 21 15:41 omnipitr
drwxr-xr-x. 6 root root     4096 Sep 21 15:53 pg94
lrwxrwxrwx. 1 root root        9 Sep 21 15:53 pgsql -&gt; /opt/pg94
drwxr-xr-x. 2 root root     4096 Jun 10  2014 rh
$ sudo ln -sf /opt/mongodb-linux-x86_64-rhel70-3.0.6 /opt/mongodb
$ ll
total 49052
lrwxrwxrwx. 1 root root       38 Sep 23 10:21 mongodb -&gt; /opt/mongodb-linux-x86_64-rhel70-3.0.6
drwxr-xr-x. 3 root root     4096 Sep 23 10:20 mongodb-linux-x86_64-rhel70-3.0.6
-rw-r--r--. 1 root root 50210575 Aug 24 11:38 mongodb-linux-x86_64-rhel70-3.0.6.tgz
drwxr-xr-x. 8 root root     4096 Sep 21 15:41 omnipitr
drwxr-xr-x. 6 root root     4096 Sep 21 15:53 pg94
lrwxrwxrwx. 1 root root        9 Sep 21 15:53 pgsql -&gt; /opt/pg94
drwxr-xr-x. 2 root root     4096 Jun 10  2014 rh
$
$ sudo vim /etc/profile.d/mongodb.sh
#!/bin/env bash
export PATH=&quot;/opt/mongodb/bin:$PATH&quot;
$ source /etc/profile.d/mongodb.sh
$ mongo --version
MongoDB shell version: 3.0.6
</code></pre><h2 id="3-配置"><a href="#3-配置" class="headerlink" title="3. 配置"></a>3. 配置</h2><pre><code>$ sudo mkdir -p /data/mongodb/db0/{data,backup,log,conf}
$ sudo vim /data/mongodb/db0/conf/mongodb.conf
# base
port = 27017
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/db0/data
pidfilepath = /data/mongodb/db0/data/mongodb.pid
journal = true

# security
nohttpinterface = true
rest = false

# log
logpath = /data/mongodb/db0/log/mongodb.log
logRotate = rename
logappend = true
slowms = 50
profile = 0
verbose = false
objcheck = false
</code></pre><h2 id="4-启动"><a href="#4-启动" class="headerlink" title="4. 启动"></a>4. 启动</h2><pre><code>$ sudo systemctl stop firewalld.service
$ sudo systemctl disable firewalld.service
rm &apos;/etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service&apos;
rm &apos;/etc/systemd/system/basic.target.wants/firewalld.service&apos;
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; db.version();
3.0.6
&gt;
bye
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[MongoDB主从集群]]></title>
      <url>http://panwenhang.github.io/2016/06/23/MongoDB/2016-06-23-MongoDB%E4%B8%BB%E4%BB%8E%E9%9B%86%E7%BE%A4/</url>
      <content type="html"><![CDATA[<h2 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h2><pre><code>$ cat /etc/redhat-release
CentOS Linux release 7.0.1406 (Core)
$ uname -a
Linux zhaopin-2-201 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
$ mongo --version
MongoDB shell version: 3.0.6
</code></pre><a id="more"></a>
<h2 id="2-准备"><a href="#2-准备" class="headerlink" title="2. 准备"></a>2. 准备</h2><h3 id="1-创建目录"><a href="#1-创建目录" class="headerlink" title="(1) 创建目录"></a>(1) 创建目录</h3><pre><code>$ sudo mkdir -p /data/mongodb/db0/{data,backup,log,conf}
</code></pre><h3 id="2-编写配置文件"><a href="#2-编写配置文件" class="headerlink" title="(2) 编写配置文件"></a>(2) 编写配置文件</h3><p>1) master:</p>
<pre><code>$ sudo vim /data/mongodb/db0/conf/mongodb.conf
# base
port = 27017
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/db0/data
pidfilepath = /data/mongodb/db0/data/mongodb.pid
journal = true

# security
nohttpinterface = true
rest = false

# log
logpath = /data/mongodb/db0/log/monodb.log
logRotate = rename
logappend = true
slowms = 50
master = true
</code></pre><p>2) slave:</p>
<pre><code>$ sudo vim /data/mongodb/db0/conf/mongodb.conf
# base
port = 27017
maxConns = 800
filePermissions = 0700
fork = true
noauth = true
directoryperdb = true
dbpath = /data/mongodb/db0/data
pidfilepath = /data/mongodb/db0/data/mongodb.pid
journal = true

# security
nohttpinterface = true
rest = false

# log
logpath = /data/mongodb/db0/log/mongodb.log
logRotate = rename
logappend = true
slowms = 50

slave = true
source = 172.30.2.201:27017
</code></pre><h2 id="3-启动"><a href="#3-启动" class="headerlink" title="3. 启动"></a>3. 启动</h2><h3 id="1-先启动-master"><a href="#1-先启动-master" class="headerlink" title="(1) 先启动 master"></a>(1) 先启动 master</h3><pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 48583
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; db.isMaster();
{
        &quot;ismaster&quot; : 0,
        &quot;info&quot; : &quot;dead: data too stale halted replication&quot;,
        &quot;maxBsonObjectSize&quot; : 16777216,
        &quot;maxMessageSizeBytes&quot; : 48000000,
        &quot;maxWriteBatchSize&quot; : 1000,
        &quot;localTime&quot; : ISODate(&quot;2015-09-23T07:47:52.957Z&quot;),
        &quot;maxWireVersion&quot; : 3,
        &quot;minWireVersion&quot; : 0,
        &quot;ok&quot; : 1
}
&gt;
bye
</code></pre><h3 id="2-启动-slave"><a href="#2-启动-slave" class="headerlink" title="(2) 启动 slave"></a>(2) 启动 slave</h3><pre><code>$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 36715
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; db.isMaster();
{
        &quot;ismaster&quot; : false,
        &quot;maxBsonObjectSize&quot; : 16777216,
        &quot;maxMessageSizeBytes&quot; : 48000000,
        &quot;maxWriteBatchSize&quot; : 1000,
        &quot;localTime&quot; : ISODate(&quot;2015-09-23T07:49:32.934Z&quot;),
        &quot;maxWireVersion&quot; : 3,
        &quot;minWireVersion&quot; : 0,
        &quot;ok&quot; : 1
}
&gt;
bye
</code></pre><h3 id="3-设置开机启动"><a href="#3-设置开机启动" class="headerlink" title="(3) 设置开机启动"></a>(3) 设置开机启动</h3><pre><code>$ sudo vim /etc/rc.local
/opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
</code></pre><h2 id="4-验证"><a href="#4-验证" class="headerlink" title="4. 验证"></a>4. 验证</h2><h3 id="1-数据同步"><a href="#1-数据同步" class="headerlink" title="(1) 数据同步"></a>(1) 数据同步</h3><p>1) master:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; use aaa;
switched to db aaa
&gt; db.createCollection(&quot;test&quot;);
{ &quot;ok&quot; : 1 }
&gt; show collections;
system.indexes
system.profile
test
</code></pre><p>2) slave:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; rs.slaveOk();
&gt; show dbs;
aaa    0.078GB
local  0.078GB
&gt; use aaa;
switched to db aaa
&gt; show collections;
system.indexes
test
</code></pre><h3 id="2-读写验证"><a href="#2-读写验证" class="headerlink" title="(2) 读写验证"></a>(2) 读写验证</h3><p>由上可知,master 是可读写的</p>
<p>在从库上写测试:</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; use aaa;
switched to db aaa
&gt; db.createCollection(&quot;test2&quot;);
{ &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; }
&gt; show collections;
2015-09-23T15:57:49.240+0800 E QUERY    Error: listCollections failed: { &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; }
    at Error (&lt;anonymous&gt;)
    at DB._getCollectionInfosCommand (src/mongo/shell/db.js:646:15)
    at DB.getCollectionInfos (src/mongo/shell/db.js:658:20)
    at DB.getCollectionNames (src/mongo/shell/db.js:669:17)
    at shellHelper.show (src/mongo/shell/utils.js:625:12)
    at shellHelper (src/mongo/shell/utils.js:524:36)
    at (shellhelp2):1:1 at src/mongo/shell/db.js:646
&gt; rs.slaveOk();
&gt; show collections;
system.indexes
test
</code></pre><p>证明从库是不能写,默认也是不可读的,需要执行 rs.slaveOk(); 或者 db.getMongo().setSlaveOk(); 并且只对当前 session 有效,所以每次连接从库都需要执行。</p>
<h2 id="5-故障测试及解决方案"><a href="#5-故障测试及解决方案" class="headerlink" title="5.故障测试及解决方案"></a>5.故障测试及解决方案</h2><h3 id="1-master-宕机"><a href="#1-master-宕机" class="headerlink" title="(1) master 宕机"></a>(1) master 宕机</h3><p>1) 将 master 停机</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; use admin;
switched to db admin
&gt; db.shutdownServer();
2015-09-23T16:02:16.983+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-23T16:02:16.986+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-23T16:02:16.987+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) ok
2015-09-23T16:02:16.992+0800 I NETWORK  Socket recv() errno:104 Connection reset by peer 127.0.0.1:27017
2015-09-23T16:02:16.992+0800 I NETWORK  SocketException: remote: 127.0.0.1:27017 error: 9001 socket exception [RECV_ERROR] server [127.0.0.1:27017]
2015-09-23T16:02:16.992+0800 I NETWORK  DBClientCursor::init call() failed
&gt;
bye
$ ps aux | grep mongo | grep -v grep
</code></pre><p>2) 查看 slave 状态</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; rs.slaveOk();
&gt; show dbs;
aaa    0.078GB
local  0.078GB
&gt; use aaa;
switched to db aaa
&gt; show collections;
system.indexes
test
&gt; db.createCollection(&quot;test2&quot;);
{ &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; }
</code></pre><p>可见 slave 还是只读,不能写</p>
<p>3) 解决方法</p>
<p>将 slave 停机,变为 master 后启动</p>
<pre><code>&gt; use admin;
switched to db admin
&gt; db.shutdownServer();
2015-09-23T16:05:28.355+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-23T16:05:28.357+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-23T16:05:28.357+0800 W NETWORK  Failed to connect to 127.0.0.1:27017, reason: errno:111 Connection refused
2015-09-23T16:05:28.358+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) failed failed couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt  failed
&gt;
bye
$ ps aux | grep mongo | grep -v grep
$ sudo vim /data/mongodb/db0/conf/mongodb.conf
master = true
#slave = true
#source = 172.30.2.201:27017
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 36988
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; show dbs;
aaa     0.078GB
local  50.054GB
&gt; use aaa;
switched to db aaa
&gt; db.createCollection(&quot;test2&quot;);
{ &quot;ok&quot; : 1 }
&gt; show collections;
system.indexes
system.profile
test
test2
&gt; db.isMaster();
{
        &quot;ismaster&quot; : true,
        &quot;maxBsonObjectSize&quot; : 16777216,
        &quot;maxMessageSizeBytes&quot; : 48000000,
        &quot;maxWriteBatchSize&quot; : 1000,
        &quot;localTime&quot; : ISODate(&quot;2015-09-23T08:07:41.812Z&quot;),
        &quot;maxWireVersion&quot; : 3,
        &quot;minWireVersion&quot; : 0,
        &quot;ok&quot; : 1
}
&gt;
bye
</code></pre><p>这样 slave 就变为新的 master 了</p>
<h3 id="2-slave-宕机"><a href="#2-slave-宕机" class="headerlink" title="(2) slave 宕机"></a>(2) slave 宕机</h3><p>1) 制作新的 slave</p>
<pre><code>$ sudo rm -fr /data/mongodb/db0/data/*
$ sudo vim /data/mongodb/db0/conf/mongodb.conf
#master = true
slave = true
source = 172.30.2.202:27017
$ sudo /opt/mongodb/bin/mongod --config /data/mongodb/db0/conf/mongodb.conf
about to fork child process, waiting until server is ready for connections.
forked process: 48779
child process started successfully, parent exiting
$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; db.isMaster();
{
        &quot;ismaster&quot; : false,
        &quot;maxBsonObjectSize&quot; : 16777216,
        &quot;maxMessageSizeBytes&quot; : 48000000,
        &quot;maxWriteBatchSize&quot; : 1000,
        &quot;localTime&quot; : ISODate(&quot;2015-09-23T08:14:51.283Z&quot;),
        &quot;maxWireVersion&quot; : 3,
        &quot;minWireVersion&quot; : 0,
        &quot;ok&quot; : 1
}
</code></pre><p>2) 将新的 slave 停机</p>
<pre><code>&gt; use admin;
switched to db admin
&gt; db.shutdownServer();
2015-09-23T16:15:31.329+0800 I NETWORK  DBClientCursor::init call() failed
server should be down...
2015-09-23T16:15:31.331+0800 I NETWORK  trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2015-09-23T16:15:31.332+0800 I NETWORK  reconnect 127.0.0.1:27017 (127.0.0.1) ok
2015-09-23T16:15:31.333+0800 I NETWORK  Socket recv() errno:104 Connection reset by peer 127.0.0.1:27017
2015-09-23T16:15:31.333+0800 I NETWORK  SocketException: remote: 127.0.0.1:27017 error: 9001 socket exception [RECV_ERROR] server [127.0.0.1:27017]
2015-09-23T16:15:31.333+0800 I NETWORK  DBClientCursor::init call() failed
&gt;
bye
$ ps aux | grep mongo | grep -v grep
</code></pre><p>3) 查看新 master 状况</p>
<pre><code>$ mongo
MongoDB shell version: 3.0.6
connecting to: test
&gt; show dbs;
aaa     0.078GB
local  50.054GB
&gt; use aaa;
switched to db aaa
&gt; db.createCollection(&quot;test3&quot;);
{ &quot;ok&quot; : 1 }
&gt; show collections;
system.indexes
system.profile
test
test2
test3
&gt;
bye
</code></pre><p>4) 结论</p>
<p>可见 master 是可以正常读写的,只需要再制作从库即可</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[flask环境配置]]></title>
      <url>http://panwenhang.github.io/2016/06/23/Python/2016-06-23-flask%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<h2 id="1-安装-Flask"><a href="#1-安装-Flask" class="headerlink" title="1. 安装 Flask"></a>1. 安装 Flask</h2><p>参考: <a href="http://flask.pocoo.org/docs/0.10/installation/" target="_blank" rel="external">Flask Installation</a></p>
<pre><code>$ git clone http://github.com/mitsuhiko/flask.git
$ cd flask
$ python setup.py develop
$ pip list
    click (4.0)
    Flask (0.11.dev0, /home/xxx/flask)
    itsdangerous (0.24)
    pip (7.1.0)
    setuptools (18.0.1)
    wheel (0.24.0)
</code></pre><a id="more"></a>
<h2 id="2-安装-psycopy2"><a href="#2-安装-psycopy2" class="headerlink" title="2. 安装 psycopy2"></a>2. 安装 psycopy2</h2><pre><code>$ wget https://pypi.python.org/packages/source/p/psycopg2/psycopg2-2.6.1.tar.gz
$ tar zxvf psycopg2-2.6.1.tar.gz
$ cd psycopg2-2.6.1
$ python setup.py build
$ python setup.py install
$ pip list
    click (4.0)
    Flask (0.11.dev0, /home/xxx/flask)
    itsdangerous (0.24)
    pip (7.1.0)
    psycopg2 (2.6.1)
    setuptools (18.0.1)
    SQLAlchemy (1.0.6)
    wheel (0.24.0)
</code></pre><h2 id="3-安装-SQLAlchemy"><a href="#3-安装-SQLAlchemy" class="headerlink" title="3.安装 SQLAlchemy"></a>3.安装 SQLAlchemy</h2><pre><code>$ wget https://pypi.python.org/packages/source/S/SQLAlchemy/SQLAlchemy-1.0.6.tar.gz
$ tar zxvf SQLAlchemy-1.0.6.tar.gz
$ cd SQLAlchemy-1.0.6
$ python setup.py build
$ python setup.py install
$ pip list
    click (4.0)
    Flask (0.11.dev0, /home/xxx/flask)
    itsdangerous (0.24)
    pip (7.1.0)
    setuptools (18.0.1)
    SQLAlchemy (1.0.6)
    wheel (0.24.0)
</code></pre><h2 id="4-安装-Flask-SQLAlchemy"><a href="#4-安装-Flask-SQLAlchemy" class="headerlink" title="4.安装 Flask-SQLAlchemy"></a>4.安装 Flask-SQLAlchemy</h2><pre><code>$ easy_install Flask-SQLAlchemy
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[多版本python环境]]></title>
      <url>http://panwenhang.github.io/2016/06/23/Python/2016-06-23-%E5%A4%9A%E7%89%88%E6%9C%ACpython%E7%8E%AF%E5%A2%83/</url>
      <content type="html"><![CDATA[<h2 id="1-安装-pyenv"><a href="#1-安装-pyenv" class="headerlink" title="1. 安装 pyenv"></a>1. 安装 pyenv</h2><p>参考:<a href="https://github.com/yyuu/pyenv-installer" target="_blank" rel="external">https://github.com/yyuu/pyenv-installer</a></p>
<pre><code>$ curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash
$ vim .bash_profile
    export PATH=&quot;$HOME/.pyenv/bin:$PATH&quot;
    eval &quot;$(pyenv init -)&quot;
    eval &quot;$(pyenv virtualenv-init -)&quot;
$ source .bash_profile
$ pyenv update
</code></pre><a id="more"></a>
<h2 id="2-使用-pyenv-安装-python"><a href="#2-使用-pyenv-安装-python" class="headerlink" title="2. 使用 pyenv 安装 python"></a>2. 使用 pyenv 安装 python</h2><pre><code>$ pyenv install 2.7
$ pyenv versions
    * system (set by /home/xxx/.pyenv/version)
      2.7
</code></pre><h2 id="3-使用-virtualenv"><a href="#3-使用-virtualenv" class="headerlink" title="3. 使用 virtualenv"></a>3. 使用 virtualenv</h2><h3 id="1-创建虚拟-python-环境"><a href="#1-创建虚拟-python-环境" class="headerlink" title="(1) 创建虚拟 python 环境"></a>(1) 创建虚拟 python 环境</h3><pre><code>$ pyenv virtualenv 2.7 py27
$ pyenv verions
    * system (set by /home/xxx/.pyenv/version)
      2.7
      py27
</code></pre><h3 id="2-activate-和-deactivate"><a href="#2-activate-和-deactivate" class="headerlink" title="(2) activate 和 deactivate"></a>(2) activate 和 deactivate</h3><pre><code>$ pyenv activate py27
$ pyenv deactivate
</code></pre><h2 id="4-修改默认python环境"><a href="#4-修改默认python环境" class="headerlink" title="4. 修改默认python环境"></a>4. 修改默认python环境</h2><pre><code>$ pyenv global py27
</code></pre>]]></content>
    </entry>
    
  
  
</search>
